{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Murtaza Nazir <p>building AI one tiny piece at a time.</p>"},{"location":"linear_algebra/introduction/","title":"Introduction","text":"<ol> <li>Vectors, Linear Combinations, Eliminations </li> <li>Introduction to Vector Spaces and Sub Spaces, Rank and Invertibility</li> </ol>"},{"location":"linear_algebra/vector-spaces-and-subspaces/","title":"Introduction to Vector Spaces and Sub Spaces, Rank and Invertibility","text":""},{"location":"linear_algebra/vector-spaces-and-subspaces/#introduction","title":"Introduction","text":"<p>This article will dive deep into Vector Spaces, Subspaces, Column Spaces, Null Space, Span, Rank, Invertibility and much more.  </p>"},{"location":"linear_algebra/vector-spaces-and-subspaces/#vector-spaces","title":"Vector Spaces","text":"<p>A vector space is a collection of vectors where each vector can be defined as a linear combination of all other vectors. It can also be defined as \"if \\(n\\) vectors are in a space, then all the linear combinations of all these vectors are also in that space.\"</p> <p>e.g: for 2-D vectors, the \\(xy\\) plane is a space. It contains all the 2-D vectors. All of these vectors can be represented as a linear combination of all other vectors(some scalar multipliers might be zero).</p> <p>The above space is denoted by \\(\\mathbb{R}^2\\) and is called the real space in two dimensions(or simply the two dimensional space). It the biggest space possible for two dimensional real vectors. If we include complex numbers as well, then that space will be \\(\\mathbb{C}^2\\).</p> <p>Similarly, \\(\\mathbb{R}^1\\), \\(\\mathbb{R}^3\\), \\(\\mathbb{R}^4\\), \\(\\mathbb{R}^n\\) are the real spaces in their respective dimensions. These contain all the real respective-dimensional vectors.</p> <p>So if, for example, we have two vectors </p> \\[\\mathbf{v_1}, \\mathbf{v_2} \\in \\mathbb{R}^5\\] <p>then </p> \\[c\\mathbf{v_1} + d\\mathbf{v_2} \\in \\mathbb{R}^5 \\qquad \\forall\\  c, d \\in \\mathbb{R}\\text{(i.e scalars)}\\] <p>But any vector in \\(\\mathbb{R}^2\\) is not in the \\(\\mathbb{R}^5\\) as no linear combination of 5-dimensional vectors can form a 2-dimesional vector.</p> <p>Based on the that every space must have a zero vector (when all the scalar multipliers in a linear combination are zero).</p> <p>We can even extend the concept of spaces from traditional \"column vectors\" to other \"vectors\". Basically, we need a multiplication by a scalar with a \"vector\" keep the resulting \"vector\" in that same space. Also sum of two \"vectors\" in a space should also be in the same space. </p> <p>Note: A space may not necessarily contain column vectors. In the definition of vector space, vector addition, \\(v_1 + v_2\\), and scalar mutiplication, \\(cv\\), should follow the following eight rules: 1. \\(v_1 + v_2 = v_2 + v_1\\) 2. \\(v_1 + (v_2 + v_3) = (v_1 + v_2) + v_3\\) 3. There exists a zero vector \\(0\\) such that \\(v+0 = v\\). 4. There is a unique vector \\(-v\\), for each vector \\(v\\), such that \\(v+(-v)=0\\). 5. Multiplication of vector \\(v\\) by unity, 1 ,scalar, does not change the vector,i.e \\(1v = v\\) 6. \\((c_1c_2)v = c_1(c_2v)\\) 7. \\(c(v_1 + v_2) = cv_1 + cv_2\\) 8. \\((c_1 + c_2)v = c_1v + c_2v\\)</p> <p>Any \"vectors\" following those eight rules can have a concept of spaces in them. For example: matrices. Matrices(of same dimensions) follow all the above rues and hence concept of spaces is possible. </p> <p>All real \\(3 \\times 4\\) matrices form a space. Any two such shaped matrices can be added to give the same shaped matrix. Any such shaped matrix can be multipied(with a scalar, ofcourse!) to give the same shaped matrix again. It also incudes the zero matrix. This space can be called \\(\\mathbb{R}^{3 \\times 4}\\)</p> <p>We can also consider all real functions. They will also form a space. Also has zero function. A function space is infinite-dimensional.</p> <p>In both above examples the eight conditions are also easily checked.</p> <p>We saw multiple dimensional spaces but there is another special dimensional space, the zero dimensional space \\(\\mathbb{Z}\\). By the simple definition if it has zero dimensions, it means it has no components and so one might think there is no vector. But it only contains exactly one vector.</p> <p>So at many times we can think matrices and functions as vectors but mostly we refer to vectors when we mean column vectors and with that let's talk about subspaces.</p>"},{"location":"linear_algebra/vector-spaces-and-subspaces/#subspaces","title":"Subspaces","text":"<p>A subspace is a space inside another space. This subspace is a part of the whole space as well as satisfies all the criteria to be called a space itself.</p> <p>For example, in three dimensional vector space, any plane passing through origin is a subspace of \\(\\mathbb{R}^3\\). </p> <p>Every vector in that plane is a linear combination of other vectors in the same plane. It is also passing via origin and so has the zero vector in it. One thing to note is that a plane in three-dimensional space is not \\(\\mathbb{R}^2\\). The vectors are in \\(\\mathbb{R}^3\\) as they have three components, they just lie on a plane.</p> <p>Also note that any plane that does not pass through the origin is not a subspace as it does not account for the combination when all the multipliers of a linear combination are zero. </p> <p>Apart from all the planes passing through origin, the whole \\(\\mathbb{R}^3\\) is also a subspace of itself!. The list of all subspaces of \\(\\mathbb{R}^3\\) is: 1. The whole space itself. 2. Any plane passing through origin. 3. Any line passing through origin. 4. The Zero Vector.</p> <p>Just like \\(\\mathbb{R}^3\\), \\(\\mathbb{R}^2\\) also has its subspaces: any line passing through origin,for example.</p> <p>We also talked about matrices forming spaces. So they can also form subspaces. For example in the vector space of all \\(4 \\times 4\\) real matrices i.e \\(\\mathbb{R}^{4 \\times 4}\\),a subspace will be all the \\(4 \\times 4\\) diagonal real matrices as all linear combinations of these matrices will also reult in a \\(4 \\times 4\\) real diagonal matrix. We can also have a subspace of traiangular matrices. Further more, diagonal matrices are the subspaces of triangular matrix space(of the same dimensions!).</p> <p>Also, all these spaces also have the Zero matrix in them, which is also an important check for a space.</p>"},{"location":"linear_algebra/vector-spaces-and-subspaces/#column-space-of-a-ca","title":"Column Space of \\(A\\): \\(C(A)\\)","text":"<p>A column space is related to a matrix. A column space of a matrix \\(A\\) is, the space containing only all linear combinations of columns of a matrix. It is represented as \\(C\\left(A\\right)\\) It is explained in the context of solving the equation: </p> \\[A\\mathbf{x} = \\mathbf{b}\\] <p>For all the possible \\(\\mathbf{x}\\), \\(A\\mathbf{x}\\) is linear combinations of columns of \\(A\\).</p> <p>e.g: If</p> \\[A = \\begin{bmatrix}2 &amp; 3  \\\\ 6 &amp; 1 \\\\ -1 &amp; 8 \\end{bmatrix}\\] <p>then,</p> \\[A\\mathbf{x} = \\begin{bmatrix}2 &amp; 3 \\\\ 6 &amp; 1 \\\\ -1 &amp; 8 \\end{bmatrix} \\ \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}\\] <p>By our definition of matrix vector multiplication, in the previous chapter:</p> \\[A\\mathbf{x} = x_1 \\begin{bmatrix}2 \\\\6 \\\\ -1 \\end{bmatrix} + x_2 \\begin{bmatrix}3 \\\\1 \\\\ 8 \\end{bmatrix} \\] <p>So for all \\(\\mathbf{x}\\), \\(A\\mathbf{x}\\) is all the linear combinations of columns of \\(A\\). So \\(A\\mathbf{x}\\) represents the whole column space, for all \\(\\mathbf{x}\\). For a specific \\(\\mathbf{x}\\), say \\(\\mathbf{x_1}\\) we have a specific vector \\(A\\mathbf{x_1}\\) which lies in the column space of \\(A\\).</p> <p>So for \\(A\\mathbf{x} = \\mathbf{b}\\) to have any solution, \\(\\mathbf{b}\\) has to be in the column space of \\(A\\). Otherwise there is no solution. If \\(\\mathbf{b}\\) is in \\(C\\left(A\\right)\\), then the vector of multipliers of that combination is the solution of this equation.</p> <p>For any matrix of shape \\(m\\) by \\(n\\), each column has \\(m\\) components, so the column space of that matrix is a subspace of \\(\\mathbb{R}^m\\).</p> <p>For our example above the column space will form a plane in \\(\\mathbb{R}^3\\) passing via origin.</p> <pre><code>using Plotly\n</code></pre> <pre><code># using multiple values for x\nx= [[i;j] for i=-0.7:0.01:0.7,j=-0.7:0.01:0.7]\n\n# create matrix A\nA = [2 3; 6 1; -1 8]\n# calculate the linear combination for each vector (x_1,x_2)\nlinear_combos = [A*xx for xx in x]\n\n#fetch the x, y and z cordinates from each vector\nx_s = [xx[1] for xx in linear_combos]\ny_s = [xx[2] for xx in linear_combos]\nz_s = [xx[3] for xx in linear_combos]\n\n# plot column 1\ntrace1 = scatter3d(x=[0, A[1,1]], y=[0, A[2,1]], z=[0, A[3,1]], mode=\"lines\", line=attr(width=5), name=\"vector 1\")\n#plot column 2\ntrace2 = scatter3d(x=[0, A[1,2]], y=[0, A[2,2]], z=[0,A[3,2]], mode=\"lines\", line=attr(width=5), name=\"vector 2\")\n# plot the combinations\ntrace3 = surface(x=x_s, y=y_s, z=z_s, opacity=0.8, showscale=false, name=\"Plane\")\n\np = plot([trace1,trace2,trace3], Layout(scene_camera=attr(eye=attr(x=1.5, y=1.2, z=1)), title=\"Column Space of a matrix\"))\n</code></pre> <p>The plane represents all the linear combinations of these two vectors lie on that plane which also passes through the origin. It is the column space of the matrix. It is a subspace of the whole \\(\\mathbb{R}^3\\) space. For all the vectors \\(\\mathbf{b}\\) lying in that plane, \\(A\\mathbf{x} = \\mathbf{b}\\) has solution. But for those \\(\\mathbf{b}\\) which do not lie in that plane (i.e are not in the column space), have no solution.</p> <p>Span:  Now instead of just columns we can have a set of vectors \\(V\\) and their linear combinations form a space \\(VV\\). This span is the smallest space that contains all the linear combinations of these vectors. This can also be said as The subspace \\(VV\\) is the span of set \\(V\\). So the columns \"span\" the column space.</p> <p>So, as per above observations, it entirely depends on the vector \\(\\mathbf{b}\\) to have a solution to \\(A\\mathbf{x}=\\mathbf{b}\\). If the vector \\(\\mathbf{b}\\) is in the column space of \\(A\\) or not. But there is one vector for \\(\\mathbf{b}\\) we would always have solution(s) for. The only vector(of the same dimension) which lies in all subspaces. The zero vector.</p> <p>That means, \\(A\\mathbf{x} = 0\\) will always have solution(s). (\\(0\\) here is a zero vector, not a scalar.)</p> <p>This gives introduction to another important subspace called the Null Space. </p>"},{"location":"linear_algebra/vector-spaces-and-subspaces/#null-space-of-a-na","title":"Null Space of \\(A\\): \\(N(A)\\)","text":"<p>We will now deal with special equations where the right hand side is always a zero vector, i.e \\(\\mathbf{b} =0\\).</p> <p>Let's look at the possible solutions of \\(A\\mathbf{x} = 0\\):</p> <ol> <li>\\(\\mathbf{x} = 0\\) is always a solution to it.</li> <li>If \\(\\mathbf{x_1}\\) is a solution, then so is \\(c_1\\mathbf{x_1}\\), \\(c_1\\) being a scalar.</li> </ol> <p>If</p> \\[A\\mathbf{x_1} = 0\\] <p>then</p> \\[Ac_1\\mathbf{x_1} = c_1A\\mathbf{x_1} = 0\\] <ol> <li>If \\(\\mathbf{x_1}\\) and \\(\\mathbf{x_2}\\) are two solutions, then \\(x_1 + x_2\\) is also a solution. It can be shown as:</li> </ol> <p>If \\(\\mathbf{x_1}\\) and \\(\\mathbf{x_2}\\) are solutions, then:</p> \\[A\\mathbf{x_1} = 0\\] <p>and</p> \\[A\\mathbf{x_2} = 0\\] <p>and so,</p> \\[\\begin{align}A\\mathbf{x_1} + A\\mathbf{x_2} = 0 \\\\ \\implies A \\left(\\mathbf{x_1} + \\mathbf{x_2}\\right) = 0\\end{align}\\] <p>so \\(\\mathbf{x_1} + \\mathbf{x_2}\\) is also a solution.</p> <ol> <li>By the above two points we can show that all the linear combinations of the solutions are also solutions to \\(A\\mathbf{x} = 0\\).</li> </ol> <p>So we can conclude the solutions of \\(A\\mathbf{x} = 0\\) form a subspace. This subspace is called the Null Space of matrix \\(A\\). It is denoted by \\(N(A)\\).</p> <p>If the matrix \\(A\\) is \\(m\\) by \\(n\\), then the null space, \\(N(A)\\) is a subspace of \\(\\mathbb{R}^n\\) (while the column space, \\(C(A)\\), was in \\(\\mathbb{R}^m\\)).</p>"},{"location":"linear_algebra/vector-spaces-and-subspaces/#finding-the-null-space","title":"Finding the Null Space","text":"<p>The column space is simply the space of all linear combinations of columns of \\(A\\) and the columns are known. But for null space, we need to find the solutions.</p> <p>We know that the solutions were found by elimination. We will do the same, but with some special extensions. We have used elimination on the square matrices (i.e \\(m\\) equations \\(m\\) variables, but let's generalize it to rectangluar matrices).</p> <p>Let's say we have two equations and two variables:</p> \\[\\begin{align}x_1 + 7x_2 &amp;= 0 \\\\ 2x_1 + 14x_2 &amp;=0\\end{align}\\] <p>If we eliminate, we get:</p> \\[\\begin{align}x_1 + &amp;7 x_2 = 0 \\\\ &amp;0x_2 = 0\\end{align}\\] <p>Here \\(x_2\\) is called the free variable. The above system of equations is equivalent to just one of the above eqautions(both are same!). </p> <p>We can set the free variable to anything and get corresponding \\(x_1\\) for that solution. Preferably, we set \\(x_2=1\\), and so \\(x_1 = -7\\).</p> <p>This solution \\((-7,1)\\) is a specific solution to the equation above. The nullspace is all the linear combinations of this solution. Since we are having linear combinations of just one vector, it will result in a line through origin. That line represents the nullspace of the equation system. </p> <p>So, \\(c\\begin{bmatrix}-7 \\\\ 1\\end{bmatrix}\\) is the nullspace of this matrix for all scalars, \\(c\\).</p> <p>Another Example</p> <p>Let's say we have two equations and four variables, e.g:</p> \\[\\begin{align}3x_1 + 2x_2 + 4x_3 - 3x_4 &amp;=0 \\\\ 6x_1 + 3x_2 - x_3 + 4x_4 &amp;= 0\\end{align}\\] <p>It can be written in matrix form as, \\(A\\mathbf{x} = 0\\), where</p> \\[A = \\begin{bmatrix} \\mathbf{3} &amp; 2 &amp; 4 &amp; -3 \\\\ 6 &amp; 3 &amp; -1 &amp; 4 \\end{bmatrix}\\] <p>and</p> \\[\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3\\\\ x_4 \\end{bmatrix}\\] <p>So, essentially we are trying to find the nullspace of \\(A\\). We will proceed with elimination, but note that we need not to perform the same elimination on Right Hand Side as it is a Zero vector and it won't change.</p> <p>Eliminate using the first pivot (bold-face in matrix above):</p> \\[E_1A = \\begin{bmatrix} 3 &amp; 2 &amp; 4 &amp; -3 \\\\ 0 &amp; \\mathbf{-1} &amp; -9 &amp; 10 \\end{bmatrix}\\] <p>And now we are done. This is the closest to \"upper traingular\" system we can get. The Pivots are: 3, -1. The two columns, which have pivots are called pivot columns. The rest of the columns are called free columns.</p> \\[\\begin{align}E_1A = U =  &amp;\\begin{bmatrix} 3 &amp; 2 &amp; 4 &amp; -3 \\\\ 0 &amp; -1 &amp; -9 &amp; 10 \\end{bmatrix}\\\\ &amp; \\underbrace{\\uparrow \\hspace{28px} \\uparrow}_\\text{pivot columns} \\hspace{10px} \\underbrace{\\uparrow \\hspace{28px} \\uparrow}_\\text{free columns}\\end{align}\\] <p>The variables corresponding to the free columns are the free variables, in this case, \\(x_3\\) and \\(x_4\\). Now to get the nullspace, we will first need the specific solutions and then the nullspace will formed by all the linear combinations of those specific solutions. </p> <p>For each specific solution, we will set each free variable equal to 1 and all other free varaibles to zero. Since this has two free columns, which means two free variables, we have two specific solutions.</p> <p>One will have \\(x_3=1\\),\\(x_4=0\\) and the other will have \\(x_3=0\\), \\(x_4=1\\). </p> <p>For specific solution 1,</p> <p>Use \\(x_3=1,x_4=0\\) in the second row and we get \\(x_2=-9\\). and then backsubstituting \\(x_2,x_3,x_4\\) into row 1, we get \\(x_1 = \\frac{14}{3}\\) </p> <p>Our first specific solution is </p> \\[s_1 = \\begin{bmatrix} \\frac{14}{3} \\\\ -9 \\\\ 1 \\\\0\\end{bmatrix}\\] <p>For second specific solution, we will keep \\(x_3 = 0\\) and \\(x_4 = 1\\).</p> <p>Back substituting,</p> <p>\\(x_2 = 10\\) and \\(x_1 = \\frac{-17}{3}\\)</p> <p>Second specific solution is </p> \\[s_2 = \\begin{bmatrix} \\frac{-17}{3} \\\\ 10 \\\\ 0 \\\\1\\end{bmatrix}\\] <p>The nullspace contains all the linear combinations of \\(s_1\\) and \\(s_2\\). If we put these two specific solutions in a matrix.</p> \\[ S = \\begin{bmatrix} \\frac{14}{3} &amp; \\frac{-17}{3} \\\\ -9 &amp; 10 \\\\ 1 &amp; 0 \\\\0 &amp; 1 \\end{bmatrix}\\] <p>We can also say the nullspace of \\(A\\) was the columnspace of \\(S\\).</p> \\[ N(A) = C(S)\\]"},{"location":"linear_algebra/vector-spaces-and-subspaces/#reduced-row-echelon-form","title":"Reduced Row Echelon Form","text":"<p>We most often take a step further from converting the matrix \\(A\\) to upper triangular \\(U\\) during elimination. We convert it into, what's called, a reduced row echelon form, \\(R\\). It gives us a better view at the nullspace of \\(A\\) which is same as \\(U\\) and as we will see for \\(R\\) as well.</p> <p>In this form we do not stop at the upper triangular \\(U\\) form but we remove the elements above the pivots as well. And we also divide each pivot row to make its pivot unity.</p> <p>In our above example, we had reduced till upper traingular:</p> \\[ U =  \\begin{bmatrix} \\mathbf{3} &amp; 2 &amp; 4 &amp; -3 \\\\ 0 &amp; \\mathbf{-1} &amp; -9 &amp; 10 \\end{bmatrix}\\] <p>Our pivots (in bold-face) will eliminate all the elements above them as well. The first pivot has not to change anything. But we can remove the elements above the second pivot.</p> \\[\\begin{align}\\text{row}_1 \\leftarrow \\text{row}_1 + 2\\text{row}_2\\\\ E_2U =  \\begin{bmatrix} \\mathbf{3} &amp; 0 &amp; -14 &amp; 17 \\\\ 0 &amp; \\mathbf{-1} &amp; -9 &amp; 10 \\end{bmatrix}\\end{align}\\] <p>Now we have removed the entries, we have to make our pivots unity.</p> \\[\\begin{align}\\text{row}_1 \\leftarrow \\frac{\\text{row}_1}{3}\\text{ ; }\\text{row}_2 \\leftarrow \\frac{\\text{row}_2}{-1}\\\\ E_3U = R =  \\begin{bmatrix} \\mathbf{1} &amp; 0 &amp; \\frac{-14}{3} &amp; \\frac{17}{3} \\\\ 0 &amp; \\mathbf{1} &amp; 9 &amp; -10 \\end{bmatrix}\\end{align}\\] <p>Here we have reached the reduced row echelon form. The pivot columns form a sub identity matrix. But here is the good part. If we look at our two special solutions,\\(s_1\\) and \\(s_2\\), the free columns seem to have half of the solution. Well actually the negative of the free columns.</p> <p>If we show our reduced matrix in the form of this identity and free columns, i.e:</p> \\[R = \\begin{bmatrix} I &amp; F \\end{bmatrix}\\] <p>Then our special solution matrix is:</p> \\[S = \\begin{bmatrix} -F \\\\I \\end{bmatrix}\\] <p>Let's look at one more example, in which I will try to encapsulate all cases this form can have. Let's have a 3 by 5 matrix and find its nullspace, i.e three equations and 5 variables.</p> \\[ A = \\begin{bmatrix} \\mathbf{3} &amp; 4 &amp; 2 &amp; -3 &amp; 8 \\\\ 6 &amp; 8 &amp; 3 &amp; 4 &amp; 2 \\\\ 12 &amp; 16 &amp; 7 &amp; -2 &amp; 18 \\end{bmatrix}\\] <p>Now to find it's null space, we will find it's special solutions for \\(A\\mathbf{x} = 0\\). For that we will use elimination to convert it into \\(U\\) form. Eliminating using the first pivot (bold-face).</p> \\[\\begin{align}\\text{row}_2 \\leftarrow \\text{row}_2 - 2\\text{row}_1\\\\ \\text{row}_3 \\leftarrow \\text{row}_3 - 4\\text{row}_1\\\\ E_1A = \\begin{bmatrix} 3 &amp; 4 &amp; 2 &amp; -3 &amp; 8 \\\\ 0 &amp; 0 &amp; -1 &amp; 10 &amp; -14 \\\\ 0 &amp; 0 &amp; -1 &amp; 10 &amp; -14  \\end{bmatrix}\\end{align}\\] <p>Now the number at second pivot has become ZERO, which calls for a row exchange, but every element below it is also zero and hence no row to exchange with. So, elimination done? No. This whole column is actually a free column and not a pivot column. The next column to it is the pivot column and \\(-1\\) (bold-face, below) is the second pivot. </p> \\[E_1A = \\begin{bmatrix} 3 &amp; 4 &amp; 2 &amp; -3 &amp; 8 \\\\ 0 &amp; 0 &amp; \\mathbf{-1} &amp; 10 &amp; -14 \\\\ 0 &amp; 0 &amp; -1 &amp; 10 &amp; -14\\end{bmatrix}\\] <p>Now going on with the elimination with this pivot, we get:</p> \\[\\begin{align}\\text{row}_3\\leftarrow \\text{row}_3-\\text{row}_2\\\\E_2E_1A = U = \\begin{bmatrix} 3 &amp; 4 &amp; 2 &amp; -3 &amp; 8 \\\\ 0 &amp; 0 &amp; -1 &amp; 10 &amp; -14 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0  \\end{bmatrix}\\end{align}\\] <p>Now we have finished the elimination and reached the \\(U\\) phase.</p> <p>Note: Each column and row can have at most one pivot.</p> <p>At this point, usually we would check if our right hand side (which was \\(\\mathbf{b}\\)) had zero at the last entry or not \\(-\\) to check for no solution. But in null space, all the RHS are zero, so we don't need to worry.</p> <p>So the pivot columns are column 1 and column 3, and free columns are column 2, column 4 and column 5, which means \\(x_2\\), \\(x_4\\) and \\(x_5\\) are free variables. That means we have three specific solutions, one when \\((x_2, x_4, x_5)=(1,0,0)\\), one when \\((x_2, x_4, x_5)=(0,1,0)\\) and another when \\((x_2,x_4,x_5)=(0,0,1)\\).  </p> <p>Back substitute to find the solution of:</p> \\[ U\\mathbf{x} = \\begin{bmatrix} 3 &amp; 4 &amp; 2 &amp; -3 &amp; 8 \\\\ 0 &amp; 0 &amp; -1 &amp; 10 &amp; -14 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0  \\end{bmatrix}\\ \\begin{bmatrix}x_1\\\\x_2\\\\x_3\\\\x_4\\\\x_5\\end{bmatrix} = \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 0\\\\ 0\\end{bmatrix}\\] <p>Back-substituting \\(x_2=1\\), \\(x_4=0\\) and \\(x_5=0\\) to compute \\(s_1\\):</p> <ol> <li> <p>The last equation(row) contributes nothing.</p> </li> <li> <p>The second equation(row) gives \\(x_3 = 0\\)</p> </li> <li> <p>And the first equation gives \\(x_1 = \\frac{-4}{3}\\)</p> </li> </ol> <p>and so our first specific solution is:</p> \\[s_1 = \\begin{bmatrix} \\frac{-4}{3} \\\\ 1 \\\\0 \\\\ 0\\\\0\\end{bmatrix}\\] <p>Back-substituting \\(x_2=0\\), \\(x_4=1\\) and \\(x_5=0\\) to compute \\(s_2\\):</p> <ol> <li> <p>The last equation(row) contributes nothing.</p> </li> <li> <p>The second equation(row) gives \\(x_3 = 10\\)</p> </li> <li> <p>And the first equation gives \\(x_1 = \\frac{-17}{3}\\)</p> </li> </ol> <p>and so our first specific solution is:</p> \\[s_2 = \\begin{bmatrix} \\frac{-17}{3} \\\\ 0 \\\\10 \\\\ 1 \\\\ 0\\end{bmatrix}\\] <p>Back-substituting \\(x_2=0\\), \\(x_4=0\\) and \\(x_5=1\\) to compute \\(s_3\\):</p> <ol> <li> <p>The last equation(row) contributes nothing.</p> </li> <li> <p>The second equation(row) gives \\(x_3 = -14\\)</p> </li> <li> <p>And the first equation gives \\(x_1 = \\frac{20}{3}\\)</p> </li> </ol> <p>and so our first specific solution is:</p> \\[s_3 = \\begin{bmatrix} \\frac{20}{3} \\\\ 0 \\\\-14 \\\\ 0 \\\\ 1\\end{bmatrix}\\] <p>And so the null space is all the linear combinations of these three vectors. The null space will be 3-D surface inside a 4-Dimensional space. We can also group these solutions into a matrix \\(S\\), and so \\(N(A) = C(S)\\).</p> \\[S = \\begin{bmatrix} \\frac{-4}{3}&amp; \\frac{-17}{3} &amp; \\frac{20}{3}  \\\\ 1 &amp; 0 &amp; 0 \\\\0 &amp; 10 &amp; -14 \\\\ 0 &amp; 1&amp; 0 \\\\ 0 &amp; 0 &amp; 1\\end{bmatrix}\\] <p>Keeping this \\(S\\) in mind, we move on to the Row Echelon form of A. Let's check our \\(U\\) highlighting its pivots.</p> \\[U=\\begin{bmatrix} \\mathbf{3} &amp; 4 &amp; 2 &amp; -3 &amp; 8 \\\\ 0 &amp; 0 &amp; \\mathbf{-1} &amp; 10 &amp; -14 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0  \\end{bmatrix}\\] <p>Reduce the elements above the pivots as well and make pivots unity.</p> \\[\\begin{align}\\text{row}_1 \\leftarrow \\text{row}_1+2\\text{row}_2\\\\ E_3U = \\begin{bmatrix} \\mathbf{3} &amp; 4 &amp; 0 &amp; 17 &amp; -20 \\\\ 0 &amp; 0 &amp; \\mathbf{-1} &amp; 10 &amp; -14 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0  \\end{bmatrix}\\end{align}\\] \\[\\begin{align}\\text{row}_1 \\leftarrow \\frac{\\text{row}_1}{3}\\\\ \\text{row}_2 = -1(\\text{row}_2)\\\\ E_4E_3U = R = \\begin{bmatrix} \\mathbf{1} &amp; \\frac{4}{3} &amp; 0 &amp; \\frac{17}{3} &amp; \\frac{-20}{3} \\\\ 0 &amp; 0 &amp; \\mathbf{1} &amp; -10 &amp; 14 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0  \\end{bmatrix}\\end{align}\\] <p>All pivots are Zero and no element above or below the pivots are non-zero. By definition, we have reached the reduced row echelon form.</p> <p>If we look at the matrix \\(S\\), there are negative of elements of free columns of \\(R\\) in matrix \\(S\\). </p> <p>Let me tell you how it relates, if our reduced row echelon form to looks like:</p> \\[ R = \\begin{bmatrix} I &amp; F \\\\ 0 &amp; 0 \\end{bmatrix}\\] <p>where:</p> <p>\\(F\\) are the free columns.</p> <p>\\(\\begin{bmatrix}0 &amp; 0 \\end{bmatrix}\\) are all the zero rows until the end of the matrix.</p> <p>then our null solution matrix is:</p> \\[S = \\begin{bmatrix} -F \\\\ I' \\end{bmatrix} \\] <p>where \\(I'\\) is the identity matrix of the same size as the number of free columns in \\(R\\).</p> <p>But our current example isn't looking like the form we want, for that we can do column exchanges, but that changes the system and so we need to change the variable vector as well. Let's first write our full equation:</p> \\[R\\mathbf{x} = 0\\] <p>in our example,</p> \\[\\begin{bmatrix} \\mathbf{1} &amp; \\frac{4}{3} &amp; 0 &amp; \\frac{17}{3} &amp; \\frac{-20}{3} \\\\ 0 &amp; 0 &amp; \\mathbf{1} &amp; -10 &amp; 14 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0  \\end{bmatrix} \\ \\begin{bmatrix} \\pmb{x_1} \\\\ x_2 \\\\ \\pmb{x_3} \\\\ x_4 \\\\x_5 \\end{bmatrix} = 0\\] <p>If we want to exchange our 2nd and 3rd column of \\(R\\), we need to exchange our 2nd and 3rd variable as well to preserve the multiplication.</p> \\[\\begin{bmatrix} \\mathbf{1} &amp; 0 &amp; \\frac{4}{3} &amp; \\frac{17}{3} &amp; \\frac{-20}{3} \\\\ 0  &amp; \\mathbf{1} &amp; 0 &amp; -10 &amp; 14 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0  \\end{bmatrix} \\ \\begin{bmatrix} \\pmb{x_1} \\\\ \\pmb{x_3} \\\\ x_2 \\\\ x_4 \\\\x_5 \\end{bmatrix} = 0\\] <p>Notice I have exchanged \\(x_2\\) and \\(x_3\\) as well. Now we have the form </p> \\[\\begin{bmatrix} I &amp; F \\\\ 0 &amp; 0 \\end{bmatrix}\\] <p>So our null matrix should be </p> \\[\\begin{bmatrix} -F \\\\ I' \\end{bmatrix}\\] \\[-F = \\begin{bmatrix} \\frac{-4}{3} &amp; \\frac{-17}{3} &amp; \\frac{20}{3} \\\\ 0 &amp; 10 &amp; -14\\end{bmatrix}\\] <p>and since there are 3 free columns \\(I'\\) is a 3 \\(\\times\\) 3 identity matrix.</p> \\[ I' = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}\\] <p>And so our null matrix is:</p> \\[ \\begin{bmatrix} -F \\\\ I' \\end{bmatrix}  = \\begin{bmatrix} \\frac{-4}{3} &amp; \\frac{-17}{3} &amp; \\frac{20}{3} \\\\ 0 &amp; 10 &amp; -14 \\\\ 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} \\] <p>and we are done? No. A small change needs to be made. This null matrix corresponds to the changed variable vector, i.e in each column the first entry corresponds to \\(x_1\\), second entry to \\(x_3\\) (not \\(x_2\\)), third entry to \\(x_2\\) (not \\(x_3\\)), fourth entry to \\(x_4\\) and fifth entry to \\(x_5\\). This change was because of the column exchange we did back there.</p> <p>So to get back the correct form of the null matrix we exchange the row2 and row 3 of the above matrix, and so:</p> \\[S = \\begin{bmatrix} \\frac{-4}{3} &amp; \\frac{-17}{3} &amp; \\frac{20}{3} \\\\  1 &amp; 0 &amp; 0 \\\\ 0 &amp; 10 &amp; -14 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}\\] <p>And now we have finished. These three columns are the special solutions we have to find the whole null space.</p> <p>However it is not necessary, to turn \\(R\\) into the said form and do column exchanges, the \\(R\\) form itself makes the back substitution very easy.</p> <p>But if we use the \\(\\begin{bmatrix} I &amp; F \\\\ 0 &amp; 0 \\end{bmatrix}\\), we must know that sometimes there may not be a bunch of zero rows and so our form might be \\(\\begin{bmatrix} I &amp; F \\end{bmatrix}\\) which will again have the same solution. The zero rows do not contribute to the nullspace. However if the RHS were not a zero vector, these rows would decide the possibility of no solution.</p> <p>If our matrix \\(A\\) is of size \\(m \\times n\\) and,</p> <ol> <li> <p>If \\(n &gt; m\\), i.e more variables than equations(like in our previuos example), we will atleast have \\(n-m\\) free columns (because each row and column can at most have one pivot), so there will be atleast \\(n-m\\) special solutions, which means we will have infinite \"non-zero\" solutions, which will form a nullspace. These will have a form of  \\(R = \\begin{bmatrix} I &amp; F \\\\ 0 &amp; 0 \\end{bmatrix}\\) or \\(\\begin{bmatrix} I &amp; F \\end{bmatrix}\\).</p> </li> <li> <p>If \\(m &gt; n\\), i.e more equations than variables, we will atleast have \\(m-n\\) rows of zeros at the bottom. Since the columns are less than rows, all the columns can be pivots, so it may or may not have any \\(F\\) column. The possible scenarios are \\(\\begin{bmatrix} I &amp; F \\\\ 0 &amp; 0 \\end{bmatrix}\\) and \\(\\begin{bmatrix} I\\\\ 0 \\end{bmatrix}\\). In the first case there are special solutions and hence infinite solutions. In the second case, since there is no free column, there is only one vector in nullspace, the zero vector, i.e there is no non-zero null vector.</p> </li> <li> <p>If \\(m=n\\), i.e equal number of equations and variables. It has many possibilities. It may or may not have \\(F\\) and even it may or may not have zero rows. So possibilities are \\(\\begin{bmatrix} I &amp; F \\\\ 0 &amp; 0 \\end{bmatrix}\\), \\(\\begin{bmatrix} I \\\\ 0 \\end{bmatrix}\\) , \\(\\begin{bmatrix} I &amp; F \\end{bmatrix}\\) and even \\(\\begin{bmatrix} I\\end{bmatrix}\\). The last case also has just one null vector, the zero vector. If a matrix is able to reduce to the last case, it is said to be invertible. More on it later.</p> </li> </ol> <p>I highly recommend you to work on some problems yourself and try to get the feel for this form.</p>"},{"location":"linear_algebra/vector-spaces-and-subspaces/#the-rank-of-a-matrix","title":"The Rank of a Matrix","text":"<p>Now we are introducing an important concept for our matrices, the rank of the matrix.</p> <p>Our matrix defines a linear system. If a matrix is \\(m \\times n\\), that is supposed to be it's size. But is it it's true size? Well, what do I mean by true size? </p> <p>Let's dig into some basic questions. What did the pivot columns  signify? Why does elimination cause some rows to be Zero? What do free columns signify?</p> <p>For that let's revisit our example of reducing a matrix to reduced row echelon form and finding the pivots.</p> <p>Our initial matrix was:</p> \\[ A = \\begin{bmatrix} 3 &amp; 4 &amp; 2 &amp; -3 &amp; 8 \\\\ 6 &amp; 8 &amp; 3 &amp; 4 &amp; 2 \\\\ 12 &amp; 16 &amp; 7 &amp; -2 &amp; 18 \\end{bmatrix}\\] <p>and the reduced row echelon form of \\(A\\) was:</p> \\[R = \\begin{bmatrix} \\mathbf{1} &amp; \\frac{4}{3} &amp; 0 &amp; \\frac{17}{3} &amp; \\frac{-20}{3} \\\\ 0 &amp; 0 &amp; \\mathbf{1} &amp; -10 &amp; 14 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0  \\end{bmatrix}\\] <p>The size of \\(A\\) is \\(3 \\times 5\\).</p> <p>The last row turned to be zero. It is because it did not add anything new to the system. It was a linear combination of the rows above it. Actually,</p> \\[\\text{row}_3 \\text{ of }A = 2(\\text{row}_1 \\text{ of }A) + \\text{row}_2 \\text{ of }A\\] <p>So the true size of this matrix from the row size is actually 2 not 3.</p> <p>Now let's look at pivot columns and free columns. Why was the second column declared as a free column? Because it did not have a pivot. But what does not having a pivot mean? It means this column is a linear combination of all the pivot columns before it. </p> <p>Every free column is a linear combination of the pivot columns before them. The special solutions tell us these combinations.</p> \\[\\begin{align}\\text{col}_2 \\text{ of }A &amp;= \\frac{4}{3}\\left(\\text{col}_1 \\text{ of }A\\right) ; &amp; s_1 = \\left(\\frac{-4}{3}, 1 , 0, 0, 0\\right) \\\\ \\text{col}_4 \\text{ of }A &amp;= \\frac{17}{3}\\left(\\text{col}_1 \\text{ of }A\\right) + (-10)\\left(\\text{col}_3 \\text{ of }A\\right) ; &amp; s_2 = \\left(\\frac{-17}{3}, 0 , 10, 1, 0\\right) \\\\ \\text{col}_5 \\text{ of }A &amp;= \\frac{-20}{3}\\left(\\text{col}_1 \\text{ of }A\\right) + 14\\left(\\text{col}_3 \\text{ of }A\\right) ; &amp; s_2 = \\left(\\frac{20}{3}, 0 , -14, 0, 1\\right) \\end{align}\\] <p>So these three columns also don't provide any new information to the linear system. So the true size of this matrix from the column size is actually 2 not 5.</p> <p>So, we can say the true size of this matrix is \\(2 \\times 2\\).</p> <p>The number of the pivot columns is the true size of every matrix from the column size. And we can argue that non-zero rows are all the pivot rows. And the number of pivot rows is same as number of pivot columns(becasue a column and a row can have at most one pivot).</p> <p>So the number of true columns and true rows is actually the same no matter the size of the matrix and that is called the rank of the matrix. </p> <p>The rank of the matrix is defined as the number of pivots.</p> <p>So in our example,</p> \\[r(A) = 2\\] <p>So if a matrix of size \\(m \\times n\\) has rank \\(r\\), it means it has \\(r\\) pivots and so \\(n-r\\) free columns which means \\(n-r\\) special solutions.</p> <p>The concept of Rank is very important and it will pop up every once in a while. We will not go too deep into this concept as we will lose track of what we are doing. We will revisit it's applications whenever needed. But for now, let's move on to finding solutions to a linear system.</p>"},{"location":"linear_algebra/vector-spaces-and-subspaces/#solving-amathbfxmathbfb","title":"Solving \\(A\\mathbf{x}=\\mathbf{b}\\)","text":"<p>We have already solved the form \\(A\\mathbf{x} = \\mathbf{b}\\) for a unique solution. We will revisit these briefly and also include a new method to do it along with for the infinite solutions. We will see how reduced row echelon form, \\(R\\) makes both the solutions easy and see the role of rank in it.</p> <p>We know \\(A\\mathbf{x}=\\mathbf{b}\\) is solvable only if \\(\\mathbf{b}\\) is in column space of \\(A\\). We used to reduce this form to \\(U\\mathbf{x} = \\mathbf{b'}\\) and then back substituted. But now we will go ahead and reduce it further to \\(R\\mathbf{x} = \\mathbf{d}\\).</p> <p>Now \\(R\\) has the general form of \\(\\begin{bmatrix} I &amp; F \\\\ 0 &amp; 0 \\end{bmatrix}\\). This matrix (\\(R\\) as well as \\(A\\)) has a rank \\(r\\).</p> <p>If the matrix is \\(m \\times n\\) and the reduced form turned out to be \\(\\begin{bmatrix} I &amp; F\\end{bmatrix}\\) or \\(\\begin{bmatrix} I \\end{bmatrix}\\) (i.e no Zero rows and \\(m \\leq n\\)), which means all the rows have a pivot and so the rank of this matrix is \\(r = m \\leq n\\). This is called the full row rank matrix. In this form there is always either 1 or infinte number of solutions to \\(A\\mathbf{x} = \\mathbf{b} (\\text{or } R\\mathbf{x} = \\mathbf{d})\\). </p> <p>In case of the reduced form turned out to be \\(\\begin{bmatrix} I \\\\ 0\\end{bmatrix}\\) or \\(\\begin{bmatrix} I \\end{bmatrix}\\) (i.e no free columns and \\(n \\leq m\\)), which means all the columns have a pivot and so the rank of this matrix is \\(r = n \\leq m\\). This is called full column rank matrix. This form can have zero or one solution.</p> <p>The intersection of these two cases, i.e if a matrix is reduced to \\(\\begin{bmatrix} I \\end{bmatrix}\\), it is an invertible matrix. It's rank is \\(r=m=n\\). It has exactly one solution. \\(\\mathbf{x} = A^{-1}\\mathbf{b}\\). More on it in a moment.</p> <p>Coming back to the general case, when \\(R = \\begin{bmatrix} I &amp; F \\\\ 0 &amp; 0 \\end{bmatrix}\\). We will now augment \\(A\\) and \\(\\mathbf{b}\\) as \\(\\begin{bmatrix} A &amp; \\mathbf{b}\\end{bmatrix}\\) and perform elimination on the full matrix to turn it into \\(\\begin{bmatrix} R &amp; \\mathbf{d}\\end{bmatrix}\\).</p> \\[E\\begin{bmatrix} A &amp; \\mathbf{b}\\end{bmatrix} = \\begin{bmatrix} R &amp; \\mathbf{d}\\end{bmatrix}\\] <p>where:</p> <ul> <li>\\(R = EA\\)</li> <li>\\(\\mathbf{d} = E\\mathbf{b}\\)</li> <li>\\(E\\) is the elimination matrix.</li> </ul> <p>Let's talk about the no-solution case first. It is only possible if \\(R\\) has zero rows. If \\(A\\) of size \\(m \\times n\\) has rank \\(r\\), then the number of zero rows in \\(R\\) = \\(m-r\\), and so the last \\(m-r\\) elements of \\(\\mathbf{d}\\) must be zero inorder to have any solution. If they are not zero, we don't have any solution.</p> <p>Now we have eliminated the possibility of no-solution, by checking all the last \\(m-r\\) elements of \\(\\mathbf{d}\\) are zero. We find all the solutions of \\(A\\mathbf{x} = \\mathbf{b}\\) by finding a particular solution, any solution, \\(\\mathbf{x_p}\\) to \\(A\\mathbf{x} = \\mathbf{b}\\). Once we have found one solution, all solutions are this particular solution plus some vector in nullspace of \\(A\\); \\(\\mathbf{x_n} \\in N(A)\\).</p> \\[\\begin{align}\\mathbf{x} &amp;= \\mathbf{x_p} + \\mathbf{x_n} \\\\ \\implies A\\mathbf{x} &amp;= A(\\mathbf{x_p} + \\mathbf{x_n}) \\\\ A\\mathbf{x} &amp;= A\\mathbf{x_p} + A\\mathbf{x_n}\\\\A\\mathbf{x} &amp;= \\mathbf{b} + 0 \\\\A\\mathbf{x} &amp;= \\mathbf{b}\\end{align}\\] <p>The particular solution can be any solution, but one easy to find is to set all the free variables as zero. Let's see an example:</p> \\[ A \\mathbf{x} = \\begin{bmatrix} 3 &amp; 4 &amp; 2 &amp; -3 &amp; 8 \\\\ 6 &amp; 8 &amp; 3 &amp; 4 &amp; 2 \\\\ 12 &amp; 16 &amp; 7 &amp; -2 &amp; 18 \\end{bmatrix} = \\begin{bmatrix} 5 \\\\ 9 \\\\ 0\\end{bmatrix} = \\mathbf{b}\\] <p>Let's use augmented matrix and perform elimination:</p> \\[\\begin{bmatrix} A &amp; \\mathbf{b}\\end{bmatrix} =  \\begin{bmatrix} \\mathbf{3} &amp; 4 &amp; 2 &amp; -3 &amp; 8 &amp; \\vdots &amp; 5 \\\\ 6 &amp; 8 &amp; 3 &amp; 4 &amp; 2 &amp; \\vdots &amp; 9 \\\\ 12 &amp; 16 &amp; 7 &amp; -2 &amp; 18 &amp; \\vdots &amp; 0\\end{bmatrix}\\] \\[\\begin{align}\\text{row}_2 \\leftarrow \\text{row}_2 - 2\\text{row}_1\\\\ \\text{row}_3 \\leftarrow \\text{row}_3 - 4\\text{row}_1\\\\ \\text{row}_1 \\leftarrow \\frac{\\text{row}_1}{3}\\\\ E_1 \\begin{bmatrix} A &amp; \\mathbf{b}\\end{bmatrix} =  \\begin{bmatrix} 1 &amp; \\frac{4}{3} &amp; \\frac{2}{3} &amp; -1 &amp; \\frac{8}{3} &amp; \\vdots &amp; \\frac{5}{3} \\\\ 0 &amp; 0 &amp; \\mathbf{-1} &amp; 10 &amp; -14 &amp; \\vdots &amp; -1 \\\\ 0 &amp; 0 &amp; -1 &amp; 10 &amp; -14 &amp; \\vdots &amp; -20\\end{bmatrix}\\end{align}\\] \\[\\begin{align}\\text{row}_3 \\leftarrow \\text{row}_3 - \\text{row}_2\\\\ \\text{row}_1 \\leftarrow \\text{row}_1 + \\frac{2}{3}\\text{row}_2\\\\ \\text{row}_2 \\leftarrow -1(\\text{row}_2)\\\\ E_2E_1 \\begin{bmatrix} A &amp; \\mathbf{b}\\end{bmatrix} = E \\begin{bmatrix} 1 &amp; \\frac{4}{3} &amp; 0 &amp; \\frac{17}{3} &amp; \\frac{-20}{3} &amp; \\vdots &amp; 1 \\\\ 0 &amp; 0 &amp; \\mathbf{1} &amp; -10 &amp; 14 &amp; \\vdots &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\vdots &amp; -19\\end{bmatrix} =  \\begin{bmatrix} R &amp; \\mathbf{d}\\end{bmatrix}\\end{align}\\] <p>Elimination done. Two pivots, hence rank \\(r=2\\). Last row is zero for \\(R\\), but the corresponding element for \\(\\mathbf{d}\\) isn't zero. Hence no solution.</p> <p>Let's take the same matrix with a different RHS.</p> \\[\\mathbf{b} = \\begin{bmatrix}7 \\\\ 12 \\\\26 \\end{bmatrix}\\] <p>Since the matrix is same, we will apply the same elimination to the new RHS.</p> \\[\\begin{align}\\mathbf{d} &amp;= E\\mathbf{b}\\\\ \\implies \\mathbf{d} &amp;= \\begin{bmatrix} 1 \\\\ 2 \\\\ 0 \\end{bmatrix}\\end{align}\\] <p>The new eliminated augmented matrix is:</p> \\[\\begin{bmatrix} R &amp; \\mathbf{d} \\end{bmatrix} = \\begin{bmatrix} 1 &amp; \\frac{4}{3} &amp; 0 &amp; \\frac{17}{3} &amp; \\frac{-20}{3} &amp; \\vdots &amp; 1 \\\\ 0 &amp; 0 &amp; \\mathbf{1} &amp; -10 &amp; 14 &amp; \\vdots &amp; 2 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\vdots &amp; 0\\end{bmatrix}\\] <p>Rank, as already discussed, was 2 and now the last \\(m-r\\) rows of \\(\\mathbf{d}\\) are also zero, so we will have solution(s). Now we need the null vector of \\(A\\) and a particular solution of this.</p> <p>Null vector is any linear combination of the specific solutions to \\(A\\mathbf{x} = 0\\). The specific solutions are easy to find with \\(R\\) form and in this example, we have three free columns and so three specific solutions.</p> <p>Null vector:</p> \\[\\mathbf{x_n} = c_1 \\begin{bmatrix}\\frac{-4}{3} \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0\\end{bmatrix} + c_2 \\begin{bmatrix}\\frac{-17}{3} \\\\ 0\\\\ 10 \\\\ 1 \\\\ 0\\end{bmatrix} + c_3 \\begin{bmatrix}\\frac{20}{3} \\\\ 0 \\\\ -14 \\\\ 0 \\\\ 1\\end{bmatrix}\\] <p>where \\(c_1, c_2, c_3\\) are scalar multpliers.</p> <p>Now we need a particular solution. We can have any one by setting arbitrary values to free variables and back substituting to find the whole. One interesting is to set zero for all free values. Let's try that. </p> <p>Set \\(x_2 = x_4 = x_5 = 0\\).</p> <p>Backsubstitute, we get \\(x_3 = 2\\), \\(x_1 = \\frac{5}{3}\\).</p> <p>If you did the backsubstitution along, you would have certainly found that those values came directly from \\(\\mathbf{d}\\). Set the free variables to be zero and the pivot variables come from \\(\\mathbf{d}\\). Keep this in the back of your mind, it will come back later.</p> <p>So our particular solution is:</p> \\[x_p = \\begin{bmatrix}1 \\\\ 0 \\\\ 2 \\\\ 0 \\\\ 0 \\end{bmatrix}\\] <p>Now the whole set of solutions is:</p> \\[\\begin{align}\\mathbf{x} &amp;= \\mathbf{x_p} + \\mathbf{x_n} \\\\ \\implies \\mathbf{x} &amp; = \\begin{bmatrix}1 \\\\ 0 \\\\ 2 \\\\ 0 \\\\ 0 \\end{bmatrix} + c_1 \\begin{bmatrix}\\frac{-4}{3} \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0\\end{bmatrix} + c_2 \\begin{bmatrix}\\frac{-17}{3} \\\\ 0\\\\ 10 \\\\ 1 \\\\ 0\\end{bmatrix} + c_3 \\begin{bmatrix}\\frac{20}{3} \\\\ 0 \\\\ -14 \\\\ 0 \\\\ 1\\end{bmatrix}; \\qquad \\forall c_1,c_2,c_3 \\in R \\end{align}\\] <p>This is the complete solution of this example. Both parts of our solution, \\(\\mathbf{x_p}\\) and \\(\\mathbf{x_n}\\), came from the same Reduced augmented matrix.</p> <p>Does the set of all the solutions to \\(A\\mathbf{x} = \\mathbf{b}\\) for any \\(A\\) and \\(\\mathbf{x}\\) form a subspace? </p> <p>I highly recommend to practice on a few different forms of questions to get the hang of this.</p>"},{"location":"linear_algebra/vector-spaces-and-subspaces/#invertibility","title":"Invertibility","text":"<p>A matrix \\(A\\) is said to be invertible if there exists another matrix \\(A^{-1}\\) such that:</p> \\[ AA^{-1} = A^{-1}A = I\\] <p>The fact that \\(AA^{-1} = A^{-1}A\\), implies that both these matrices have to be square and of same size. So rectangular matrices cannot have inverses. </p> <p>Also, each invertible matrix has a unique inverse. It has a simple proof:</p> <p>Let's say \\(B\\) and \\(C\\) are two inverses of \\(A\\), then:</p> \\[B = BI = B(AC) = (BA)C = IC = C\\] <p>Now we have to find an inverse of a matrix, we can formalize this problem as:</p> \\[AX = I\\] <p>Where \\(X\\) is a matrix and not a vector, we have to find this matrix, if it exists. But we know that this \\(X\\) is unique for this \\(A\\) and \\(X\\) is a square matrix of the same size as \\(A\\).</p> <p>Let's have an example for a \\(3 \\times 3\\) marix.</p> \\[A = \\begin{bmatrix} 1 &amp; 5 &amp; 3 \\\\ 2 &amp; 9 &amp; 4 \\\\ 5 &amp; 7 &amp; 1 \\end{bmatrix}\\] <p>We need to find its inverse(if it exists!).</p> \\[AX = I \\\\ \\begin{bmatrix} 1 &amp; 5 &amp; 3 \\\\ 2 &amp; 9 &amp; 4 \\\\ 5 &amp; 7 &amp; 1 \\end{bmatrix} \\ \\begin{bmatrix}x_{11} &amp; x_{12} &amp; x_{13} \\\\ x_{21} &amp; x_{22} &amp; x_{23} \\\\ x_{31} &amp; x_{32} &amp; x_{33} \\end{bmatrix} = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}\\] <p>This above problem can be broken into three equations:</p> \\[A\\ \\begin{bmatrix} x_{11} \\\\ x_{21} \\\\ x_{31} \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix},\\] \\[A\\ \\begin{bmatrix} x_{12} \\\\ x_{22} \\\\ x_{32} \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}\\] <p>and </p> \\[A\\ \\begin{bmatrix} x_{13} \\\\ x_{23} \\\\ x_{33} \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}\\] <p>Since the matrix \\(X\\) has to be unique, each of these above equations should have just one unique solution, which means the nullspace should only have the zero vector and so the final solution should be only the particular solution.</p> \\[\\begin{align}\\mathbf{x} &amp;= \\mathbf{x_p} + 0 \\leftarrow \\text{(zero vector of null space)}\\\\ \\implies \\mathbf{x} &amp;= \\mathbf{x_p}\\end{align}\\] <p>For just a unique solution, there should be no free columns, so the rank of the matrix should be equal to it's number of coulmns and since it is a square matrix, the rank is also equal to the number of rows, eliminating the possibility of zero rows in reduced form which can lead to no solution. </p> <p>So a matrix is invertible, iff it is a square full ranked matrix.</p> <p>To check if a matrix is invertible, we simply check its rank, by reducing and counting the pivots. We can also reduce it to its reduced row echelon form and if that form ends of like an identity matrix, then it is an invertible matrix. </p> <p>Now we know which matrix is an invertible matrix. But what is the inverse of that matrix? And how to find it?</p> <p>To find the inverse we simply have to solve the above equations and find the values of elements of matrix \\(X\\).</p> <p>Now we know we have just one unique solution which is also our particular solution to each of the above equations. While looking for the particular solution to a linear system, we used to set the free variables to be zero and the values of pivot variables came from \\(\\mathbf{d}\\).</p> <p>Since in our form there are no free variables, the whole particular solution is equal to the \\(\\mathbf{d}\\), i.e:</p> \\[\\mathbf{x_p} = \\mathbf{d}\\] <p>So we just reduce our RHS and find the values.</p> <p>Let's recall our equations:</p> \\[\\begin{bmatrix} 1 &amp; 5 &amp; 3 \\\\ 2 &amp; 9 &amp; 4 \\\\ 5 &amp; 7 &amp; 1 \\end{bmatrix}\\ \\begin{bmatrix} x_{11} \\\\ x_{21} \\\\ x_{31} \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix},\\] \\[\\begin{bmatrix} 1 &amp; 5 &amp; 3 \\\\ 2 &amp; 9 &amp; 4 \\\\ 5 &amp; 7 &amp; 1 \\end{bmatrix}\\ \\begin{bmatrix} x_{12} \\\\ x_{22} \\\\ x_{32} \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}\\] <p>and </p> \\[\\begin{bmatrix} 1 &amp; 5 &amp; 3 \\\\ 2 &amp; 9 &amp; 4 \\\\ 5 &amp; 7 &amp; 1 \\end{bmatrix}\\ \\begin{bmatrix} x_{13} \\\\ x_{23} \\\\ x_{33} \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}\\] <p>Solving the first equation, we will augment the matrix and the RHS and then reduce, the resulting \\(\\mathbf{d}\\) will be the solution for the first column of the inverse matrix. </p> \\[\\begin{bmatrix} 1 &amp; 5 &amp; 3 &amp; \\vdots &amp; 1 \\\\ 2 &amp; 9 &amp; 4 &amp; \\vdots &amp; 0\\\\ 5 &amp; 7 &amp; 1 &amp; \\vdots &amp; 0\\end{bmatrix}\\] \\[\\begin{align}\\text{row}_2 \\leftarrow \\text{row}_2 - 2 \\text{row}_1\\\\ \\text{row}_3 \\leftarrow \\text{row}_3 - 5 \\text{row}_1\\\\ \\begin{bmatrix} 1 &amp; 5 &amp; 3 &amp; \\vdots &amp; 1 \\\\ 0 &amp; -1 &amp; -2 &amp; \\vdots &amp; -2\\\\ 0 &amp; -18 &amp; -14 &amp; \\vdots &amp; -5\\end{bmatrix}\\end{align}\\] \\[\\begin{align}\\text{row}_3 \\leftarrow \\text{row}_3 - 18 \\text{row}_2\\\\ \\text{row}_1 \\leftarrow \\text{row}_1 + 5 \\text{row}_2\\\\ \\text{row}_2 \\leftarrow -1(\\text{row}_2)\\\\ \\begin{bmatrix} 1 &amp; 0 &amp; -7 &amp; \\vdots &amp; -9 \\\\ 0 &amp; 1 &amp; 2 &amp; \\vdots &amp; 2\\\\ 0 &amp; 0 &amp; 22 &amp; \\vdots &amp; 31\\end{bmatrix}\\end{align}\\] \\[\\begin{align}\\text{row}_1 \\leftarrow \\text{row}_1 + \\frac{7}{22} \\text{row}_3\\\\ \\text{row}_2 \\leftarrow \\text{row}_2 - \\frac{2}{22} \\text{row}_3\\\\ \\text{row}_3 \\leftarrow \\frac{\\text{row}_3}{22}\\\\ \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; \\vdots &amp; \\frac{19}{22} \\\\ 0 &amp; 1 &amp; 0 &amp; \\vdots &amp; \\frac{-9}{11}\\\\ 0 &amp; 0 &amp; 1 &amp; \\vdots &amp; \\frac{31}{22}\\end{bmatrix}\\end{align}\\] <p>And </p> \\[\\mathbf{d} = \\begin{bmatrix} \\frac{19}{22} \\\\ \\frac{-9}{11} \\\\ \\frac{31}{22}\\end{bmatrix} = \\mathbf{x_p} = \\mathbf{x}\\] <p>This is the first column of the inverse matrix of \\(A\\). We can do the same with other two equations, but instead of augmenting these RHS's one at a time and reducing, we can augment all of them and reduce all of the RHS at the same time.</p> <p>We augment the matrix as:</p> \\[\\begin{bmatrix} A &amp; I\\end{bmatrix}\\] <p>then reduce to:</p> \\[\\begin{bmatrix} I &amp; X \\end{bmatrix}\\] <p>\\(X\\) is the inverse of \\(A\\).</p> <p>It can also be seen as:</p> \\[E\\begin{bmatrix} A &amp; I\\end{bmatrix} = \\begin{bmatrix} I &amp; X \\end{bmatrix}\\] <p>Which means \\(EA = I\\) and \\(EI= X\\), So \\(E\\) is the inverse of \\(A\\) and \\(X= E\\).</p> <p>This method of finding the inverse is called the Gauss-Jordan method of finding the inverse.</p> <p>Let's finish our current example and then look at one more example to conclude this topic.</p> <p>Let's augment:</p> \\[\\begin{bmatrix} 1 &amp; 5 &amp; 3 &amp; \\vdots &amp; 1 &amp; 0 &amp; 0 \\\\ 2 &amp; 9 &amp; 4 &amp; \\vdots &amp; 0 &amp; 1 &amp; 0\\\\ 5 &amp; 7 &amp; 1 &amp; \\vdots &amp; 0 &amp; 0 &amp; 1\\end{bmatrix}\\] \\[\\begin{align}\\text{row}_2 \\leftarrow \\text{row}_2 - 2 \\text{row}_1\\\\ \\text{row}_3 \\leftarrow \\text{row}_3 - 5 \\text{row}_1\\\\ \\begin{bmatrix} 1 &amp; 5 &amp; 3 &amp; \\vdots &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; -1 &amp; -2 &amp; \\vdots &amp; -2 &amp; 1 &amp; 0\\\\ 0 &amp; -18 &amp; -14 &amp; \\vdots &amp; -5 &amp; 0 &amp; 1\\end{bmatrix}\\end{align}\\] \\[\\begin{align}\\text{row}_3 \\leftarrow \\text{row}_3 - 18 \\text{row}_2\\\\ \\text{row}_1 \\leftarrow \\text{row}_1 + 5 \\text{row}_2\\\\ \\text{row}_2 \\leftarrow -1(\\text{row}_2)\\\\ \\begin{bmatrix} 1 &amp; 0 &amp; -7 &amp; \\vdots &amp; -9 &amp; 5 &amp; 0 \\\\ 0 &amp; 1 &amp; 2 &amp; \\vdots &amp; 2 &amp; -1 &amp; 0\\\\ 0 &amp; 0 &amp; 22 &amp; \\vdots &amp; 31 &amp; -18 &amp; 1\\end{bmatrix}\\end{align}\\] \\[\\begin{align}\\text{row}_1 \\leftarrow \\text{row}_1 + \\frac{7}{22} \\text{row}_3\\\\ \\text{row}_2 \\leftarrow \\text{row}_2 - \\frac{2}{22} \\text{row}_3\\\\ \\text{row}_3 \\leftarrow \\frac{\\text{row}_3}{22}\\\\ \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; \\vdots &amp; \\frac{19}{22} &amp; \\frac{-8}{11} &amp; \\frac{7}{22} \\\\ 0 &amp; 1 &amp; 0 &amp; \\vdots &amp; \\frac{-9}{11} &amp; \\frac{7}{11} &amp; \\frac{-1}{11}\\\\ 0 &amp; 0 &amp; 1 &amp; \\vdots &amp; \\frac{31}{22} &amp; \\frac{-9}{11} &amp; \\frac{1}{22}\\end{bmatrix}\\end{align}\\] <p>So,</p> \\[A^{-1} = \\begin{bmatrix}  \\frac{19}{22} &amp; \\frac{-8}{11} &amp; \\frac{7}{22} \\\\  \\frac{-9}{11} &amp; \\frac{7}{11} &amp; \\frac{-1}{11}\\\\  \\frac{31}{22} &amp; \\frac{-9}{11} &amp; \\frac{1}{22}\\end{bmatrix}\\] <pre><code>A = [1 5 3; 2 9 4; 5 7 1]\ninv_A = [19/22 -8/11 7/22; -9/11 7/11 -1/11; 31/22 -9/11 1/22]\nA*inv_A\n</code></pre> <pre><code>3\u00d73 Array{Float64,2}:\n 1.0  -4.44089e-16  -5.55112e-17\n 0.0   1.0          -5.55112e-17\n 0.0  -7.77156e-16   1.0\n</code></pre> <pre><code>inv_A \n</code></pre> <pre><code>3\u00d73 Array{Float64,2}:\n  0.863636  -0.727273   0.318182 \n -0.818182   0.636364  -0.0909091\n  1.40909   -0.818182   0.0454545\n</code></pre> <p>We can also find the inverse in julia using:</p> <pre><code>inv(A)\n</code></pre> <pre><code>3\u00d73 Array{Float64,2}:\n  0.863636  -0.727273   0.318182 \n -0.818182   0.636364  -0.0909091\n  1.40909   -0.818182   0.0454545\n</code></pre> <p>Let's look at one more example to clear this procedure.</p> <p>Let's say our matrix,</p> \\[A = \\begin{bmatrix} 2 &amp; 6 &amp; 8 &amp; 0 \\\\ 9 &amp; 3 &amp; 18 &amp; 6 \\\\ 2 &amp; 0 &amp; 0 &amp; 7 \\\\ 11 &amp; 9 &amp; 26 &amp; 6 \\end{bmatrix}\\] <p>To find the inverse, let's augment \\(A\\) and \\(I\\),</p> \\[\\begin{bmatrix} A &amp; I \\end{bmatrix} = \\begin{bmatrix} 2 &amp; 6 &amp; 8 &amp; 0 &amp; \\vdots &amp; 1 &amp; 0 &amp; 0 &amp; 0\\\\ 9 &amp; 3 &amp; 18 &amp; 6 &amp; \\vdots &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 2 &amp; 0 &amp; 0 &amp; 7 &amp; \\vdots &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 11 &amp; 9 &amp; 26 &amp; 6 &amp; \\vdots &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix}\\] <p>Elimiating,</p> \\[E_1\\begin{bmatrix} A &amp; I \\end{bmatrix} = \\begin{bmatrix} 1 &amp; 3 &amp; 4 &amp; 0 &amp; \\vdots &amp; \\frac{1}{2} &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; -24 &amp; -18 &amp; 6 &amp; \\vdots &amp; \\frac{-9}{2} &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; -6 &amp; -8 &amp; 7 &amp; \\vdots &amp; -1 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; -24 &amp; -18 &amp; 6 &amp; \\vdots &amp; \\frac{-5}{2} &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix}\\] \\[E_2E_1\\begin{bmatrix} A &amp; I \\end{bmatrix} = \\begin{bmatrix} 1 &amp; 0 &amp; \\frac{7}{4} &amp; \\frac{3}{4} &amp; \\vdots &amp; \\frac{-1}{16} &amp; \\frac{3}{24} &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; \\frac{3}{4} &amp; \\frac{-1}{4} &amp; \\vdots &amp; \\frac{9}{48} &amp; \\frac{-1}{24} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\frac{-7}{2} &amp; \\frac{11}{2} &amp; \\vdots &amp; \\frac{1}{8} &amp; \\frac{-1}{4} &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\vdots &amp; -1 &amp; -1 &amp; 0 &amp; 1 \\end{bmatrix}\\] <p>We got a row of zeros for the matrix, which means this matrix is not full row ranked and so not full ranked at all. And so the inverse of this matrix is not possible.</p> <p>I think this is a good stopping point. I would recommend practicing for everything that we have covered here. Next up we will try to explain Independence, basis and dimensions of subspaces.</p> <p>{% include linalg/2.spaces/plot1.html %}</p>"},{"location":"linear_algebra/vectors-linear-combinations/","title":"Vectors, Linear Combinations, Eliminations","text":"","tags":["vectors","elimination matrix","permutation matrix","linear combinations"]},{"location":"linear_algebra/vectors-linear-combinations/#introduction","title":"Introduction","text":"<p>The whole field of linear algebra, as the name suggests, is based on linear combinations of different \"things\". We will get to know what these \"things\" are. How can we represent the different combinations and what these different combinations represent, if they represent anything. We will basically work with vectors and then with matrices. So let's begin. </p>","tags":["vectors","elimination matrix","permutation matrix","linear combinations"]},{"location":"linear_algebra/vectors-linear-combinations/#vectors","title":"Vectors","text":"","tags":["vectors","elimination matrix","permutation matrix","linear combinations"]},{"location":"linear_algebra/vectors-linear-combinations/#introduction_1","title":"Introduction","text":"<p>A vector is, simply put, a data holding structure. It can hold entries of data. We can store values of a specific feature in a vector. We can store coordinates of any point in a n-dimensional space. </p> <p>The specific function of a vector holding coordinates of a point is used the most. The same data can represent an arrow from origin to the point stored in the vector(the definition physicists usually identify vectors by).</p> <p>So a vector can represent just \\(n\\) numbers(data), Arrow from origin, or a point in a space. The vector is said to be \\(n\\)-dimensional.</p>","tags":["vectors","elimination matrix","permutation matrix","linear combinations"]},{"location":"linear_algebra/vectors-linear-combinations/#representation-of-a-vector","title":"Representation of a Vector","text":"<p>How do we represent vectors? There are basically two ways to do it. A column vector and a row vector. In a column vector, we stack all the numbers in a single vertical fashion and in a row vector, we stack them in horizontal fashion. While both are fine and can be converted from one form to another, the column vector is conventionally used.</p> <p>e.g: a vector containing 2 and 3 is represented as:</p> \\[ \\mathbf{a} = \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix}\\] <p>This vector corresponds to point \\((2,3)\\).</p> <p>A column vector(or a row vector) can also be represented by paranthesis. So the above vector can also be \\((2,3)\\). It does not mean it is a row vector. It is actually a column vector(in this case), because we were using column vector. It is just easier to write it.</p> <p>A 3-dimensional column vector \\(\\begin{bmatrix} x  \\\\ y  \\\\ z \\end{bmatrix}\\) can also be written as \\(( x,y,z)\\) and it sill is column vector. A row vector will be \\(\\begin{bmatrix} x &amp; y &amp; z \\end{bmatrix}\\).</p> <p>For now we will primarily represent vectors as columns, unless specified otherwise.</p> <p>We can do it in a simple way in julia.</p> <pre><code>vector = [1, 2, 3]\n</code></pre> <pre><code>3-element Array{Int64,1}:\n 1\n 2\n 3\n</code></pre> <p>It can be also created like:</p> <pre><code>vector = [1; 2; 3]\n</code></pre> <pre><code>3-element Array{Int64,1}:\n 1\n 2\n 3\n</code></pre> <pre><code>vector = [1\n          2\n          3]\n</code></pre> <pre><code>3-element Array{Int64,1}:\n 1\n 2\n 3\n</code></pre>","tags":["vectors","elimination matrix","permutation matrix","linear combinations"]},{"location":"linear_algebra/vectors-linear-combinations/#operations-on-vectors","title":"Operations on Vectors","text":"<p>In linear algebra, we work with two important operations, multiplication by a scalar and addition.</p> <p>When we multiply a vector by a scalar, all the values in the vector (called the components) are multiplied by the same number.</p> <p>e.g:</p> \\[  c\\begin{bmatrix} 1 \\\\ 2 \\\\ 3\\end{bmatrix} = \\begin{bmatrix} c \\\\ 2c \\\\ 3c\\end{bmatrix}\\] <p>We can add two vectors if their dimensions are same. The resultant vector is the vector of corresponding sums of components of two vectors.</p> <p>e.g:</p> \\[  \\begin{bmatrix} 1 \\\\2 \\\\3 \\end{bmatrix} + \\begin{bmatrix} 6 \\\\7\\\\8\\end{bmatrix} = \\begin{bmatrix} 7 \\\\ 9 \\\\11 \\end{bmatrix}\\] <p>If you are familiar with vectors from basic physics, you know the addition of two vectors is the third side of a triangle formed by placing the tail of one vector at the head of the other vector.</p> <p>If we combine the above two operations on any number of vectors, i.e we multiply each vector by a scalar and then add all the vectors, we get a linear combination of those vectors.</p> <p>So for any two vectors, \\(\\mathbf{w}\\) and\\(\\mathbf{v}\\), a linear combination is:</p> \\[  c \\mathbf{w} + d\\mathbf{v}\\] <p>where \\(c,d\\) are scalars. </p> <p>for any \\(n\\) vectors, \\(\\mathbf{v}_1,\\mathbf{v}_2,\\dots\\) a linear combination will be:</p> \\[  c_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2 + \\cdots + c_n\\mathbf{v}_n \\] <p>We will talk about what all these linear combinations for all \\(c\\) and \\(d\\) represent, later.</p> <p>Example in julia for three vectors with scalars being 2,3,-1.</p> <pre><code>v1 = [1;2;3;4]\nv2 = [4;3;5;1]\nv3 = [22,45,0,1]\nc1,c2,c3 = 2,3,-1\nc1*v1 + c2*v2 + c3*v3\n</code></pre> <pre><code>4-element Array{Int64,1}:\n  -8\n -32\n  21\n  10\n</code></pre> <p>Besides the two important operations in linear algebra, we can do other operations on vectors.</p> <p>Dot Product: We can multiply two vectors to get a scalar. It is called a dot product. It can occur between two vectors of same dimensions. We just multiply the corresponding elements of the two vectors and sum up the products.</p> <p>e.g:</p> \\[ \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} \\cdot \\begin{bmatrix} 4 \\\\ 5\\end{bmatrix} = (1)(4)+(2)(5) = 14\\] <p>We can achive this in julia as:</p> <pre><code>using LinearAlgebra\n</code></pre> <pre><code>v1 = (1,2);\nv2 = (4,5);\n\ndot(v1,v2)\n</code></pre> <pre><code>14\n</code></pre> <p>It can be also calculated as:</p> \\[ \\mathbf{v_1} \\cdot \\mathbf{v_2} = \\mathbf{v_1}^T \\mathbf{v_2}\\] <p>\\(\\mathbf{v_1}^T \\mathbf{v_2}\\) is also called the inner product and the \\(\\mathbf{v_1} \\mathbf{v2}^T\\) is called the outer product. The outer product produces a matrix.</p> <p>For two vectors, \\(\\mathbf{w}\\) and \\(\\mathbf{v}\\), the dot product is also calculated as:</p> \\[  \\mathbf{w} \\cdot \\mathbf{v} = \\lVert \\mathbf{w} \\rVert \\lVert \\mathbf{v} \\rVert \\cos \\theta \\] <p>where \\(\\lVert \\mathbf{x} \\rVert\\) is the length of vector \\(\\mathbf{x}\\) and \\(\\theta\\) is the angle between the two vectors.</p> <p>Because of the cosine, the dot prouct of perpendicular vectors is zero. Also dot product of a vector with itself gives the square of its length.</p> <p>Since \\(\\cos \\theta \\leq 1\\),</p> \\[  \\lvert \\mathbf{w} \\cdot \\mathbf{v} \\rvert \\leq \\lVert \\mathbf{w} \\rVert \\lVert \\mathbf{v} \\rVert \\] <p>It is the Cauchy-Schwarz-Buniakowsky inequality.</p>","tags":["vectors","elimination matrix","permutation matrix","linear combinations"]},{"location":"linear_algebra/vectors-linear-combinations/#matrices","title":"Matrices","text":"<p>As we have established, we save data in vectors. Now we work with multiple vectors (for their linear combinations of course!). We will save multiple vectors in a matrix. Let's do some examples.</p> <p>Let's say we have three vectors:</p> \\[  \\mathbf{u} = \\begin{bmatrix} 1 \\\\ -1 \\\\0\\end{bmatrix} \\quad ; \\quad \\mathbf{v} = \\begin{bmatrix} 0 \\\\ 1 \\\\-1\\end{bmatrix} \\quad ;\\quad \\mathbf{w} = \\begin{bmatrix} 0 \\\\ 0 \\\\1\\end{bmatrix}\\] <p>Their linear combination, \\(\\mathbf{b}\\) for scalars \\(c_1\\), \\(c_2\\) and \\(c_3\\)  is:</p> \\[ \\mathbf{b} = c_1\\mathbf{u} + c_2 \\mathbf{v} + c_3 \\mathbf{w} = c_1\\begin{bmatrix} 1 \\\\ -1 \\\\0\\end{bmatrix} + c_2 \\begin{bmatrix} 0 \\\\ 1 \\\\-1\\end{bmatrix} + c_3 \\begin{bmatrix} 0 \\\\ 0 \\\\1\\end{bmatrix} = \\begin{bmatrix} c_1 \\\\ c_2 - c_1 \\\\ c_3- c_2 \\end{bmatrix}\\] <p>Now if we use all our vectors as columns of a matrix, \\(A\\), that matrix multiplies the vector \\(\\mathbf{c} = (c_1,c_2,c_3)\\):</p> \\[  A\\mathbf{c} = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ -1 &amp; 1 &amp; 0 \\\\ 0 &amp; -1 &amp; 1 \\end{bmatrix}\\ \\begin{bmatrix}c_1\\\\ c_2 \\\\ c_3 \\end{bmatrix} = \\begin{bmatrix} c_1 \\\\ c_2 - c_1 \\\\ c_3- c_2 \\end{bmatrix} = \\begin{bmatrix} b_1 \\\\ b_2 \\\\ b_3 \\end{bmatrix} = \\mathbf{b} \\] <p>Now instead of being just numbers, \\(c_1,c_2,c_3\\) are now also forming a vector. This gives a new view-point to look at it. The output vector, \\(A\\mathbf{c}\\)(or \\(\\mathbf{b}\\)) is a combination of the columns of \\(A\\).</p> <p>It can also be thought as the matrix \\(A\\) above, acts on the vector \\(\\mathbf{c}\\) and transforms into vector \\(\\mathbf{b}\\). This specific matrix \\(A\\) above is a \"difference matrix\" because \\(\\mathbf{b}\\) contains differences of vector \\(\\mathbf{c}\\). The vector \\(\\mathbf{c}\\) is the input and the output is \\(\\mathbf{b}\\). The top difference is \\(c_1-c_0 = c_1 -0\\)</p>","tags":["vectors","elimination matrix","permutation matrix","linear combinations"]},{"location":"linear_algebra/vectors-linear-combinations/#linear-equations","title":"Linear Equations","text":"","tags":["vectors","elimination matrix","permutation matrix","linear combinations"]},{"location":"linear_algebra/vectors-linear-combinations/#introduction_2","title":"Introduction","text":"<p>Let's change the view again. Upto now, the vector \\(\\mathbf{c}\\) was known. The right hand side \\(\\mathbf{b}\\) was unknown. Now we think of \\(\\mathbf{b}\\) as known and look for \\(\\mathbf{c}\\). </p> <p>We were earlier asking to compute the linear combination of \\(c_1\\mathbf{u} + c_2 \\mathbf{v} + c_3 \\mathbf{w}\\) to find \\(\\mathbf{b}\\). Now we are asking which combination of \\(\\mathbf{u,v,w}\\) produces a particular vector \\(\\mathbf{b}\\)?</p> <p>From now on we will call the unknown vector \\(\\mathbf{x}\\) instead of \\(\\mathbf{c}\\), because duh!.</p> <p>This problem is the inverse problem i.e to find the input vector \\(\\mathbf{x}\\) that gives the desired output \\(\\mathbf{b}= A\\mathbf{x}\\).</p> <p>Equations: For our previous matrix, \\(A\\),</p> \\[ A\\mathbf{x} = \\mathbf{b}\\\\ \\implies \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ -1 &amp; 1 &amp; 0 \\\\ 0 &amp; -1 &amp; 1 \\end{bmatrix}\\ \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} = \\begin{bmatrix} b_1 \\\\ b_2 \\\\ b_3 \\end{bmatrix}\\] Equations Solutions \\(x_1 = b_1\\) \\(x_1 = b_1\\) \\(-x_1 +x_2 = b_2\\) \\(x_2 = b_1 + b_2\\) \\(-x_2 +x_3 = b_3\\) \\(x_3 = b_1 + b_2 + b_3\\) <p>So the solution vector \\(\\mathbf{x}\\), for the above equation is:</p> \\[ \\begin{align}\\mathbf{x} &amp;= \\begin{bmatrix} b_1 \\\\ b_1 + b_2 \\\\ b_1 + b_2 + b_3 \\end{bmatrix} \\\\ &amp;= \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 1 &amp; 1 \\end{bmatrix}\\ \\begin{bmatrix} b_1 \\\\ b_2 \\\\ b_3 \\end{bmatrix}\\end{align}\\] <p>Now let's compare the effects on this vector with \\(A\\). \\(A\\) acted on a vector and gave the \"differences\" of the vector elements and this new matrix gives the \"sums\" of the elements of the vector it acts on. The new matrix is the inverse of the original \\(A\\) and is shown as \\(A^{-1}\\).</p> <p>So if,</p> \\[ \\begin{align} A\\mathbf{x} &amp;= \\mathbf{b} \\\\ \\implies \\mathbf{x} &amp;= A^{-1}\\mathbf{b}\\end{align}\\] <p>This was the solution for this problem because the matrix \\(A\\) was invertible(we will talk more about it later). </p> <p>Let's talk about when there is more or less than one solution.</p> <p>Let's say we have three equations:</p> \\[  x_1 - x_3 = b_1\\] \\[  x_2 - x_1 = b_2\\] \\[  x_3 - x_2 = b_3\\] <p>Now if \\(b_1 = b_2 = b_3 = 0\\), then</p> <p>\\(\\begin{bmatrix} x_1 - x_3 \\\\ x_2 - x_1 \\\\ x_3 - x_2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}\\) is solved by all vectors \\(\\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} = \\begin{bmatrix} k \\\\ k \\\\ k \\end{bmatrix}\\)</p> <p>where \\(k\\) is any constant.</p> <p>Now if, say, \\(b_1 =1, b_2 =1 , b_3 =1\\), then:</p> <p>\\(\\begin{bmatrix} x_1 - x_3 \\\\ x_2 - x_1 \\\\ x_3 - x_2 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}\\) has no solution. Left side sums upto 0 and right side to 9. </p> <p>But Let's open the problem in the form \\(A\\mathbf{x} = \\mathbf{b}\\),</p> \\[  A\\mathbf{x} = \\begin{bmatrix} 1 &amp; 0 &amp; -1 \\\\ -1 &amp; 1 &amp; 0 \\\\ 0 &amp; -1 &amp; 1 \\end{bmatrix}\\  \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}= \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} = \\mathbf{b} \\] <p>Now our question transforms into finding the linear combination of columns of \\(A\\) that is equal to \\(\\mathbf{b}\\).</p> <p>Now if we think about it geometrically, no combination of the columns will the vector \\(\\mathbf{b} =(1,1,1)\\). The combinations(of columns) don't fill up the whole three-dimensional space but a plane given by \\(x + y + z =0\\)</p> <pre><code>A = [1 0 -1; -1 1 0; 0 -1 1]\nb=[1; 1; 1];\n</code></pre> <pre><code>using Plotly\n</code></pre> <pre><code>xx= [j for i=-0.7:0.01:0.7,j=-0.7:0.01:0.7]\nyy = xx';\nz = -xx - yy\n\ntrace1 = scatter3d(x=[0, A[1,1]], y=[0, A[2,1]], z=[0, A[3,1]], mode=\"lines\", line=attr(width=5), name=\"vector 1\")\ntrace2 = scatter3d(x=[0, A[1,2]], y=[0, A[2,2]], z=[0,A[3,2]], mode=\"lines\", line=attr(width=5), name=\"vector 2\")\ntrace3 = scatter3d(x=[0, A[1,3]], y=[0, A[2,3]], z=[0,A[3,3]], mode=\"lines\", line=attr(width=5), name=\"vector 3\")\ntrace4 = scatter3d(x=[0, b[1]], y=[0, b[2]], z=[0,b[3]], mode=\"lines\", line=attr(width=5), name=\"vector b\")\ntrace5 = surface(x=xx, y=yy, z=z, opacity=0.8, showscale=false, name=\"Plane\")\n\n\nplot([trace1,trace2,trace3,trace4,trace5], Layout(scene_camera=attr(eye=attr(x=1, y=1.2, z=1))))\n</code></pre> <p>Here \\(A\\mathbf{x}\\) represents all the linear combination of columns (for all values of \\(\\mathbf{x}\\), of course!). Now the sum of two vectors in same direction will have all its linear combinations in that direction. Similarly two vectors in different directions will have all their linear combinations in the plane defined by these two vectors. Now for three vectors, any two vectors will form a plane and if the third vector is not in that plane, then all three vectors are said to be independent and their linear combinations will fill up the whole three dimensional space. But if the third vector is a linear combination of the first two (i.e, it is in the same plane), then the vectors are said to be dependent and the third vector does not contribute anything new and so the result will be a plane.</p> <p>Now in our matrix, the third column is a linear combination of the first two and hence the linear combination of these three vectors can only form a plane and we would have a solution if the vector \\(\\mathbf{b}\\) was in that plane. And since the vector \\((1,1,1)\\) is not in plane determined by the columns of the matrix, this equation has no solution. See Figure above.</p> <p>If all this seems a bit too fast or shallow, don't worry we will cover it in detail later.</p>","tags":["vectors","elimination matrix","permutation matrix","linear combinations"]},{"location":"linear_algebra/vectors-linear-combinations/#solving-linear-equations","title":"Solving Linear Equations","text":"","tags":["vectors","elimination matrix","permutation matrix","linear combinations"]},{"location":"linear_algebra/vectors-linear-combinations/#introduction_3","title":"Introduction","text":"<p>Let's just simply start with solving two equations with two variables. A two variable equation represents a line in a two dimensional space and all the points on that line satisfy that equation.</p> <p>Now if we have two equations and we have to find solutions(s) that satisfy both these equations. Now based on the coefficients, a system of equations may have a unique solution, an infinite number of solution, or no solution.</p> <p>We have a unique solution, if both the lines intersect at one point and that point is the solution to the system. We have infinite solutions when both lines run over each other. We have no solution if the lines don't intersect at all(like parallel lines).</p> <p>Let's see an example of a unique solution case.</p> <p>Let's say we have two equations:</p> \\[  4x - y = 9 \\\\ 3x + 2y = 4\\] <p>The first equation \\(4x-y=9\\) produces a straight line in the \\(xy\\) plane. There are infinite points on it but a special point \\((2,-1)\\) also satisfies it and is on the line.</p> <p>The line by second equation \\(3x + 2y =4\\) is another line that also passes through \\((2, -1)\\) and so the solution is \\(x=2\\) and \\(y=-1\\).</p> <pre><code>x = [i for i=-5:1:5]\ny1 = 4*x .- 9\ny2 = (4 .- (3*x))/2\n\ntrace1 = scatter(x=x,y=y1, mode=\"lines\")\ntrace2 = scatter(x=x,y=y2, mode=\"lines\")\n\nplot([trace1, trace2], Layout(showlegend=false, annotations=[attr(x=2,\n            y=-1,\n            text=\"(2,-1)\")], title=\"Equations\", xaxis_title=\"x\", yaxis_title=\"y\"))\n</code></pre> <p>This was the row picture. Let's look at the column picture.</p> <p>We will recognize the same system as a vector system. Instead of using numbers, we will use vectors. It can be shown as:</p> \\[ x\\begin{bmatrix}4 \\\\3 \\end{bmatrix} + y \\begin{bmatrix}-1 \\\\2 \\end{bmatrix} = \\begin{bmatrix}9 \\\\4 \\end{bmatrix}\\] <p>Now the same problem has transformed into \"find the combination of those two vectors that equals the vector on the right. Choosing \\(x=2\\) and \\(y=-1\\) (same as before!), we will get the right result.</p> <p>Let's see how the column picture reveals the answer. Let's draw each vector, then the vector multipied and the vector on the right.</p> <pre><code>vec1 = [4;3]\nvec2 = [-1;2]\nb = [9;4]\n\ntrace1 = scatter(x=[0,vec2[1]], y=[0,vec2[2]],mode=\"lines\", name= \"vector 2\")\ntrace2 = scatter(x=[0,vec1[1]], y=[0,vec1[2]],mode=\"lines\", name= \"vector 1\")\ntrace3 = scatter(x=[0,b[1]], y=[0,b[2]],mode=\"lines\", name= \"b\", line=attr(color=\"red\"))\ntrace4 = scatter(x=[0,2*vec1[1]], y=[0,2*vec1[2]],mode=\"lines\", name= \"2 x vector 1\", line = attr(color=\"green\"), opacity=0.5)\ntrace5 = scatter(x=[0,-1*vec2[1]], y=[0,-1*vec2[2]],mode=\"lines\", name= \"-1 x vector 2\", line= attr(color=\"blue\"))\ntrace6 = scatter(x=[2*vec1[1],b[1]], y=[2*vec1[2], b[2]], mode=\"lines\", name= \"-1 x vector 2\", line = attr(color=\"blue\", dash=\"dash\"))\ntrace7 = scatter(x=[-1*vec2[1],b[1]], y=[-1*vec2[2], b[2]], mode=\"lines\",name= \"2 x vector 1\", line = attr(color=\"green\", dash=\"dash\"))\n\n\n\nplot([trace1,trace2,trace3, trace4,trace5,trace6,trace7],\n    Layout(xaxis=attr(scaleanchor=\"y\", scaleratio=1),  width=800))\n</code></pre> <p>Here you can see the combination of these vectors lead to the final vector \\(\\mathbf{b}\\). </p> \\[ 2\\begin{bmatrix} 4 \\\\3 \\end{bmatrix} -1\\begin{bmatrix} -1 \\\\2 \\end{bmatrix} = \\begin{bmatrix} 9 \\\\4 \\end{bmatrix}\\] <p>By the simple look of the eye the row picture looks better than the column picture. It's all in your right to choose, but I think it is easier to see a combinaton of four vectors in four dimensional space, than to visualize four hyper-planes meeting at a point.</p> <p>Same problem, different pictures, same solutions. We combine the problem into a matrix problem as:</p> \\[ A\\mathbf{x}= \\mathbf{b}\\\\ \\implies \\begin{bmatrix} 4 &amp; -1 \\\\ 3 &amp; 2\\ \\end{bmatrix}\\ \\begin{bmatrix} x \\\\ y \\end{bmatrix} = \\begin{bmatrix} 9 \\\\ 4 \\end{bmatrix}  \\] <p>The matrix, \\(A = \\begin{bmatrix} 4 &amp; -1 \\\\ 3 &amp; 2\\ \\end{bmatrix}\\) is called the coefficient matrix. Its rows give the row picture and its columns give you the column picture. And we can try it in julia like.</p> <pre><code>A = [ 4 -1; 3 2]\nx = [2;-1]\nA*x\n</code></pre> <pre><code>2-element Array{Int64,1}:\n 9\n 4\n</code></pre> <p>And it has given the same result as the vector \\(\\mathbf{b}\\).</p> <p>In  three-equations-three-variables, the row picture will be that every equation will repersent a plane in the 3-D world and the solution will be all the intersecting points of these planes.</p> <p>The column picture is three vectors (the vector of coefficients of \\(x\\) in all the equations, the vector of coefficients of \\(y\\) and that of \\(z\\)) lying in a 3-D space and their combination resulting in the vector \\(\\mathbf{b}\\).</p> <p>The matrix form represents these both pictures.</p>","tags":["vectors","elimination matrix","permutation matrix","linear combinations"]},{"location":"linear_algebra/vectors-linear-combinations/#elimination","title":"Elimination","text":"<p>Until now, we just guessed the right answer to a system of equations and just verified the solution. But let's now try to actually find the solution to a linear system.</p> <p>Let's start with 2 variables and 2 equations and return back to the system example we used before.</p> \\[  4x - y = 9 \\\\ 3x + 2y = 4\\] <p>Now, let's see we can get the answer \\((x,y) = (2,-1)\\).</p> <p>This equation is pretty easy to solve. I'm sure you can do as well. </p> <p>I would like to eliminate the \\(x\\)-part of the second equation in the system. So, I multiply the first equation by \\(\\frac{3}{4}\\) and then subtract it from the second equation. </p> <p>Resulting,</p> \\[  \\begin{align}4x &amp;- y = 9 \\\\ &amp;\\frac{11}{3}y = \\frac{-11}{3}\\end{align}\\] <p>The new equation gives \\(y = -1\\), quickly and substituting that in the first equation we get \\(x=2\\).</p> <p>That was simple and you have done it tons of times. But I would like to introduce some terms here and maybe even formalize this procedure.</p> <p>We used the the coefficient of \\(x\\) in the first equation to eliminate the first term in the second equation. This is called a pivot and the variable associated with it (here, \\(x\\)) is called a pivot variable.  </p> <p>The pivot can never be zero, as we cannot eliminate a non zero coefficient by it. No matter what we multiply it by we will always get a zero and that subtracted from other equation will not change them at all. </p> <p>The resulting system was in a \"upper triangular form\" (if we align the variables). The pivots always lie on the diagonal of this traingular form after completed elimination.</p> <p>We have a few more terminology to introduce. But let's look at a few  more cases of this system. The solution we got before was simple as the solution was unique.</p> <p>But if solution isn't unique, two cases arise: No solution and infinite solutions.</p> <p>Let's say the system is:</p> \\[ 3x + 4y = 7 \\\\ 6x + 8y = 3\\] <p>Let's do the simple elimination again. Multiply the first equation by 2 and subtract from the second one to get the triangluar form:</p> \\[ \\begin{align}3x + &amp;4y = 7 \\\\  &amp;0y = -11\\end{align}\\] <p>There is no solution \\(0y=-11\\). Usually we would divide -11 by second pivot, but here is no second pivot(Zeros are not pivots!) making it a no solution system. In the row form it is represented by two parallel lines. The column picture shows that the vector \\(\\begin{bmatrix} 3 \\\\6 \\end{bmatrix}\\) and the vector \\(\\begin{bmatrix} 4 \\\\8 \\end{bmatrix}\\) are in the same direction. And so all it's linear combinations will be restricted on a line along that direction. And the vector \\(\\mathbf{b} = \\begin{bmatrix} 7 \\\\3 \\end{bmatrix}\\) does not lie on that line and so no combination can do it. Hence no solution.</p> <p>For the infinite soution let's just change the vector \\(\\mathbf{b}\\) to a vector which is on that line, say \\(\\begin{bmatrix} 1 \\\\2 \\end{bmatrix}\\).</p> <p>So the equations are:</p> \\[ 3x + 4y = 1 \\\\ 6x + 8y = 2\\] <p>Eliminating..., we get:</p> \\[ \\begin{align}3x + &amp;4y = 7 \\\\  &amp;0y = 0\\end{align}\\] <p>Every \\(y\\) satisfies \\(0y=0\\). The \\(y\\) is said to be a \"free variable\", i.e we can choose \\(y\\) freely and \\(x\\) is then computed using the first equation. There is actually just one equation \\(3x + 4y =7\\). This also has just one pivot. </p> <p>In the row picture, both lines are the same. In the column picture the vector \\(\\mathbf{b} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}\\) lies on the line determined by the coefficient vectors. In fact it is one-third times the first column and it can also be defined as one-fourth times the second column and hence we can have an infinte combinations of these vectors that can end up on that vector \\(\\mathbf{b}\\).</p> <p>Sometimes we may have to exchange rows. It is because we want to have that traingular form, there might occur a Zero where there should be a pivot and we can exchange that row with the nearest row below which has a non-zero number at that place. e.g, a system is:</p> \\[ 0x + 3y = 9 \\\\ 3x + 4y = 0\\] <p>Here we have a Zero where there should be a pivot in the first row. So we cannot perform elimination. But we can exchange it with the row below to get a pivot at that place and proceed with elimination.</p> \\[  \\begin{align}3x + &amp;4y = 0 \\\\  &amp;3y = 9\\end{align} \\] <p>Now this form is already in the trianglular form and can be back-substituted. So, \\(y=3\\) from equation 2 and so \\(x = -4\\) from the first. </p> <p>To understand Gaussian elimination, we need to go beyond 2-equations and two variables. Let's try a 3-equation and 3-variable system.</p> <p>Let's say a system is:</p> \\[ \\begin{align}\\mathbf{2}&amp;x + 4y - 6z = -2 \\\\ 4&amp;x+3y-8z = 0 \\\\ -2&amp;x + 6 y + 7z =3\\end{align} \\] <p>The first pivot is in the bold-face. Now as we have to make it traingular, we will have to eliminate all the \\(x\\) terms in the second and third equation. </p> <p>So subtracting \\(2\\) times the first equation from second and \\(-1\\) times the first equation from third.</p> \\[ \\begin{align}2x + 4y - 6z &amp;= -2 \\\\ \\mathbf{-5}y+4z &amp;= 4 \\\\  10y + z &amp;=1\\end{align}\\] <p>Now the second pivot is shown in bold-face. We need to eliminate the \\(y\\) term in the third equation with it. So subtracting \\(-2\\) times the second equation from the third,</p> \\[ \\begin{align}2x + 4y - 6z &amp;= -2 \\\\ -5y+4z &amp;= 4 \\\\   \\mathbf{9}z &amp;=9\\end{align}\\] <p>The third pivot is in bold-face. We have finished the forward pass of the Elimination. The pivots are 2,-5,9. The last two pivots were hidden in original system but elimination revealed them. Now it is ready for back substitution and</p> <p>\\(z = 1\\) and so \\(y = 0\\) and finally \\(x = 2\\).</p> <p>The row picture has three planes which meet at one point \\((2,0,1)\\).</p> <p>The column picture has three vectors in the 3d space whose specific combination \\(2\\mathbf{v_1} + 0 \\mathbf{v_2} + 1 \\mathbf{v_3}\\) produce the vector \\(\\mathbf{b}\\), the output vector.</p> <p>So the process of elimination can be summarized as, for any n by n problem:</p> <ol> <li> <p>Use first equation* to create zeroes below the first pivot.</p> </li> <li> <p>Use the second equation* to create zeroes below the second pivot.</p> </li> <li> <p>Keep going untill you get an triangular form or all the equations below have Zero coefficients.</p> </li> <li> <p>*Do row exhanges wherever necessary, as Zero cannot be a pivot.</p> </li> <li> <p>If you get a complete triangular form, then back substitution will reveal the unique solution. If you get all the coefficients to be Zero then if the cooresponding output is also zero, you have infinite solution, and if the corresponding output is not zero then there is no solution.</p> </li> </ol>","tags":["vectors","elimination matrix","permutation matrix","linear combinations"]},{"location":"linear_algebra/vectors-linear-combinations/#matrix-multiplication","title":"Matrix Multiplication","text":"<p>Before talking about further elimination, let's talk about matrix multiplication a bit more.</p> <p>Let's say we multiply a matrix and a column vector. The result will be another column vector., e.g:</p> \\[ \\begin{bmatrix} 2 &amp; 4 &amp; -1 \\\\ 1 &amp; 0 &amp; 2 \\end{bmatrix}\\ \\begin{bmatrix}1 \\\\ 2\\\\ 3\\end{bmatrix}\\] <p>This multiplication, by the simple formula of multiplying rows and columns and sum upto get one element of the resultant matrix, gives us the result:</p> \\[ \\begin{bmatrix} 2 &amp; 4 &amp; -1 \\\\ 1 &amp; 0 &amp; 2 \\end{bmatrix}\\ \\begin{bmatrix}1 \\\\ 2\\\\ 3\\end{bmatrix} = \\begin{bmatrix} (2)(1) + (4)(2) + (-1)(3) \\\\ (1)(1) + (0)(2) + (2)(3) \\end{bmatrix} = \\begin{bmatrix} 7 \\\\ 7 \\end{bmatrix}\\] <p>Although this multiplication can also be represented as a linear combination of columns of the matrix using the elements of the vector:</p> \\[ \\begin{bmatrix} 2 &amp; 4 &amp; -1 \\\\ 1 &amp; 0 &amp; 2 \\end{bmatrix}\\ \\begin{bmatrix}1 \\\\ 2\\\\ 3\\end{bmatrix} = 1 \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} + 2 \\begin{bmatrix} 4 \\\\ 0 \\end{bmatrix} + 3 \\begin{bmatrix} -1 \\\\ 2 \\end{bmatrix} =  \\begin{bmatrix} 7 \\\\ 7 \\end{bmatrix} \\] <p>Similarly if we multiply two matrices, each column of the resultant matrix will be a linear combination of columns of left matrix using the elements of the corresponding column of right matrix as multipliers. If it seems too much, let's see an example:</p> \\[ \\begin{bmatrix} 2 &amp; 4 &amp; -1 \\\\ 1 &amp; 0 &amp; 2 \\end{bmatrix}\\ \\begin{bmatrix}1 &amp; 4 \\\\ 2 &amp; 5\\\\ 3 &amp; 6\\end{bmatrix} = \\begin{bmatrix}7 &amp; 22\\\\ 7 &amp; 16 \\end{bmatrix}\\] <p>It can broken into two parts: </p> <ol> <li> <p>The left matrix multiplying the first column of the right matrix to give the first column of the resultant matrix.</p> </li> <li> <p>The left matrix multiplying the second column of the right matrix to give the second column of the resultant matrix.</p> </li> </ol> <p>So in the above example the first column of the resultant matrix is the first column plus twice the second column plus three times the third column of the left matrix,i.e a linear combination of columns of left matrix with the multipliers being the elements of the first column of right matrix.</p> <p>Similarly we can see the second column of the resultant matrix again being the linear combination of columns of the left matrix with the numbers of the second column of the right matrix as multipliers.</p> <p>Again if we multiply a row vector and a matrix,</p> \\[ \\begin{bmatrix} 2 &amp; 4 &amp; -1 \\end{bmatrix}\\ \\begin{bmatrix}1 &amp; 4 \\\\ 2 &amp; 5\\\\ 3 &amp; 6\\end{bmatrix}\\] <p>The result will be a row vector which will be linear combination of rows of the matrix with multipliers being elements of the row vector.</p> \\[ \\begin{bmatrix} 2 &amp; 4 &amp; -1 \\end{bmatrix}\\ \\begin{bmatrix}1 &amp; 4 \\\\ 2 &amp; 5\\\\ 3 &amp; 6\\end{bmatrix} = 2 \\begin{bmatrix} 1 &amp; 4 \\end{bmatrix} + 4 \\begin{bmatrix} 2 &amp; 5 \\end{bmatrix} + (-1) \\begin{bmatrix} 3 &amp; 6 \\end{bmatrix} = \\begin{bmatrix} 7 &amp; 22 \\end{bmatrix}\\] <p>Similarly, if we multiply two matrices we can show that each row is a linear combination of rows of the right matrix with elements from the corresponding row of the left matrix are the multipliers.</p> <p>So a matrix multiplication can be shown as linear combination of rows(of the right matrix) as well as a linear combination of columns(of the left matrix).</p> <p>We will put the above facts, particularly about the row combination to our use to perform subtraction of equations and hence elimination.</p>","tags":["vectors","elimination matrix","permutation matrix","linear combinations"]},{"location":"linear_algebra/vectors-linear-combinations/#elimination-using-matrices","title":"Elimination Using Matrices","text":"<p>Let's use the system again we used before for this,</p> \\[ \\begin{align}2&amp;x + 4y - 6z = -2 \\\\ 4&amp;x+3y-8z = 0 \\\\ -2&amp;x + 6 y + 7z =3\\end{align} \\] <p>This system can be conveniently changed into,</p> \\[  \\begin{bmatrix} \\mathbf{2} &amp; 4 &amp; -6 \\\\ 4 &amp; 3 &amp; -8 \\\\ -2 &amp; 6 &amp; 7\\end{bmatrix}\\ \\begin{bmatrix}x \\\\ y \\\\ z \\end{bmatrix} = \\begin{bmatrix}-2 \\\\ 0 \\\\3 \\end{bmatrix}\\] <p>and so can be also written as \\(A\\mathbf{x} = \\mathbf{b}\\)</p> <pre><code>A = [2 4 -6;\n     4 3 -8;\n    -2 6  7]\n\nb = [-2; 0; 3];\n</code></pre> <p>Now, to perform first step of elimination, we have to remove the elements below the first pivot (bold face) using row subtractions. Now whatever row changes we make in \\(A\\), we have to make in \\(\\mathbf{b}\\) as well.</p> <p>We have to subtract 2 times the first row from the second and -1 times the first row from the third. To perform this operation, I will multiply both sides by a matrix \\(E_1\\).</p> <p>I would say the matrix \\(E_1\\) is:</p> \\[  E_1 = \\begin{bmatrix}1 &amp; 0 &amp; 0 \\\\ -2 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 \\end{bmatrix}\\] <pre><code>E1 = [1 0 0;\n     -2 1 0\n      1 0 1];\n</code></pre> <p>This matrix, when pre-multiplied, will always perform the above operation on any matrix(which is compatible for matrix multiplication, of course!). Let's see why.</p> <p>Let's think of the resultant matrix. The first row of that matrix will be a combination of the rows of the right matrix (on which the operation is being done on) with the first row of \\(E_1\\) being the multipliers. </p> <p>First row of resultant matrix: \\(1 \\times \\text{row1}\\) (of original matrix) + \\(0 \\times \\text{row2} + 0 \\times \\text{row3} = \\text{row1}\\) </p> <p>So the first row is not changed.</p> <p>Similarly second row of resultant matrix: \\(-2 \\times \\text{row1} + 1 \\times \\text{row2} + 0 \\times \\text{row3} = \\text{row2} - (2)\\ \\text{row1}\\)</p> <p>So we have subtracted twice row 1 from row 2.</p> <p>Third row of resultant matrix: \\(1 \\times \\text{row1} + 0 \\times \\text{row2} + 1 \\times \\text{row3} = \\text{row3} - (-1)\\text{row1}\\)</p> <p>So we have subtracted -1 times row 1 from row 3.</p> <p>Let's now perform \\(E_1A\\) and \\(E_1\\mathbf{b}\\):</p> \\[ E_1A = \\begin{bmatrix}1 &amp; 0 &amp; 0 \\\\ -2 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 \\end{bmatrix}\\ \\begin{bmatrix} 2 &amp; 4 &amp; -6 \\\\ 4 &amp; 3 &amp; -8 \\\\ -2 &amp; 6 &amp; 7\\end{bmatrix} = \\begin{bmatrix} 2 &amp; 4 &amp; -6 \\\\ 0 &amp; -5 &amp; 4 \\\\ 0 &amp; 10 &amp; 1\\end{bmatrix}\\] \\[ E_1\\mathbf{b} = \\begin{bmatrix}1 &amp; 0 &amp; 0 \\\\ -2 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 \\end{bmatrix}\\ \\begin{bmatrix} -2 \\\\ 0\\\\ 3\\end{bmatrix} = \\begin{bmatrix} -2 \\\\ 4\\\\ 1\\end{bmatrix}\\] <pre><code>E1*A\n</code></pre> <pre><code>3\u00d73 Array{Int64,2}:\n 2   4  -6\n 0  -5   4\n 0  10   1\n</code></pre> <pre><code>E1*b\n</code></pre> <pre><code>3-element Array{Int64,1}:\n -2\n  4\n  1\n</code></pre> <p>So now our equation is:</p> \\[ E_1A\\mathbf{x} = \\begin{bmatrix} 2 &amp; 4 &amp; -6 \\\\ 0 &amp; \\mathbf{-5} &amp; 4 \\\\ 0 &amp; 10 &amp; 1\\end{bmatrix}\\ \\begin{bmatrix} x \\\\ y\\\\z\\end{bmatrix} = \\begin{bmatrix} -2 \\\\ 4\\\\ 1\\end{bmatrix} = E_1\\mathbf{b}\\] <p>The second pivot(bold face) has to remove the elements below it. Now we have to subtract -2 times row from row 3. We will make another Elimination matrix, \\(E2\\) to perform this operation. But let's now make it row by row so we understand how it works.</p> <p>So in the row1 resultant matrix, we do not want any change in the matrix.</p> <p>hence row1 of elimination matrix = \\(\\begin{bmatrix} 1 &amp; 0 &amp; 0\\end{bmatrix}\\) i.e we want 1 of the first row and none of any other row, giving us the original row back.</p> <p>In row2 of resultant matrix we again do not want any change,</p> <p>so row2 of elimination matrix = \\(\\begin{bmatrix} 0 &amp; 1 &amp; 0\\end{bmatrix}\\) i.e we want 1 of second row and none of any other row.</p> <p>Now in row3 we want to eliminate 10 using pivot -5, so we want to subtract -2 times the row 2 from row 3 which is also adding 2 times row 2 to row 3:</p> <p>so row3 of elimination matrix = \\(\\begin{bmatrix} 0 &amp; 2 &amp; 1\\end{bmatrix}\\) i.e we want twice of row2 and row 1, which will eliminate the required numbers.</p> <p>So the final elimination matrix will be:</p> \\[ E_2 = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 2 &amp; 1 \\end{bmatrix}\\] <pre><code>E2 = [1 0 0; 0 1 0; 0 2 1];\n</code></pre> <p>So let's now perform \\(E_2(E_1A)\\) and \\(E_2(E_1\\mathbf{b})\\):</p> \\[ E_2(E_1A) =\\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 2 &amp; 1 \\end{bmatrix}\\ \\begin{bmatrix} 2 &amp; 4 &amp; -6 \\\\ 0 &amp; -5 &amp; 4 \\\\ 0 &amp; 10 &amp; 1\\end{bmatrix} = \\begin{bmatrix} 2 &amp; 4 &amp; -6 \\\\ 0 &amp; -5 &amp; 4 \\\\ 0 &amp; 0 &amp; 9 \\end{bmatrix}\\] \\[ E_2(E_1\\mathbf{b}) = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 2 &amp; 1 \\end{bmatrix}\\ \\begin{bmatrix} -2 \\\\ 4\\\\ 1\\end{bmatrix} = \\begin{bmatrix} -2 \\\\ 4 \\\\ 9 \\end{bmatrix}\\] <pre><code>E2*(E1*A)\n</code></pre> <pre><code>3\u00d73 Array{Int64,2}:\n 2   4  -6\n 0  -5   4\n 0   0   9\n</code></pre> <pre><code>E2*(E1*b)\n</code></pre> <pre><code>3-element Array{Int64,1}:\n -2\n  4\n  9\n</code></pre> <p>So now the equation is:</p> \\[ E_2(E_1A)\\mathbf{x} = \\begin{bmatrix} 2 &amp; 4 &amp; -6 \\\\ 0 &amp; -5 &amp; 4 \\\\ 0 &amp; 0 &amp; 9 \\end{bmatrix} \\ \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix} = \\begin{bmatrix} -2 \\\\ 4 \\\\ 9 \\end{bmatrix} = E_2(E_1\\mathbf{b}) \\] <p>Here we have reached to the upper triangular form and hence have completed the forward pass. the backward pass is simple.</p> <p>So we apply elimination matrices to change \\(A \\rightarrow U\\), where \\(U\\) is an upper traingular matrix. We apply the same matrices to change \\(\\mathbf{b} \\rightarrow \\mathbf{b}'\\) and finally solve \\(U\\mathbf{x} = \\mathbf{b'}\\) using back-substitution(if there is a unique solution).</p> <p>One more thing to mention about matrix multiplication is that it holds on associativity. So, if \\(P, Q, R\\) are three matrices(compatible for matrix multiplication in the order), then:</p> \\[ P(QR) = (PQ)R\\] <p>So basically we can multiply all our elimination matrices first, and then multiply that with our \\(A\\) and \\(\\mathbf{b}\\).</p> \\[E = E_nE_{n-1}\\cdots E_2E_1\\] <p>Let's see how this matrix \\(E\\) looks like in our example.</p> \\[ E = E_2E_1  = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 2 &amp; 1 \\end{bmatrix} \\ \\begin{bmatrix}1 &amp; 0 &amp; 0 \\\\ -2 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 \\end{bmatrix} = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ -2 &amp; 1 &amp; 0 \\\\ \\mathbf{-3} &amp; 2 &amp; 1 \\end{bmatrix}\\] <p>So we multiply by this elimination matrix to our matrix \\(A\\) to form an Upper triangular matrix, \\(U\\), </p> \\[ E_2(E_1A) = (E_2E_1)A = EA = U\\] <p>and</p> \\[ E_2(E_1\\mathbf{b}) = (E_2E_1)\\mathbf{b} = E\\mathbf{b} =\\mathbf{b'} \\] <pre><code>E = E2*E1\n</code></pre> <pre><code>3\u00d73 Array{Int64,2}:\n  1  0  0\n -2  1  0\n -3  2  1\n</code></pre> <p>Now the matrix \\(E\\) has almost all the same multipliers that we use in individual except a few(shown in bold-face of matrix \\(E\\)). It is because the effect of the first elimination is shown in second and so on. In our example the second row is subtracted by 2 times the first row and the third row is  added by 1 times first row. Then the new third row is added 2 times the new second row. So essentially, the original third row was subtracted 3 times the first row and 2 times the original second row.</p> <p>However to show it in a better way, we can use the inverse of \\(E\\),</p> <p>\\(EA = U\\) can be shown as \\(A = E^{-1}U\\)</p> <p>Let's look at \\(E^{-1}\\)</p> \\[ E^{-1} = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 2 &amp; 1 &amp; 0 \\\\ -1 &amp; -2 &amp; 1 \\end{bmatrix}\\] <pre><code>L=inv(E)\n</code></pre> <pre><code>3\u00d73 Array{Float64,2}:\n  1.0   0.0  0.0\n  2.0   1.0  0.0\n -1.0  -2.0  1.0\n</code></pre> <p>The \\(E^{-1}\\) has the correct values that mutiply the pivots, before subtracting them from the lower rows going from \\(A\\) to \\(U\\). SInce it is an lower traingular matrix, we represent it by \\(L\\). </p> \\[ A = LU\\] <p>The matrix \\(L\\) can be used at many places.</p> <ol> <li> <p>It can be used to factorize \\(A\\).</p> </li> <li> <p>It has the memory of pivot multipliers before subtraction.</p> </li> <li> <p>*It can be used to compute transformed right hand side,\\(\\mathbf{b'}\\), as \\(\\mathbf{b} = L\\mathbf{b'}\\).</p> </li> </ol> <p>*Although we can also concatenate \\(A\\) and \\(\\mathbf{b}\\) as \\(\\begin{bmatrix} A &amp; \\mathbf{b} \\end{bmatrix}\\) and then run it though the forward elimination but most applications tend to do them separately.</p>","tags":["vectors","elimination matrix","permutation matrix","linear combinations"]},{"location":"linear_algebra/vectors-linear-combinations/#row-exchanges","title":"Row Exchanges","text":"<p>Here we had prepared everything but considering no row exchange will be needed. Let's remind that row exchanges are needed when zeros occur at pivot places.</p> <p>What matrix to use when we have to exchange two rows? Let's take an example: end{bmatrix}</p> \\[ A = \\begin{bmatrix}\\mathbf{1} &amp; 3 &amp; 5 \\\\ 3 &amp; 9 &amp; 7 \\\\ 4 &amp; 8 &amp; 9\\end{bmatrix} \\] <p>Let's first eliminate the first column using the first pivot(bold-face),</p> \\[ E_1A = \\begin{bmatrix}1 &amp; 0 &amp; 0 \\\\ -3 &amp; 1 &amp; 0 \\\\ -4 &amp; 0 &amp; 1 \\end{bmatrix} \\ \\begin{bmatrix}\\mathbf{1} &amp; 3 &amp; 5 \\\\ 3 &amp; 9 &amp; 7 \\\\ 4 &amp; 8 &amp; 9\\end{bmatrix} = \\begin{bmatrix} 1 &amp; 3 &amp; 5 \\\\ 0 &amp; 0 &amp; -8 \\\\ 0 &amp; -4 &amp; -11 \\end{bmatrix}\\] <p>As you can see the second diagonal element is zero, which cannot be a pivot. So we will exchange the second and third row. the matrix we will apply is called a permutation matrix. </p> \\[ P_{23} = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 1 &amp; 0 \\end{bmatrix}\\] <p>So the first resultant row will be the same. The resultant second row will be zero times first and second row and 1 times third row ,i.e the third row. The resultant third row will be zero times first and third row but 1 times the second, i.e the second row. So, essentially the second and the third row have been exchanged. A permutation matrix is basically identity matrix with the same rows exchanged which need to be changed in the coefficient matrix.</p> \\[ P_{23}(E_1A) = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 1 &amp; 0 \\end{bmatrix} \\ \\begin{bmatrix} 1 &amp; 3 &amp; 5 \\\\ 0 &amp; 0 &amp; -8 \\\\ 0 &amp; -4 &amp; -11 \\end{bmatrix} = \\begin{bmatrix} 1 &amp; 3 &amp; 5 \\\\ 0 &amp; \\mathbf{-4} &amp; -11 \\\\ 0 &amp; 0 &amp; -8  \\end{bmatrix}\\] <p>Now we have the new pivot and we can move with elimination. </p> <p>Note: Row exchanges will alter the final the elimination matrix, \\(E\\) as well as the inverse \\(L\\) which will not have the values at the same place.  </p> <p>Now one way to keep it in the same way is if we perform all the row exchanges first, using the product of all permutation matrices, \\(P\\) and then perform the elimination using \\(E\\).</p> <p>Final equation is:</p> \\[ PA = LU\\] <p>This is a good stopping point in basics. We will next see what does the infinite solutions mean, how we represent them. What are vector spaces and subspaces, rank, invertibility and more.</p> <p>{% include linalg/1.basics/plot1.html %}</p> <p>{% include linalg/1.basics/plot2.html %}</p> <p>{% include linalg/1.basics/plot3.html %}</p>","tags":["vectors","elimination matrix","permutation matrix","linear combinations"]},{"location":"neural_networks/convolutional_neural_networks/convolutions/","title":"Convolutions","text":"","tags":["neural networks","momentum"]},{"location":"neural_networks/convolutional_neural_networks/convolutions/#introduction","title":"Introduction","text":"<p>Convolution operation(denoted by \\(*\\)) is one of the most important, if not the most important technique in Signal Processing. It gives us the idea of how a linear invariant system converts an input signal to an output signal. From now on, we will refer to digital signal as simply signal.</p> <p>Let's first introduce some terminology:</p> <ol> <li>Input Signal\\((x[n])\\): As the name suggests, it is a signal that acts as an input to a system.</li> <li>Output System\\((y[n])\\): The output that the system generates by acting over the input signal.</li> <li>Impulse: An impulse is a signal that is zero everywhere except at a single point.</li> <li>Delta function\\((\\delta [n])\\): It is an impulse with value unity at sample number 0 and zero at every other sample number. It is a normalized impulse, also called unit impulse.</li> </ol> \\[\\delta [n] = \\begin{cases} 1 &amp; n=0\\\\ 0 &amp; \\text{otherwise}\\end{cases} \\tag{1}\\] <ol> <li>Impulse response\\((h[n])\\): The output of a system when a delta function\\((\\delta [n])\\) is give as input.</li> </ol> <p>The impulse response of a system defines the system. Different systems will have different impulse responses.</p> <p>Now, how to obtain an output from a system for an input signal? We use what is called a Decomposition technique.</p> <p>Decomposition: Break the input signal into smaller units. Pass those units through the system to get corresponding outputs. Add those outputs to get the final output signal.</p> <p>There are two main decompositions in signal processing: Fourier Decomposition and Impulse Decomposition. When impulse decomposition is done, it can be described by a mathematical operation called convolutions(denoted \\(*\\)).</p> <p>We do an impulse decomposition, i.e we decompose our input signal in multiple impulses and then pass those impulses through the system to produce corresponding outputs. The outputs are added to generate the final output signal.</p>","tags":["neural networks","momentum"]},{"location":"neural_networks/convolutional_neural_networks/convolutions/#impulse-as-a-delta-function","title":"Impulse as a delta function","text":"<p>Any impulse function can be represented as a shifted and scaled delta function. e.g: An impulse, say \\(a[n]\\) with zeroes everywhere except at sample no. 2, has a value of -1.3. This impulse can be represented as</p> <p></p> <p>In the abpve figure, the LHS is an impulse \\(a[n]\\) of -1.3 at sample number 2 and the RHS is a standard delta \\(\\delta [n]\\) i.e impulse of unity at sample number 0.</p> <p>Now, we can represent this impulse \\(a[n]\\) as a scaled and shifted \\(\\delta [n]\\) as:</p> \\[a[n] = -1.3\\ \\delta[n-2]\\] <p>Similarly any impulse \\(b[n]\\) which has value \\(u\\) and sample number \\(p\\), i.e</p> \\[b[n] = \\begin{cases}u &amp; n=p\\\\0 &amp; \\text{otherwise} \\end{cases}\\] <p>then,</p> \\[b[n] = u\\ \\delta[n-p] \\tag{2}\\]","tags":["neural networks","momentum"]},{"location":"neural_networks/convolutional_neural_networks/convolutions/#output-of-an-impulse","title":"Output of an Impulse","text":"<p>Now if \\(a[n]\\) is passed through a system with impulse response \\(h[n]\\), what will be the output? Well \\(h[n]\\) is the output when the input is \\(\\delta [n]\\). What could be output when input is \\(-1.3 \\delta[n-2]\\)? </p> <p>The output is shifted and scaled by the same amount as the delta function is to form the input impulse. These are the properties of homogeneity and shift invariance.</p> <p>So if we know the impulse response of any system, we know the output of any impulse.</p> <p>With the information of how outputs to impulses are produced, let's revisit how do we produce output to a complete signal by Impulse Decomposition, which is mathematically equivalent to convolution of input signal and impulse response.</p> <p>First we decompose the input signal into impulses which can be viewed as shifted and scaled delta functions. These shifted and scaled delta functions produced similar shifted and scaled impulse responses as outputs. These outputs are finally synthesized(added) to produce the final output.</p> <p>This whole operation is called convolution of impulse response \\(h[n]\\) and input signal \\(x[n]\\) and is denoted as:</p> \\[y[n] = x[n] * h[n] \\tag{3}\\] <p>Note: \\(*\\) is not multiplication, it is convolution. </p> <p>So, to know impulse response of a system, we know output of any input signal. It is something that defines the system.</p> <p>This impulse response is sometimes called a kernel. And the output as feature map.</p>","tags":["neural networks","momentum"]},{"location":"neural_networks/convolutional_neural_networks/convolutions/#dimensions","title":"Dimensions","text":"<p>Let's say our input signal has \\(m\\) number of samples and the kernel has \\(k\\) number of samples, then output signal(the convolution of input signal and the kernel, will be \\(m+k-1\\).</p> <p>Note: When we say a signal has \\(s\\) samples, it means we know these \\(s\\) samples. The signal actually continues from \\(-\\infty\\) to \\(+\\infty\\). We assume every signal(be that input, kernel or output) has values of zero anywhere outside of those \\(s\\) samples.</p> <p>If an input signal has, say, \\(200\\) samples, and a kernel is \\(20\\) samples long, then we have to shift and scale the kernel for each impulse as that impulse is shifted and scaled from the delta function. Now for the first impulse of input(i.e sample 0), the kernel will be scaled but not shifted and thus will contribute to output signal from sample 0 to sample 19 (because kernel is 20 samples long). For second impulse of input (i.e sample no. 1), the kernel will shift 1 step and will be scaled as well, and thus will contribute to output signal from sample 1 to sample 20. The intersecting output contrbutions at a given sample are added. Similarly at \\(m^{th}\\) impulse of input(i.e at sample no. \\(m-1\\)), the kernel will be shifted \\(m-1\\) places (and scaled as well) and thus will contribute to output signal from  sample no. \\(m-1\\) to sample no. \\(m+20-1\\) i.e \\(m+k-1\\). </p>","tags":["neural networks","momentum"]},{"location":"neural_networks/convolutional_neural_networks/convolutions/#example","title":"Example","text":"<p>Let's see an example to make things more clear. Let's say we have an input of 9 samples and a kernel of 4 samples. We will store these in <code>numpy</code> arrays.</p> <pre><code>import numpy as np\ninput_signal = np.array([1.1,0,-1.05,1.4,-1.2,-1.4,-1,0,0.7])\nkernel = np.array([1,0.5,-0.3,0.2])\n</code></pre> <p>Now input is 9 samples long(\\(m=9\\)) and kernel is 4 samples long (\\(k=4\\)) so output will be \\(m+k-1 = 9+4-1 = 12\\) samples long.</p> <ol> <li>Let's start with creating an output array of 12 values with each element equal to zero.</li> <li>We will then extend our kernels to be of the same size as our outputs and then shift and scale the kernels. </li> <li>Finallly we will add them to our output signal.</li> </ol> <pre><code>def simple_convolve(input_signal, kernel):\n    \"\"\"A simple way of convolving two signals.\n    input_signal: numpy array containing the input signal.\n    kernel: numpy array containing the kernel.\"\"\"\n\n    output_signal = np.zeros(len(input_signal)+len(kernel)-1)\n\n    for i in range(len(input_signal)):\n\n        shifted_kernel = np.hstack((np.zeros(i), kernel, np.zeros(len(output_signal)-i-len(kernel))))\n        scaled_shifted_kernel = input_signal[i]*shifted_kernel\n\n        #add the scaled and shifted kernel to the output\n        output_signal+=scaled_shifted_kernel\n\n    return output_signal\n</code></pre> <pre><code>output_signal = simple_convolve(input_signal, kernel);output_signal\n</code></pre> <pre><code>array([ 1.1  ,  0.55 , -1.38 ,  1.095, -0.185, -2.63 , -1.06 , -0.32 ,\n        0.72 ,  0.15 , -0.21 ,  0.14 ])\n</code></pre> <pre><code>import matplotlib.pyplot as plt\ntext_color = \"orange\"\nback_color = (33/255,33/255,33/255)\nf, ax = plt.subplots(1,3, figsize=(18,4), gridspec_kw=dict(width_ratios=[3,1,3]))\n\nfor subplot, data, title in zip(ax, (input_signal, kernel, output_signal), ('Input Signal', 'Kernel', 'Output Signal')):\n    subplot.plot(data, '.', markersize=20, color=text_color)\n    subplot.set_title(title, color=text_color)\n    subplot.set_ylim(-3,3)\n    subplot.set_xticks(list(range(len(data))))\n    subplot.set_xticklabels(list(map(str,range(len(data)))), color=text_color)\n    subplot.set_yticklabels(list(map(str,np.arange(-3,4))), color=text_color)\n    subplot.set_facecolor(back_color)\n    subplot.grid(True, alpha=0.3, linestyle='--')\n    subplot.set_xlabel(\"Sample Number\", color=text_color)\n\nf.set_facecolor(back_color)\nf.set_alpha(0)\nf.tight_layout()\n</code></pre> <p></p> <p>The process can be shown graphically as:</p> <p> </p> <p>The key take away from the above graphic is this frame:</p> <p></p> <p>These are the contributions of each shifted and scaled kernels to the final output signal. This figure shows how each impulse from the input signal changes the kernel and then these changes are added to the final output.</p> <p>Note: The kernel is in orange color, the cyan color is just extensions of zeros to make it the same size as that of the output signal.</p>","tags":["neural networks","momentum"]},{"location":"neural_networks/convolutional_neural_networks/convolutions/#convolution-is-commutative","title":"Convolution is commutative","text":"<p>Let's again convolve the input signal with kernel.</p> <pre><code>output1 = simple_convolve(input_signal=input_signal, kernel=kernel); output1\n</code></pre> <pre><code>array([ 1.1  ,  0.55 , -1.38 ,  1.095, -0.185, -2.63 , -1.06 , -0.32 ,\n        0.72 ,  0.15 , -0.21 ,  0.14 ])\n</code></pre> <p>Now let's now reverse the input_signal and the kernel.</p> <pre><code>output2 = simple_convolve(input_signal=kernel, kernel=input_signal); output2\n</code></pre> <pre><code>array([ 1.1  ,  0.55 , -1.38 ,  1.095, -0.185, -2.63 , -1.06 , -0.32 ,\n        0.72 ,  0.15 , -0.21 ,  0.14 ])\n</code></pre> <p>As you can see, both the outputs are same. This is an important property of convolution that is put to use, i.e:</p> \\[a[n]*b[n] = b[n]*a[n] \\tag{4}\\] <p>The result of convolution is always same, no matter the order of the signals. This operation is commutative.</p> <p>Now, what does this mean in Signal Processing? It means we can exchange the impulse response and the input signal to generate the same output, but that does not make any sense physically as the impulse response of a system is fixed and cannot be altered. It is what defines the system. So changing the impulse response means to changing the system completely.</p> <p>So basically, for signal processing, it does not have any special meaning. It is simply a mathematical tool that can be leveraged to implement convolution operation.</p>","tags":["neural networks","momentum"]},{"location":"neural_networks/convolutional_neural_networks/convolutions/#the-formula","title":"The formula?","text":"<p>Now we have implemented convolution, but what if we need to know just what the, say, 8th element of the convolved output signal is? Our current implementation will calculate that by computing the whole output signal and then giving us the output. This method is slow and if the input signal is hundreds of thousands(or even millions) of points, it will take a huge amount of time to do that, only to return one element.</p> <p>So what we need is a mathematical formula of what convolution is.</p> \\[(a*b)[n] = ?\\] <p>This is what we need to find out. For that let's recall the important take away from the animation we saw earlier. I'll add the x axis at each figure, we will need it.</p> <p></p> <p>Now the output signal is the sum of all these signals. These signals are added at their corresponding samples. So the 8th element of the output signal is the sum of all 8th elements in each of these signals.</p> <p>Now, after observations, you can tell that the 8th elements of most of these signals is zero (in cyan). These were the extensions to make the kernel longer. And these zeros don't contribute anything to the final sum. This is how our calculation gets fast, we only only sum values from those signals, which were not extensions.</p> <p>So, for element 8, only \\(x[5]h[n-5], x[6]h[n-6], x[7]h[n-7] \\text{ and  }x[8]h[n-8]\\) contributes (because only these signals are  orange  at sample number 8).</p> <p>So we can say,</p> \\[y[8] = x[5]h[n-5] + x[6]h[n-6] + x[7]h[n-7] + x[8]h[n-8]; \\qquad \\text{for }n=8\\] <p>Substituting \\(n=8\\),</p> \\[y[8] = x[5]h[3] + x[6]h[2] + x[7]h[1] + x[8]h[0]\\] <p>It can be written as,</p> \\[\\begin{align} y[8] &amp;= x[8-3]h[3] + x[8-2]h[2] + x[8-1]h[1] + x[8-0]h[0] \\\\ \\implies y[8] &amp;= \\sum_{k=0}^{3}x[8-k]h[k] \\end{align}\\] <p>This means 4 samples of kernel are multiplied with 4 samples of input.</p> <p>Let's look at output sample 6. This value comes from the sum of all the  orange  samples at sample number 6 in the above figure.</p> <p>so,</p> \\[y[6] = x[3]h[n-3] + x[4]h[n-4] + x[5]h[n-5] + x[6]h[n-6]; \\qquad \\text{for }n=6\\] \\[y[6] = x[3]h[3] + x[4]h[2] + x[5]h[1] + x[6]h[0]\\] <p>Again, it can be written as:</p> \\[\\begin{align} y[6] &amp;= x[6-3]h[3] + x[6-2]h[2] + x[6-1]h[1] + x[6-0]h[0] \\\\ \\implies y[6] &amp;= \\sum_{k=0}^3 x[6-k]h[k]\\end{align}\\] <p>Generalizing, for a kernel of size \\(n_k\\), running from \\(0\\) to \\(n_k-1\\)</p> \\[y[i] = (x*h)[i] = \\sum_{k=0}^{n_k-1}x[i-k]h[k] \\tag{5}\\] <p>This equation is called convolution sum. It let's us calculate the output at any point independent of other output points. As \\(k\\) runs from \\(0\\) to \\(n_k-1\\), each sample from the kernel, \\(h[k]\\) is multiplied with corresponding input samples \\(x[i-k]\\) and then summed up to form the output at sample \\(i\\).</p> <p></p> <p>As you can see in the above figure, the kernel is flipped and multiplied with the corresponding input elements (white). These products are then summed up to form the final output(white in ouput).</p>","tags":["neural networks","momentum"]},{"location":"neural_networks/convolutional_neural_networks/convolutions/#padding","title":"Padding","text":"<p>Coming back to Eq. 5, let's see how we can generate, \\(y[0]\\) and \\(y[11]\\).</p> \\[y[i] = (x*h)[i] = \\sum_{j=0}^{n_k-1}x[i-j]h[j] \\tag{5}\\] <p>For \\(i=0\\) and \\(n_k=4\\),</p> \\[\\begin{align}y[0] = (x*h)[0] &amp;= \\sum_{j=0}^{3}x[0-j]h[j] \\\\ &amp;=x[0]h[0] + x[-1]h[1] + x[-2]h[2] + x[-3]h[3] \\end{align}\\] <p>For \\(i=11\\) and \\(n_k=4\\),</p> \\[\\begin{align}y[11] = (x*h)[11] &amp;= \\sum_{j=0}^{3}x[11-j]h[j] \\\\ &amp;=x[11]h[0] + x[10]h[1] + x[9]h[2] + x[8]h[3] \\end{align}\\] <p>In both these equations, we have some inputs, which are not part of our original input signal. Our original signal has sample number 0 to 8. So what would \\(x[-3]\\), \\(x[2]\\), \\(x[-1]\\) and \\(x[9]\\), \\(x[10]\\), \\(x[11]\\) mean?</p> <p>Actually the input signal (as well as output and kernel), all are present at all samples. It is just they are zero (i.e no signal). And they don't contribute to the convolution and hence are not saved. (Besides, we cannot save infinite numbers in a computer, can we?). But, as it turns out we need some of these zero points, for our outputs. Specifically, we need \\(n_k-1\\) points on both sides of an input signal. So we just extend our signal with zeros for these samples. This is what we call padding. Padding is a technique where we add a bunch of zeros to a signal where the signal does not exist.</p> <p></p> <p>The above animation shows how the flipped kernel moves along the input signal. At each stop it multiplies the elements of the kernel and that of the input signal which overlap with the kernel. These products are summed up to form the corresponding output sample. Look at this animation and the equation 5. Make sure you understand both. Practice a bit on paper. Remember, the kernel is flipped. </p> <p>The cyan points in the input signal in above animation are the padding of that signal. The far left and far right data points of the output signal are based on incomplete information, because some of the inputs are padded. We say that the kernel is not fully immersed in the input signal while computing these points. If the kernel is \\(n_k\\) samples long, then the \\(n_k-1\\) samples of the ouput signal on both sides are based on less information. The farther elements of these \\(n_k-1\\) elements (on both sides) are based on lesser information than the closer ones to the center. So the extreme points in the output signal are usually not used. Another way to do this is to control the padding. If we intitially pad the the signal less than \\(n_k-1\\) points on both sides, it is equivalent of ignoring the outputs that would have formed because of these points.</p> <p>With that in mind, let's write the code for simple convolution using this method. We will control the padding as well.</p> <pre><code>import numpy as np\n</code></pre> <pre><code>def conv(input_signal, kernel, padding=None):\n    \"\"\"Performs convolution of input and kernel with padding.\n\n    Parameters\n    ----------\n    `input_signal`: iterable containing the input signal.\n\n    `kernel`: iterable contatining the kernel.\n\n    `padding`: int, the amount of zero padding to be done in each sides of the input signal.\n               Default is `len(kernel)-1`\n    \"\"\"\n\n    if padding is None:\n        padding = len(kernel)-1 # default padding is the full padding\n\n    padded_inp = np.pad(input_signal,padding)\n    output_signal=[]\n\n    #traverse through the input signal until kernel overlaps with the end point\n    for i in range(len(padded_inp)-(len(kernel)-1)):\n        # perform convolution sum.\n        current_out = np.sum(padded_inp[i:i+len(kernel)]*np.array(kernel[::-1])) #kernel is flipped.\n        output_signal.append(current_out)\n\n    return np.array(output_signal)\n</code></pre> <pre><code>output3 = conv(input_signal=input_signal, kernel=kernel); output3\n</code></pre> <pre><code>array([ 1.1  ,  0.55 , -1.38 ,  1.095, -0.185, -2.63 , -1.06 , -0.32 ,\n        0.72 ,  0.15 , -0.21 ,  0.14 ])\n</code></pre> <pre><code>output4 = conv(input_signal=kernel, kernel=input_signal); output4\n</code></pre> <pre><code>array([ 1.1  ,  0.55 , -1.38 ,  1.095, -0.185, -2.63 , -1.06 , -0.32 ,\n        0.72 ,  0.15 , -0.21 ,  0.14 ])\n</code></pre> <p>As you can see, the commutative property still holds up.</p> <p>Let's look at some examples,</p> <pre><code>from math import floor, ceil\nimport matplotlib as mpl\nmpl.rcParams[\"mathtext.fontset\"]=\"cm\"\n\ninput_color = \"orange\"\nkernel_color = \"white\"\noutput_color = (0,1,0)\n\n\n\ndef plot_signals(input_signal, kernel, input_name=None, kernel_name=None, output_name=None, padding=None, fig_size=(10,5), axes=None, f=None, tight_layout=True):\n    \"\"\"Plots a 1 x 3 grid of input signal, kernel and output signal\n\n    Parameters:\n    -----------\n    `input_signal`: iterable, containing the input signal.\n\n    `kernel`: iterable, containing the impulse response of a system.\n\n    `input_name`: [optional] str, name for the input signal. It will be displayed in parenthesis. Can include latex as well.\n\n    `kernel_name`: [optional] str, name for kernel. Will be displayed in parenthesis. Can include latex as well.\n\n    `padding`: [optional] int, padding to be used for convolution.\n\n    `fig_size`: [optional] tuple(int, int), containing width and height of the figure.\n    \"\"\"\n    p_flag = False #flag is set if padding is None\n    if padding is None:\n        p_flag = True\n        padding = len(kernel)-1\n    output_signal = conv(input_signal, kernel, padding)\n\n    init_color, final_color = np.array([1,0,0]), np.array([0.1,0.9,0])\n    unusable_colors = [list(final_color - (i*(final_color-init_color)/(len(kernel)-1))) for i in range(padding)]\n\n    if axes is None:\n        f, axes = plt.subplots(1,3,figsize=fig_size, facecolor=back_color, gridspec_kw=dict(width_ratios=[len(input_signal),len(kernel),len(output_signal)]))\n\n    input_name = rf\"Input Signal $x[n]$ ({input_name})\" if input_name is not None else r\"Input Signal $x[n]$\"\n    kernel_name = rf\"Kernel $h[n]$ ({kernel_name})\" if kernel_name is not None else r\"Kernel $h[n]$\"\n\n\n    if output_name is None:\n        output_name = rf\"Output Signal $y[n] = x[n] * h[n]$\"+f\"\\nPadding: {f'full({len(kernel)-1})' if p_flag else padding}\"\n\n    for ax, data, name, color,label_color in zip(axes, (input_signal, kernel, output_signal), (input_name, kernel_name, output_name), \n                                                 (input_color, kernel_color, list(reversed(unusable_colors)) +\\\n                                                  [output_color]*(len(output_signal)-2*padding)+unusable_colors), (input_color, kernel_color, output_color)):\n        if ax is not None:\n            ax.scatter(list(range(len(data))),data,s=20, c=color)\n            ax.set(facecolor=back_color, alpha=0, ylim=(np.floor(min(data))-1,ceil(max(data))+1), \n                   yticks=range(floor(min(data))-1,ceil(max(data))+2,max(1,ceil((max(data)-min(data)+2)/15))), xticks=range(0,len(data)+1,10))\n\n            ax.grid(True,linestyle='--',alpha=0.3)\n\n            ax.set_yticklabels(list(map(str,ax.get_yticks())), color=label_color)\n            ax.set_xticklabels(list(map(str,ax.get_xticks())), color=label_color)\n\n            ax.set_title(name, color=label_color, size=18)\n\n            for spine in ax.spines.values():\n                spine.set_visible(False)\n\n    if tight_layout:\n        f.tight_layout()\n\n    return f,axes\n</code></pre> <p>Let's use a new input signal. A sine with a negative ramp.</p> <pre><code>x = np.arange(101)/3\ninput_signal = np.sin(x)-0.3*(x)\n</code></pre> <p>Let's try to invert this signal using a known inverter kernel.</p> <pre><code>inverter = np.zeros(31)\ninverter[15] = -1\n</code></pre> <pre><code>plot_signals(input_signal, inverter, fig_size=(30,4), input_name=r\"$\\sin {\\left({}^n/_3\\right)} - 0.3 n$\", kernel_name=\"Inverter\");\n</code></pre> <p></p> <p>Look at the output. The signal has been inverted but not at the end points. This is because while computing these points, the kernel was not fully immersed into input signal because of the padding. These extreme output points are not usable and hence are to be ignored. Another way is to not compute them at all, i.e reduce the padding. Our current kernel is 31 samples long, and so the padding will be 30 samples on each side. Let's try padding of just 15 samples.</p> <pre><code>plot_signals(input_signal, inverter, fig_size=(30,4), padding = 15, \n             input_name=r\"$\\sin {\\left({}^n/_3\\right)} - 0.3 n$\", kernel_name=\"Inverter\");\n</code></pre> <p></p> <p>Much better. But remember there are still extreme points which are based on less information. It is just they are based on comparatively more information than the ones before them. If we want only those points which are based on complete information, then we have to set the padding to be zero. The redder samples are based on lesser information.</p> <pre><code>plot_signals(input_signal, inverter, fig_size=(30,4), padding = 0, \n             input_name=r\"$\\sin {\\left({}^n/_3\\right)} - 0.3 n$\", kernel_name=\"Inverter\");\n</code></pre> <p></p> <p>Notice the difference in sizes of output signal for each padding. We had mentioned earlier that if an input signal is \\(m\\) samples long and a kernel is \\(k\\) sampes long then the output signal will be \\(m+k-1\\) signals long. This is actually true for full padding.</p> <p>For padding \\(p\\),</p> \\[o =  m + 2p - k + 1 \\tag{6}\\] <p>For full padding, \\(p=k-1\\),</p> \\[\\begin{align}o_{full} &amp;= m + 2(k-1) -k +1 \\\\ &amp;= m + k -1 \\end{align}\\] <p>There is another padding, called same padding. It keeps the input and output size the same.</p> <p>For same padding, \\(o=m\\),</p> \\[ m = m + 2p - k +1 \\\\ \\implies p_{same} = \\frac{k-1}{2} \\tag{7}\\] <p>Let's try another kernel. A known high pass filter. Since our input signal consists of a sine function and a linear function, we would like to just keep the higher frequency part(i.e the sine part). The linear part should be removed.</p> <pre><code>highpass_filter = np.linspace(-0.23,0.23,31)**2-(0.23**2)\nhighpass_filter[15]=1\n</code></pre> <pre><code>def plot_all(input_signal, kernel, paddings=[0,15,None], input_name=None, kernel_name=None, fig_size=(10,5)):\n    from matplotlib.gridspec import GridSpec    \n\n    axes = []\n    gs = GridSpec(len(paddings), 3,wspace=0.1, width_ratios=[len(input_signal),len(kernel),len(input_signal)+len(kernel)-1])\n\n    f = plt.figure(figsize=fig_size, facecolor=back_color)\n    input_ax = f.add_subplot(gs[(len(paddings)//2),0])\n    kernel_ax = f.add_subplot(gs[(len(paddings)//2),1])\n\n    plot_signals(input_signal, kernel,input_name=input_name, kernel_name=kernel_name, padding=None,axes=[input_ax,kernel_ax], f=f, tight_layout=False)\n\n    out_ax = f.add_subplot(gs[0,2])\n    plot_signals(input_signal, kernel, padding=paddings[0],axes=[None,None,out_ax], f=f, tight_layout=False)\n    for index in range(1,len(paddings)):\n        out_ax = f.add_subplot(gs[index,2], sharex=out_ax)\n        out_name = f\"Padding: {paddings[index] if paddings[index] is not None else f'full ({len(kernel)-1})'}\"\n        plot_signals(input_signal, kernel, padding=paddings[index],axes=[None,None,out_ax], f=f, output_name=out_name, tight_layout=False)\n</code></pre> <p><pre><code>plot_all(input_signal, highpass_filter, input_name=r\"$$\\sin {\\left({}^n/_3\\right)} - 0.3 n$$\", paddings=[0,(len(highpass_filter)-1)//2, None], #zero, same and full\n         kernel_name=\"High Pass Filter\", fig_size=(30,12))\n</code></pre> </p>","tags":["neural networks","momentum"]},{"location":"neural_networks/convolutional_neural_networks/convolutions/#properties-of-convolution","title":"Properties of Convolution","text":"","tags":["neural networks","momentum"]},{"location":"neural_networks/convolutional_neural_networks/convolutions/#1-delta-function-delta-n","title":"1. Delta Function \\(\\delta [n]\\)","text":"<ul> <li>It is the identity function for convolution. Just like \\(0\\) is for addition (\\(a+0=a\\)) or like \\(1\\) is for multiplication (\\(a \\times 1 = a\\)), similarly:</li> </ul> \\[ x[n] * \\delta[n] = x[n] \\tag{7}\\] <ul> <li>If the delta function is scaled, then the output is also scaled:</li> </ul> \\[x[n] * k\\delta [n] = kx[n] \\tag{8}\\] <p>So it can be used as an amplifier or as an attenuator. We can even invert a signal like we did above by setting \\(k=-1\\) and can do much more with this simple property.</p> <ul> <li>If the delta function is shifted by an amount, the output function is also shifted by the same amount:</li> </ul> \\[x[n] * \\delta[n+d] = x[n+d] \\tag{9}\\]","tags":["neural networks","momentum"]},{"location":"neural_networks/convolutional_neural_networks/convolutions/#2-commutative-property","title":"2. Commutative Property","text":"<p>It does not matter which signal is convolved with which signal, (or which signal is slided over which signal, provided padding is appropriate), the result will be the same.</p> \\[a[n] * b[n] = b[n] * a[n] \\tag{10}\\] <p>This property doesn't mean anything in the signal processing physically. It is just a convenient tool for mathematics and certain implementations.</p>","tags":["neural networks","momentum"]},{"location":"neural_networks/convolutional_neural_networks/convolutions/#3-associative-property","title":"3. Associative Property","text":"\\[ (a[n] * b[n]) * c[n] = a[n] * (b[n] * c[n]) \\tag{11}\\] <p>It provides idea how cascaded systems work. Suppose, we have two kernels, \\(h_1[n]\\) and \\(h_2[n]\\), and we apply them respectively:</p> \\[y[n] = (x[n]*h_1[n])*h_2[n]\\] <p>Now by associative property,</p> \\[y[n] = x[n] * (h_1[n]*h_2[n])\\] <p>Let's say \\(h_1[n]* h_2[n] = H[n]\\), then:</p> \\[y[n] = x[n] * H[n]\\] <p>So if we are convolving a signal with \\(n\\) kernels one after another, we can equivalently convolve the signal with the convolution of all these kernels with each other. It means a cascaded system can be replaced by a single system. The impulse response is the convolution of all the impulse responses of individual systems.</p>","tags":["neural networks","momentum"]},{"location":"neural_networks/convolutional_neural_networks/convolutions/#4-distributive-property","title":"4. Distributive Property","text":"\\[ a[n] * b[n] + a[n] * c[n] = a[n] * (b[n] + c[n]) \\tag{12}\\] <p>It provides idea of how parallel systems work. If \\(n\\) systems share the common input and their outputs are finally added (or subtracted), we can repalce it with a single system with impulse response being the addition (or subtraction) of individual impulse responses  of the systems.</p>","tags":["neural networks","momentum"]},{"location":"neural_networks/convolutional_neural_networks/convolutions/#correlation","title":"Correlation","text":"<p>Correlation is another operation that is somewhat similar to convolution. Correlation uses two signals and produces an output called the cross correlation of the two input signals. If a signal is correlated with itself, the output is called autocorrelation.</p> <p>The main difference between calculating the output of correlation from that of convolution is that the kernel is not flipped.</p> <p>If convolution, \\(y[n]\\), of \\(a[n]\\) and \\(b[n]\\) is:</p> \\[y[n] = a[n] * b[n]\\] <p>then the correlation, \\(c[n]\\), of \\(a[n]\\) and \\(b[n]\\) is:</p> \\[c[n] = a[n] * b[-n]\\] <p>The formula for correlation will be:</p> \\[c[i] = \\sum_{j=0}^{n_k-1}a[i+j]b[j] \\tag{13}\\] <p>for a kernel that runs from sample number \\(0\\) to \\(n_k-1\\).</p> <p>Correlation, unlike convolution, is not commutative. The order matters. We cannot interchange the signals and expect the same cross-correlation. The second signal, which is moved along the first signal is called a target signal, \\(t[n]\\). It is called so because correlation is a technique of pattern recognition. The amplitude of the output signal at any point is a measure of how much the input signal resembles the target signal at that point.</p> <p>It means a peak in the cross-correlation will mean a high resemblance, while a negative peak means the opposite.</p> <p>Let's see a quick example of it.</p> <pre><code>pattern = np.sin(np.linspace(np.pi/2,np.pi,21))\nsignal = np.hstack((np.zeros(50),pattern,np.zeros(50), pattern, np.zeros(50), -pattern,np.zeros(50)))\nnoise = np.random.random(len(signal))*0.2*2-1\nsignal = signal + noise # add some noise\n</code></pre> <pre><code>f = plot_all(input_signal=signal, kernel=pattern[::-1], # we send in the reversed kernel because it is correlation\n            paddings=[0, (len(pattern)-1)//2, None], fig_size=(30,12))\n</code></pre> <p></p> <p>The above plot shows how different paddings for correlation can have spikes at different points for the same pattern, ex: in zero padding, the peak is at the corresponding sample number when the pattern starts. In same padding, the peak occurs at the same sample number as the middle of the pattern. In full padding, the peak occurs at the sample number when the pattern ends. Notice the negative peak for an opposite pattern.</p> <p>Also, the kernel(pattern) is reversed, as we sent it that way, so that the convolution function will reverse it again making it a correlation.</p> <p>In correlation a target signal goes along the input signal, multiplying the corresponding elements and summing them up. Now if the target signal happens to coincide with that portion of the input signal, it will only generate positive values which will sum up to a bigger number than the surrounding points. Convolution does the same, except it flips the kernel first, so in a way, convolution is matching the reverse of the kernel in the input signal.</p> <p>Note: Convolution and Correlation seem very similar, but they are very different operations when it comes to the Signal Processing. Convolution is related to how linear systems produce outputs while correlation is used for pattern matching in signals. It is just that their math is very similar.</p> <p>I think this is a good stopping point. Although we only covered one dimensional signals, this gave us a good idea into how and what convolution as well as correlation is and does. We will next start how we can convolve in higher dimensional spatial signal, like pictures and other data. Since ConvNets are mostly used on image data, we will introduce those concepts in the next one along with the basic idea of how convolutions work.</p>","tags":["neural networks","momentum"]},{"location":"neural_networks/convolutional_neural_networks/convolutions/#references","title":"References","text":"<ul> <li>Steven W. Smith. The Scientist and Engineer's Guide to Digital Signal Processing.</li> <li>I. Goodfellow, Y. Bengio and A. Courville. Deep Learning.</li> <li>https://lpsa.swarthmore.edu/Convolution/Convolution.html</li> </ul>","tags":["neural networks","momentum"]},{"location":"neural_networks/convolutional_neural_networks/introduction/","title":"Convolutional Neural Networks","text":"","tags":["neural","networks","convolution","image","recognition","pattern","recognition","deep","networks"]},{"location":"neural_networks/convolutional_neural_networks/introduction/#introduction","title":"Introduction","text":"<p>On September 30, 2012, Alex Krizhevsky competed in the ImageNet Large Scale Visual Recognition Challenge. His submission achieved a top-5 error rate of 15.3%. The runner up had an error rate of 26.1% giving a huge 10.8 points lead. What was so special about this model? He will go on to publish his model architechture and techniques in his paper, AlexNet. This would revolutionize the Deep Learning community that had been dormant for long. At the time of writing this(April 2020), AlexNet paper has been cited about 60,833 times. The one of the most important feature, among others, was that it was a Convolutional Neural Network.</p> <p>This series will be devoted to learn about the Convolutional Neural Networks -  the mathematics, the implementations, the results. Nowadays, CNNs have become very popular in the visual learning and so it is mostly taught with image examples. I, however, would like to take other examples as well, so that the reader may not find it highly correlating with just the visual aspect of it and lose sight of the general notion of CNNs. </p> <p>We will first start with convolutions, more precisely, discrete convolutions. We will try to understand what a convolution is. Why do we do convolutions at all? What does it mean to do a convolution? and the mathematics and other parts around it.</p> <p>Check out Convolutions</p>","tags":["neural","networks","convolution","image","recognition","pattern","recognition","deep","networks"]},{"location":"neural_networks/multiayer-perceptron/","title":"Multi-Layer Perceptron","text":"<p>{% include lists.md %}</p>","tags":["neural networks","perceptron","perceptron convergence theorem","backpropagation"]},{"location":"neural_networks/multiayer-perceptron/improvements-to-mlp/","title":"3 Improvements for the network","text":"","tags":["neural networks","momentum"]},{"location":"neural_networks/multiayer-perceptron/improvements-to-mlp/#31-introduction","title":"3.1 Introduction","text":"<p>As you can see, the network takes a lot of time to train. Sometimes the network may not converge at all even after a lot of iterations. It is because the error get's stuck at a local minima. Since we update the weights from all the examples at the same time, the error follows the direction of the steepest gradient. But the steepest gradrient may lead to a local minima and so no number of iterations will help it overcome that minima. These are problems with the gradient descent algorithm and hence apply to every model that uses it not just the MLP. Now there are ways around it:</p>","tags":["neural networks","momentum"]},{"location":"neural_networks/multiayer-perceptron/improvements-to-mlp/#1-changing-learning-rate","title":"1. Changing Learning Rate:","text":"<p>If the local minima has a small \"width\", then a higher learning rate may just jump over it. However it makes the network unstable.</p>","tags":["neural networks","momentum"]},{"location":"neural_networks/multiayer-perceptron/improvements-to-mlp/#2-multiple-models-with-random-initialization","title":"2. Multiple models with random initialization:","text":"<p>We train the same model multiple times with different starting points and maybe from any other starting point the steepest gradient is towards global minima. This is very effective for small models. But some larger models take weeks to train, which have hundreds of millions of datapoints and we cannot afford to train multiple models.</p>","tags":["neural networks","momentum"]},{"location":"neural_networks/multiayer-perceptron/improvements-to-mlp/#3-mini-batch-and-stochastic-gradient-descent","title":"3. Mini-Batch and Stochastic Gradient Descent:","text":"<p>The algorithm we have implemented is called a Batch Gradient Descent. We update the weights for every example at the same time. But sometimes the dataset is too large to perform matrix operations and so we use small random batches of data from the large dataset and update weights for them. We keep on doing this until all the data is utilized and then start another iteration again. It is called Mini-Batch Gradient Descent. Now at each weight update i.e at every step of gradient descent isn't towards the steepest gradient of error for all examples but for just this mini-batch that we used. The downside is it may not take the best steps towards the minima but will eventually reach there after taking a \"longer road\". The upside is it may sometimes not get stuck at a local minima as it may not have the steepest gradient for that training batch. Since it can avoid a local minima, we often use it in a dataset where complete batch gradient descent can be used as well. The number of examples used in a batch is called batch size. If the batch size is one,i.e if we update weight after every example, it is called Stochastic Gradient Descent. Remember, bigger the batch size, better steps it will take to minima and smaller the batch size, more deviated the path and so higher chances of avoiding local minima. There is a middle ground we need to find to make the training better. Remember it may also cause to deviate from the global minimum and reach a local minimum.</p>","tags":["neural networks","momentum"]},{"location":"neural_networks/multiayer-perceptron/improvements-to-mlp/#32-sgd-and-mini-batch-implementation","title":"3.2 SGD and Mini-Batch Implementation","text":"<p>Let's do a batch implementation of MLP and let's also include a regression version.</p> <pre><code>class MLP_batch:\n    def init_weights(self,layer_sizes,random_state):\n        #save weights in a list of matrices\n        np.random.seed(random_state)\n        self.weights = [np.random.rand(layer_sizes[l-1]+1,layer_sizes[l])*(2/np.sqrt(layer_sizes[l-1]))-(1/np.sqrt(layer_sizes[l-1])) for l in range(1,len(layer_sizes))]\n    def sigmoid(self,x):\n        return 1/(1+np.exp(-x)) # keep beta = 1\n\n    def forward(self,A_0,is_regression,weights=None):\n        self.outputs=[]\n        A_l = A_0\n        self.outputs.append(A_0)\n        if weights is None:\n            weights = self.weights\n        for weight in weights:\n            A_lbias = np.concatenate(((-np.ones((A_l.shape[0],1)),A_l)),axis=1) # add bias to input data\n            H_l = np.matmul(A_lbias,weight) # compute the summation\n            A_l = self.sigmoid(H_l) # compute the activation\n            self.outputs.append(A_l)\n        if is_regression:\n            A_l = H_l\n            self.outputs.pop()\n            self.outputs.append(A_l)\n        return A_l # return the final output\n\n    def backward(self,T, learning_rate,is_regression,batch_size):\n        A_L = self.outputs[-1]\n        if is_regression:\n            delta_L = A_L-T\n        else:\n            delta_L = (A_L-T)*(A_L*(1-A_L)) # beta = 0\n        delta_l_next = delta_L\n\n        for i in range(len(self.weights)-1,-1,-1):\n#             print(i)\n            A_l = self.outputs[i]\n            #compute error for previous layer\n            delta_l = A_l*(1-A_l)*(np.matmul(delta_l_next,np.transpose(self.weights[i][1:,:])))\n#             A_0 A_1 A_2\n#             W_1 W_2\n#             0   1    2\n            # add bias output to output matrix\n            A_lbias = np.concatenate(((-np.ones((A_l.shape[0],1)),A_l)),axis=1)\n            #update weights using the next errors\n            self.weights[i] = self.weights[i]- (1/batch_size)*(learning_rate*(np.matmul(np.transpose(A_lbias),delta_l_next)))\n            # change the next errors for next layer\n            delta_l_next = delta_l\n\n\n\n\n\n    def train(self,input_data,input_target, epochs,layer_sizes=(100,), \n              learning_rate=0.01,batch_size=None,is_regression=False,random_state=0,verbose=0, save_weights=False):\n\n\n        X = np.array(input_data)\n        Target = np.array(input_target)\n        layer_sizes=list(layer_sizes)\n        layer_sizes.insert(0,X.shape[1])\n        n_outputs = np.unique(Target).shape[0] if np.unique(Target).shape[0]  != 2 and not is_regression else 1\n        layer_sizes.append(n_outputs)\n\n\n        self.init_weights(layer_sizes, random_state=random_state)\n        if save_weights:\n            self.saved_weights = [self.weights.copy()]\n\n        if batch_size is None:\n            batch_size=X.shape[0]\n        for e in range(epochs):\n\n\n\n\n            # shuffle the input so we don't train on same sequences\n            idx = np.arange(0,Target.shape[0])\n            np.random.shuffle(idx)\n            X=X[idx]\n            Target=Target[idx]\n\n            b=0\n            while b&lt;X.shape[0]:\n                A_0=X[b:b+batch_size,:]\n                T=Target[b:b+batch_size,:]\n                A_L = self.forward(A_0,is_regression)\n                if e%((epochs//10)+1) == 0 and verbose:\n                    print(\"epoch:\",e)\n                    print(f\"Error: {np.sum((A_L-T)**2)/T.shape[0]}\")\n                    print(f\"out: {A_L}\")\n    #                 print(\"weights\",*self.weights,sep='\\n',end='\\n\\n')\n                self.backward(T,learning_rate,is_regression,batch_size)\n\n\n                if save_weights:\n                    self.saved_weights.append(self.weights.copy())\n\n                b=b+batch_size\n        print(f\"Error: {np.sum((A_L-T)**2)/T.shape[0]}\")\n\n    def predict(self,input_data,weights=None):\n        output = self.forward(np.array(input_data),weights)\n        #since this output is a realnumber(between 0 &amp; 1)\n        # we will have a threshold to predict its class for now 0.5\n        output = (output&gt;0.5)*1\n        return output\n\n    def confmat(self,input_data,targets):\n        '''returns the confusion matrix for binary classification'''\n        outputs = self.predict(np.array(input_data))\n        T = np.array(targets).reshape(outputs.shape)\n        tp = ((T==1)&amp;(outputs==1)).sum()\n        tn = ((T==0)&amp;(outputs==0)).sum()\n        fp = ((T==0)&amp;(outputs==1)).sum()\n        fn = ((T==1)&amp;(outputs==0)).sum()\n        return np.array([[tp,fp],\n                        [fn,tn]])\n</code></pre> <p>Let's make a simple regression dataset that has just one feature and it's target is a simple linear function to clear things.</p> <pre><code>np.random.seed(364)\nX = np.random.random((12000,1))\nT = 25*X+7.8\n</code></pre> <p>Let's now try different forms of gradient descent on it. Also set the random state so we have the same starting points. We will also check the training time for the same number of epochs along with the error.</p>","tags":["neural networks","momentum"]},{"location":"neural_networks/multiayer-perceptron/improvements-to-mlp/#322-results","title":"3.2.2 Results","text":"<pre><code>#Batch Gradient Descent(Full Dataset)\nbatch_model=MLP_batch()\n%time batch_model.train(X,T,layer_sizes=(),epochs=80,is_regression=True,learning_rate=1,random_state=0)\n</code></pre> <pre><code>Error: 0.0004875391124500146\nCPU times: user 138 ms, sys: 47 \u00b5s, total: 138 ms\nWall time: 81.9 ms\n</code></pre> <pre><code>mini_batch1024 = MLP_batch()\n%time mini_batch1024.train(X,T,layer_sizes=(),epochs=80,is_regression=True,learning_rate=1,random_state=0,batch_size=1024)\n</code></pre> <pre><code>Error: 1.834208786827062e-29\nCPU times: user 197 ms, sys: 0 ns, total: 197 ms\nWall time: 100 ms\n</code></pre> <pre><code>mini_batch64 = MLP_batch()\n%time mini_batch64.train(X,T,layer_sizes=(),epochs=80,is_regression=True,learning_rate=1,random_state=0,batch_size=64)\n</code></pre> <pre><code>Error: 4.0429121392576855e-30\nCPU times: user 611 ms, sys: 3.93 ms, total: 615 ms\nWall time: 614 ms\n</code></pre> <pre><code># for SGD, we will set batch_size=1\nSGD = MLP_batch()\n%time SGD.train(X,T,layer_sizes=(),epochs=80,is_regression=True,learning_rate=1,random_state=0,batch_size=1)\n</code></pre> <pre><code>Error: 0.0\nCPU times: user 30.8 s, sys: 347 ms, total: 31.2 s\nWall time: 30.7 s\n</code></pre> <p>As you can see, SGD performed the best while taking a lot of time to train. Both of these because it made much more weight updates than any of them. For equal number of weight updates, the batch version should perform best.</p> <p>Now let's try keeping the weight updates to be same, the best way is to let SGD go over the dataset atleast once. So that is a 12000 weight updates.</p> <pre><code>#Batch Gradient Descent(Full Dataset)\nbatch_model=MLP_batch()\n%time batch_model.train(X,T,layer_sizes=(),epochs=12000,is_regression=True,learning_rate=1e-2,random_state=0)\n</code></pre> <pre><code>Error: 3.255002724680526e-06\nCPU times: user 13.6 s, sys: 90.9 ms, total: 13.7 s\nWall time: 6.87 s\n</code></pre> <pre><code>mini_batch1024 = MLP_batch()\n%time mini_batch1024.train(X,T,layer_sizes=(),epochs=1024,is_regression=True,learning_rate=1e-2,random_state=0,batch_size=1024)\n</code></pre> <pre><code>Error: 3.2897801810988825e-06\nCPU times: user 2.34 s, sys: 15.8 ms, total: 2.35 s\nWall time: 1.18 s\n</code></pre> <pre><code>mini_batch64 = MLP_batch()\n%time mini_batch64.train(X,T,layer_sizes=(),epochs=64,is_regression=True,learning_rate=1e-2,random_state=0,batch_size=64)\n</code></pre> <pre><code>Error: 3.473449823532693e-06\nCPU times: user 515 ms, sys: 25 \u00b5s, total: 515 ms\nWall time: 512 ms\n</code></pre> <pre><code># for SGD, we will set batch_size=1\nSGD = MLP_batch()\n%time SGD.train(X,T,layer_sizes=(),epochs=1,is_regression=True,learning_rate=1e-2,random_state=0,batch_size=1)\n</code></pre> <pre><code>Error: 6.853664577305198e-06\nCPU times: user 480 ms, sys: 40.5 ms, total: 521 ms\nWall time: 464 ms\n</code></pre> <p>As you can see the opposite has happened, the batch version has done great but in most time and SGD has done worst in least time. But the mini batch of 1024 examples has done acceptable in very small time. We could prefer this model over both the models. It is the middle ground we need to find and depends on what can we spare, time or cost? It also depends on the type of dataset we are using. That stuff is for trial and error, atleast for now.</p>","tags":["neural networks","momentum"]},{"location":"neural_networks/multiayer-perceptron/improvements-to-mlp/#33-visualization","title":"3.3 Visualization","text":"<p>Now let's see how the weights are updated. But before that let's project out data error in a chart. Let's see how the error varies as with the weight. We will plot the weights on x and y axis and error on z axis.</p> <pre><code>def compute_multiple_error(X,T,xx,yy):\n    print(\"computing error\")\n    e = (T[0]-(xx*X[0]-yy))**2\n    for i in range(1,len(X)):\n        if (i+1)%1000==0:\n            print(f\"{i+1}/{len(X)} done\")\n        e = e+(T[i]-(xx*X[i]-yy))**2\n    return (1/T.shape[0])*e\n</code></pre> <pre><code>xx,yy = np.meshgrid(np.linspace(-5,35,1000),np.linspace(-30,5,1000))\nerr = compute_multiple_error(X,T,xx,yy)\n</code></pre> <pre><code>computing error\n1000/12000 done\n2000/12000 done\n3000/12000 done\n4000/12000 done\n5000/12000 done\n6000/12000 done\n7000/12000 done\n8000/12000 done\n9000/12000 done\n10000/12000 done\n11000/12000 done\n12000/12000 done\n</code></pre> <pre><code>fig = go.Figure()\nfig.add_trace(go.Surface(x=xx,y=yy,z=err))\nfig.update_layout(\n        scene = {\n            \"xaxis\": {\"title\": \"Weight 1\"},\n            \"zaxis\": {\"title\": \"Error\"},\n            \"yaxis\": {\"title\": \"Weight 2\"}\n        })\nfig.show()\n</code></pre> <p>{% include mlp/plot8.html %}</p> <p>As you can see, this surface has a minimum. A 3D graph looks clumsy, let's use contours of this 3D plot. However plotly doesn't support uneven labels, we will use the logarithm of error to make the plots have regular contours.</p> <pre><code>fig = go.Figure(data=\n                [go.Contour(\n                    x=xx[0],\n                    y=yy[:,0],\n                    z=np.log1p(err),\n                    contours_coloring='lines',\n                    line_width=2,\n                    showscale=False,\n                )\n                ],\n                layout=dict(xaxis_title=\"Weight 1\",\n                           yaxis_title=\" Weight 2\",\n                           width=915, height=800,),\n\n               )\nfig.add_trace(go.Scatter(x=[25],y=[-7.8],marker=dict(symbol=\"x\", size=10)))\nfig.show()\n</code></pre> <p>{% include mlp/plot9.html %}</p> <p>These are the contour lines and the cross(x) sign is the minimum of the error, we want our models to descend to that point. Let's see how each model does it. Each line has zero gradient(i.e they lie on the same level) and the perpendicular direction at each line is the steepest gradient.</p> <pre><code>batch_model=MLP_batch()\nbatch_model.train(X,T,layer_sizes=(),epochs=150,is_regression=True,learning_rate=1,random_state=0,save_weights=True)\n\nmini_batch1024 = MLP_batch()\nmini_batch1024.train(X,T,layer_sizes=(),epochs=100,is_regression=True,learning_rate=1,random_state=0,\n                     batch_size=1024,save_weights=True)\nmini_batch64 = MLP_batch()\nmini_batch64.train(X,T,layer_sizes=(),epochs=1,is_regression=True,learning_rate=1,random_state=0,\n                     batch_size=64,save_weights=True)\n\n\nSGD = MLP_batch()\nSGD.train(X,T,layer_sizes=(),epochs=1,is_regression=True,learning_rate=1,random_state=0,batch_size=1,save_weights=True)\n</code></pre> <pre><code>Error: 3.796053431101478e-08\nError: 1.8660418967252465e-29\nError: 2.1144567049716302e-10\nError: 0.0\n</code></pre> <pre><code>weights=[np.concatenate([i[0] for i in m.saved_weights],axis=1) for m in (batch_model,mini_batch1024,mini_batch64,SGD)]\n</code></pre> <p>Let's now draw every weight update</p> <pre><code>fig = go.Figure()\n\nfig.add_trace(go.Contour(x=xx[0], y=yy[:,0], z=np.log1p(err), autocontour=False, \n                         line_width=2, showscale=False,contours_coloring='lines', \n                        ),\n             )\nfig.add_trace(go.Scatter(x=[25],y=[-7.8],mode=\"markers\",marker=dict(symbol=\"x\", size=10, color=\"red\"),\n                         name=\"Minimum\"),)\n\nfor p,name,color,opacity in zip(range(4),\n                        ['Full Batch','1024','64','1(SGD)'],\n                        ['Blue',\"Cyan\",\"Green\",\"Magenta\"],\n                        [1,0.8,0.8,0.8]):\n    fig.add_trace(go.Scatter(x=weights[p][1,:],y=weights[p][0,:],name=name,opacity=opacity,\n                             mode=\"lines+markers\",line=dict(color=color, width=2)),)\n\n\nfig.update_layout(legend=dict(x=0.8, y=1),height=800,width=915)\n\nfig.show()\n</code></pre> <p>{% include mlp/plot10.html %}</p> <p>As you can see, the bigger the batch size, the better the weight updates are. Also look the full batch version took steps which were perpendicular to the contour line at every point(i.e the steepest gradient). But we cannot always afford to use full dataset at one time. And besides, this random-looking update might help us to deviate from local minima. Still we can use a few more tricks to make it faster and better.</p>","tags":["neural networks","momentum"]},{"location":"neural_networks/multiayer-perceptron/improvements-to-mlp/#34-the-momentum","title":"3.4 The Momentum","text":"<p>While using the Mini-Batch or Stochastic Gradient Descent, we use certain ways to keep the weights on track and not deviate too much. One of them is called picking up the momentum. Since in SGD or MBD, each weight update is based upon just the examples we are considering, the weights are change according to just that batch and does not consider the whole dataset and make training longer, especially when there is no local minima and traing set is very large. </p> <p>The idea is to not just update the weights from the current batch's \\(\\Delta w\\), but from the previous batch as well. So, we update the weight of any layer \\(l\\), iteration \\(t\\) as: </p> \\[ w^{(l,t)}_{pq} \\leftarrow w^{(l,t-1)}_{pq} - \\Delta w^{(l,t)}_{pq}\\tag{90} \\] <p>where,</p> \\[ \\Delta w^{(l,t)}_{pq} = \\eta \\delta^{(l)}_qa^{(l-1)}_p + \\alpha \\Delta w^{(l,t-1)}_{pq}\\tag{91} \\] <p>and </p> <ol> <li>\\(w^{(l,t)}_{pq}\\) is the weight of layer \\(l\\) of iteration \\(t\\).</li> <li>\\(\\alpha\\) is the weightage for the previous weight update. \\(0.9\\) is a good number.</li> </ol> <p>Note: iteration means a weight update and not an epoch.</p> <p>However that method is effective as it can be, but we will need to tune learning rate again and it has different impact. The current weight has a weight of \\(\\eta\\) while the previous weight updates have \\(\\alpha\\) weight. Earlier all of it had weight of \\(\\eta\\). So the proposition here is to assign a weight of \\(1-\\alpha\\) to the current update and \\(\\alpha\\) to previous ones so to make a total weight of unity and then we use the learning rate all over it.</p> <p>Like:</p> \\[ \\Delta w^{(l,t)}_{pq} = \\eta \\big((1-\\alpha)\\delta ^{(l)}_qa^{(l-1)}_p + \\alpha \\Delta w^{(l,t-1)}_{pq}\\big) \\tag{92} \\] <p>It can be easily done by:</p> <pre><code>delta_w = (1-momentum)*(1/batch_size)*(np.matmul(np.transpose(A_lbias),delta_l_next)) + momentum*delta_w\nweight = weight - learning_rate*delta_w\n</code></pre> <p>Let's try to write it up. We need just a few changes in the code.</p> <pre><code>class MLP_momentum:\n    def init_weights(self,layer_sizes,random_state):\n        #save weights in a list of matrices\n        np.random.seed(random_state)\n        self.weights = [np.random.rand(layer_sizes[l-1]+1,layer_sizes[l])*(2/np.sqrt(layer_sizes[l-1]))-(1/np.sqrt(layer_sizes[l-1])) for l in range(1,len(layer_sizes))]\n    def sigmoid(self,x):\n        return 1/(1+np.exp(-x)) # keep beta = 1\n\n    def forward(self,A_0,is_regression,weights=None):\n        self.outputs=[]\n        A_l = A_0\n        self.outputs.append(A_0)\n        if weights is None:\n            weights = self.weights\n        for weight in weights:\n            A_lbias = np.concatenate(((-np.ones((A_l.shape[0],1)),A_l)),axis=1) # add bias to input data\n            H_l = np.matmul(A_lbias,weight) # compute the summation\n            A_l = self.sigmoid(H_l) # compute the activation\n            self.outputs.append(A_l)\n        if is_regression:\n            A_l = H_l\n            self.outputs.pop()\n            self.outputs.append(A_l)\n        return A_l # return the final output\n\n    def backward(self,T, learning_rate,is_regression,batch_size,momentum,delta_w):\n        A_L = self.outputs[-1]\n        if is_regression:\n            delta_L = A_L-T\n        else:\n            delta_L = (A_L-T)*(A_L*(1-A_L)) # beta = 0\n        delta_l_next = delta_L\n\n        for i in range(len(self.weights)-1,-1,-1):\n#             print(i)\n            A_l = self.outputs[i]\n            #compute error for previous layer\n            delta_l = A_l*(1-A_l)*(np.matmul(delta_l_next,np.transpose(self.weights[i][1:,:])))\n#             A_0 A_1 A_2\n#             W_1 W_2\n#             0   1    2\n            # add bias output to output matrix\n            A_lbias = np.concatenate(((-np.ones((A_l.shape[0],1)),A_l)),axis=1)\n\n            # compute delta_w\n            delta_w[i] = (1-momentum)*(1/batch_size)*(np.matmul(np.transpose(A_lbias),delta_l_next)) + momentum*delta_w[i]\n\n\n            #update weights using the next errors\n            self.weights[i] = self.weights[i] - learning_rate*delta_w[i]\n            # change the next errors for next layer\n            delta_l_next = delta_l\n        return delta_w\n\n\n\n\n\n    def train(self, input_data, input_target, epochs, layer_sizes=(100,), \n              learning_rate=0.01, batch_size=None, is_regression=False,\n              momentum=0.9, random_state=0,verbose=0, save_weights=False,warm_start=False):\n\n\n        X = np.array(input_data)\n        Target = np.array(input_target)\n        layer_sizes=list(layer_sizes)\n        layer_sizes.insert(0,X.shape[1])\n        n_outputs = Target.shape[1]\n        layer_sizes.append(n_outputs)\n\n        if not warm_start:\n            self.init_weights(layer_sizes, random_state=random_state)\n        if save_weights:\n            self.saved_weights = [self.weights.copy()]\n\n        if batch_size is None:\n            batch_size=X.shape[0]\n\n        # initialize delta_w to be zero for every layer\n        delta_w = [0*i for i in self.weights].copy()\n        for e in range(epochs):\n\n\n\n\n            # shuffle the input so we don't train on same sequences\n            idx = np.arange(0,Target.shape[0])\n            np.random.shuffle(idx)\n            X=X[idx]\n            Target=Target[idx]\n\n            b=0\n            while b&lt;X.shape[0]:\n                A_0=X[b:b+batch_size,:]\n                T=Target[b:b+batch_size,:]\n                A_L = self.forward(A_0,is_regression)\n                if e%((epochs//10)+1) == 0 and verbose:\n                    print(\"epoch:\",e)\n                    print(f\"Error: {np.sum((A_L-T)**2)/T.shape[0]}\")\n#                     print(f\"out: {A_L}\")\n    #                 print(\"weights\",*self.weights,sep='\\n',end='\\n\\n')\n                delta_w = self.backward(T,learning_rate,is_regression,batch_size,momentum,delta_w)\n\n\n                if save_weights:\n                    self.saved_weights.append(self.weights.copy())\n                b=b+batch_size\n        return np.sum((A_L-T)**2)/T.shape[0]\n\n    def predict(self,input_data,weights=None):\n        output = self.forward(np.array(input_data),weights)\n        #since this output is a realnumber(between 0 &amp; 1)\n        # we will have a threshold to predict its class for now 0.5\n        output = (output&gt;0.5)*1\n        return output\n\n    def confmat(self,input_data,targets):\n        '''returns the confusion matrix for binary classification'''\n        outputs = self.predict(np.array(input_data))\n        T = np.array(targets).reshape(outputs.shape)\n        tp = ((T==1)&amp;(outputs==1)).sum()\n        tn = ((T==0)&amp;(outputs==0)).sum()\n        fp = ((T==0)&amp;(outputs==1)).sum()\n        fn = ((T==1)&amp;(outputs==0)).sum()\n        return np.array([[tp,fp],\n                        [fn,tn]])\n</code></pre> <pre><code>SGD = MLP_momentum()\nSGD.train(X,T,layer_sizes=(),epochs=1,is_regression=True,learning_rate=1,random_state=0,\n          batch_size=1,save_weights=True,momentum=0)\nSGD_mom = MLP_momentum()\nSGD_mom.train(X,T,layer_sizes=(),epochs=1,is_regression=True,learning_rate=\n              1,random_state=0,\n          batch_size=1,save_weights=True,momentum=0.8)\n</code></pre> <pre><code>0.0\n</code></pre> <pre><code>weights=[np.concatenate([i[0] for i in m.saved_weights],axis=1) for m in (SGD,SGD_mom)]\n</code></pre> <p>Let's now draw every weight update</p> <pre><code>fig = go.Figure()\n\nfig.add_trace(go.Contour(x=xx[0], y=yy[:,0], z=np.log1p(err), autocontour=False, \n                         line_width=2, showscale=False,contours_coloring='lines', \n                        ),\n             )\nfig.add_trace(go.Scatter(x=[25],y=[-7.8],mode=\"markers\",marker=dict(symbol=\"x\", size=10, color=\"red\"),\n                         name=\"Minimum\"),)\n\nfor p,name,color in zip(range(2),\n                        ['Without Momentum(SGD)','With Momentum(SGD)'],\n                        ['Blue',\"Red\",],\n                               ):\n    fig.add_trace(go.Scatter(x=weights[p][1,:],y=weights[p][0,:],name=name,opacity=opacity,\n                             mode=\"lines+markers\",\n                             line=dict(color=color, width=2)),\n                 )\n\n\n\nfig.update_layout(legend=dict(x=0.8, y=1),height=800,width=915,xaxis_title=\"Weight 1\", yaxis_title=\"Weight 2\",\n                 title=\"Contour Plot of Error\")\n\nfig.show()\n</code></pre> <p>{% include mlp/plot11.html %}</p> <p>As you can see, the updates are much more smooth for SGD. This also makes it possible to use smaller learning rate and thus making it more stable. Let's now see for a complete batch and see something different.</p> <pre><code>batch_model=MLP_momentum()\nbatch_model.train(X,T,layer_sizes=(),epochs=150,is_regression=True,learning_rate=1,\n                  random_state=0,save_weights=True,momentum=0)\n\nbatch_model_mom=MLP_momentum()\nbatch_model_mom.train(X,T,layer_sizes=(),epochs=150,is_regression=True,learning_rate=1,\n                      random_state=0,save_weights=True,momentum=0.9)\n</code></pre> <pre><code>3.8757600362031304e-05\n</code></pre> <pre><code>weights=[np.concatenate([i[0] for i in m.saved_weights],axis=1) for m in (batch_model,batch_model_mom)]\n</code></pre> <pre><code>fig = go.Figure()\n\nfig.add_trace(go.Contour(x=xx[0], y=yy[:,0], z=np.log1p(err), autocontour=False, \n                         line_width=2, showscale=False,contours_coloring='lines', \n                        ),\n             )\nfig.add_trace(go.Scatter(x=[25],y=[-7.8],mode=\"markers\",marker=dict(symbol=\"x\", size=10, color=\"red\"),\n                         name=\"Minimum\"),)\n\nfor p,name,color in zip(range(2),\n                        ['Without Momentum(Batch)','With Momentum(Batch)'],\n                        ['Blue',\"Red\",],\n                               ):\n    fig.add_trace(go.Scatter(x=weights[p][1,:],y=weights[p][0,:],name=name,opacity=opacity,\n                             mode=\"lines+markers\",\n                             line=dict(color=color, width=2)),\n                 )\n\n\n\nfig.update_layout(legend=dict(x=0.8, y=1),height=800,width=915,xaxis_title=\"Weight 1\", yaxis_title=\"Weight 2\",\n                 title=\"Contour Plot of Error\")\n\nfig.show()\n</code></pre> <p>{% include mlp/plot12.html %}</p> <p>Here are a few important points to note: 1. The step size increases as we go along the right gradient but decreases and eventually stops and changes once we are not moving along the gradient as if the weights cannot stop and have a certain momentum like a block of mass.</p> <ol> <li> <p>If we use the complete dataset as the batch, then, without momentum, it will take the best steps along the steepest gradient to the minima(local or global). But with momentum, it wanders off a little. It may even reach the minimum(local ot global) and yet go past it a bit(like a ball with momentum would). If it was a local minima, it may just cross the barrier and move to global minima. If it was a global minima, then it would oscillate a little and eventually stop.</p> </li> <li> <p>For mini batch, it will depend on the batch size. If the batch size is too small, then it will act as it does for SGD,(i.e smoothen the weight updates) and if the batch size is too big, then it will act as it does for complete batch(i.e, deviate and move past minima). The batch size will decide the degree of it.</p> </li> </ol> <p>Our XOR data has probably a local minima(or a saddle point) which does not allow all the models to converge at the global minimum. Some initializations get stuck at a local minima. Here is one. try changing the epochs and learning rates, it is still very hard to get a very low error score. So we can improve it by the momentum. With the right momentum and the correct learning rate, we can get past this minima and get a lower score in very few epochs.</p> <pre><code>simple_model = MLP()\nsimple_model.train(XOR_inp,XOR_target,30001,layer_sizes=(2,),learning_rate=5, random_state=365)\n</code></pre> <pre><code>Error: 0.1250841346167755\n</code></pre> <pre><code>mom_model = MLP_momentum()\nmom_model.train(XOR_inp,XOR_target,30001,layer_sizes=(2,),learning_rate=5, random_state=365,momentum=0.9)\n</code></pre> <pre><code>3.434487809294881e-05\n</code></pre> <p>Another thing we try is called Weight Decay. The argument goes as small weights try to keep the outputs in the linear middle part of the sigmoid which change faster than at the end points(because of the gradient). It is done by multiplying all the weights by a small constant \\(\\epsilon\\) after each iteraton. It makes the network simpler and  may improve the results, but it can also sometimes make the learning significantly worse. Choosing \\(\\epsilon\\) is done experimentally.</p> <p>We will discuss more optimizations in detail later on.</p>","tags":["neural networks","momentum"]},{"location":"neural_networks/multiayer-perceptron/improvements-to-mlp/#35-misc","title":"3.5 Misc","text":"","tags":["neural networks","momentum"]},{"location":"neural_networks/multiayer-perceptron/improvements-to-mlp/#351-amount-of-training-data","title":"3.5.1 Amount of Training Data","text":"<p>Since there are many parameters(weights) to look for, there actually should be sufficient data to learn that. The amont of data actually depends on the problem, but a rule of thumb is to use data 10 times the number of weights. </p>","tags":["neural networks","momentum"]},{"location":"neural_networks/multiayer-perceptron/improvements-to-mlp/#352-number-of-hidden-layers-and-nodes","title":"3.5.2 Number of Hidden layers and Nodes","text":"<p>These are two important decisions needed to be made to get a good result. How many hidden layers to use and how many nodes in each layer? Although there is no theory for number of nodes but according to Universal Approximation Theorem, just one hidden layer will do just fine. A combination of sigmoid layers can approxiamte to any function with the sufficient number of nodes. However that one layer might need a lot of hidden nodes, so two hidden layers are usually used to be safe. The two hidden layer systemwill work for most of the problems but there are always exceptions.</p> <p>We will talk more about the Universal Approximation Theorem later on.</p>","tags":["neural networks","momentum"]},{"location":"neural_networks/multiayer-perceptron/improvements-to-mlp/#352-when-to-stop-training","title":"3.5.2 When to Stop Training?","text":"<p>This is also an important factor deciding the results. We do not want to overfit the data nor to we want to underfit it. So it is better to use a validation data to check. We will continue to train untill both the training and validation error are reducing. At the point the validation error starts increasing, we stop the training.</p> <p></p> <p>This technique is called early stopping.</p>","tags":["neural networks","momentum"]},{"location":"neural_networks/multiayer-perceptron/improvements-to-mlp/#4-mlp-in-practice","title":"4 MLP in Practice","text":"<p>Let's see some examples how MLP can be used. </p>","tags":["neural networks","momentum"]},{"location":"neural_networks/multiayer-perceptron/improvements-to-mlp/#41-a-regression-problem","title":"4.1 A Regression Problem","text":"<p>Let's try fitting a sine wave with a small noise as well.</p> <pre><code>np.random.seed(479)\nX = np.linspace(0,1,100).reshape(-1,1)\nT = np.sin(2*np.pi*X) + np.cos(4*np.pi*X)+np.random.randn(100,1)*0.4\n</code></pre> <pre><code># plot the data\nfig = go.Figure(go.Scatter(x=X.squeeze(),y=T.squeeze(), mode=\"markers\"),\n                layout=dict(xaxis_title=\"Input\", yaxis_title=\"Target Values\")\n               )\nfig.show()\n</code></pre> <p>{% include mlp/plot13.html %}</p> <p>Let's now train an MLP on this data. But before that we will normalize the data and split it into training validation and test data in the ratio 4:1:1.</p> <pre><code>X =(X-X.mean(axis=0))/X.var(axis=0)\nT =(T-T.mean(axis=0))/T.var(axis=0)\n</code></pre> <pre><code>idx = np.arange(X.shape[0])\nnp.random.shuffle(idx)\nX=X[idx,:]\nT=T[idx,:]\ntrain = X[:2*(X.shape[0]//3),:]\ntest = X[2*(X.shape[0]//3):2*(X.shape[0]//3)+(X.shape[0]//6):,:]\nval = X[2*(X.shape[0]//3)+(X.shape[0]//6):,:]\ntraintarget = T[:2*(X.shape[0]//3),:]\ntesttarget = T[2*(X.shape[0]//3):2*(X.shape[0]//3)+(X.shape[0]//6):,:]\nvaltarget = T[2*(X.shape[0]//3)+(X.shape[0]//6):,:]\n</code></pre> <pre><code># plot the data\nfig = go.Figure(data=[go.Scatter(x=train.squeeze(),y=traintarget.squeeze(),mode=\"markers\", name=\"training set\"),\n                     go.Scatter(x=val.squeeze(),y=valtarget.squeeze(),mode=\"markers\",name=\"validation set\")],\n                layout=dict(xaxis_title=\"Input\", yaxis_title=\"Target Values\")\n               )\nfig.show()\n</code></pre> <p>{% include mlp/plot14.html %}</p> <p>To start with, we will use one hidden layer, with 3 nodes and learning rate of 0.25 for 101 epochs and see what the output is.</p> <pre><code>m = MLP_momentum()\nm.train(train,traintarget,101,layer_sizes=(3,),learning_rate=0.25,is_regression=True,momentum=0.9,verbose=True,random_state=200)\n</code></pre> <pre><code>epoch: 0\nError: 0.9954347804927682\nepoch: 11\nError: 0.695684508117699\nepoch: 22\nError: 0.6610644383959196\nepoch: 33\nError: 0.6622483713072506\nepoch: 44\nError: 0.652190286530414\nepoch: 55\nError: 0.646344230816324\nepoch: 66\nError: 0.6426767619901053\nepoch: 77\nError: 0.63862336612565\nepoch: 88\nError: 0.6343649929165276\nepoch: 99\nError: 0.6300130633180782\n\n\n\n\n\n0.6296088646689288\n</code></pre> <p>As we can see, the error is decreasing, now we have to figure out a couple of things. First figure out the early stopping. We will keep track of last two validation errors after every 10 epochs to make sure the validation error actually increases(when it does!) and do not stop the program prematurely.</p> <pre><code>train_error=[]\nval_error=[]\nm = MLP_momentum()\nm.train(train,traintarget,100,layer_sizes=(5,),learning_rate=0.25,is_regression=True,momentum=0.9)\nold_val_error1 = np.infty\nold_val_error2 = np.infty\nnew_error = (1/val.shape[0])*np.sum((m.forward(val,is_regression=True)-valtarget)**2)\nval_error.append(new_error)\ntrain_error.append((1/train.shape[0])*np.sum((m.forward(train,is_regression=True)-traintarget)**2))\ncount = 0\nwhile (((old_val_error1-new_error)&gt;0.0005) or ((old_val_error2-old_val_error1)&gt;0.0005)):\n    count+=1\n    m.train(train,traintarget,100,layer_sizes=(5,),learning_rate=0.25,is_regression=True,momentum=0,warm_start=True)\n    old_val_error2=old_val_error1\n    old_val_error1=new_error\n    new_error = (1/val.shape[0])*np.sum((m.forward(val,is_regression=True)-valtarget)**2)\n    val_error.append(new_error)\n    train_error.append((1/train.shape[0])*np.sum((m.forward(train,is_regression=True)-traintarget)**2))\n\nprint(\"learning_stopped at epoch:\", count*100)\nextra_train_error=[]\nextra_val_error=[]\nfor i in range(10):\n    m.train(train,traintarget,100,layer_sizes=(5,),learning_rate=0.25,is_regression=True,momentum=0.9,warm_start=True)\n    old_val_error2=old_val_error1\n    old_val_error1=new_error\n    new_error = (1/val.shape[0])*np.sum((m.forward(val,is_regression=True)-valtarget)**2)\n    extra_val_error.append(new_error)\n    extra_train_error.append((1/train.shape[0])*np.sum((m.forward(train,is_regression=True)-traintarget)**2))\n</code></pre> <pre><code>learning_stopped at epoch: 1600\n</code></pre> <pre><code>fig = go.Figure(data=[\n    go.Scatter(y=train_error,name=\"train error before early stopping\"),\n    go.Scatter(y=val_error, name=\"validation error before early stoppping\"),\n    go.Scatter(x=list(range(len(train_error),len(train_error)+len(extra_train_error))),y=extra_train_error,\n               name= \"train error after early stopping\"),\n    go.Scatter(x=list(range(len(val_error),len(val_error)+len(extra_val_error))),y=extra_val_error,\n               name= \"validation error after early stopping\"),    \n],\n               layout=dict(xaxis_title=\"Epochs x100\", yaxis_title=\"Sum of Squares Error\"))\nfig.show()\n</code></pre> <p>{% include mlp/plot15.html %}</p> <p>As you can see there is not much change in the error after the early stopping point. We have found the for how long the network to be run but now let's figure how many hidden nodes to be used.</p> <p>We already have a early stopping function and we will run this for different sizes of hidden nodes and record the results. However we will run each model 10 times with different initialization and work on averages.</p> <pre><code>def early_stopping(train,traintarget,val,valtarget,epochs,layer_sizes,learning_rate,\n                   is_regression,momentum,random_state,diff=0.001):\n    m = MLP_momentum()\n    error=m.train(train,traintarget,epochs,layer_sizes=layer_sizes,learning_rate=learning_rate,\n            is_regression=is_regression,momentum=momentum,random_state=random_state)\n\n    old_val_error1 = 2000000000\n    old_val_error2 = 1999999999\n    new_error = (1/val.shape[0])*np.sum((m.forward(val,is_regression=True)-valtarget)**2)\n\n    count = 0\n    while (((old_val_error1-new_error)&gt;diff) or ((old_val_error2-old_val_error1)&gt;diff)):\n        count+=epochs\n        error = m.train(train,traintarget,epochs,layer_sizes=layer_sizes,learning_rate=learning_rate,\n            is_regression=is_regression,momentum=momentum,random_state=random_state,warm_start=True)\n\n        old_val_error2=old_val_error1\n        old_val_error1=new_error\n        new_error = (1/val.shape[0])*np.sum((m.forward(val,is_regression=True)-valtarget)**2)\n    return m, new_error\n</code></pre> <p>We will save error for every differnt initialization and work on averages, standard deviations, minimums and maximums. We will use a pandas dataframe to form a table.</p> <pre><code>import pandas as pd\n</code></pre> <pre><code>hidden_units = [1,2,3,5,10,25]\ndf = pd.DataFrame(index=[\"Mean Error\",\"Standard Deviation\", \"Max Error\", \"Min Error\"])\nfor unit in hidden_units:\n    errors = np.array([early_stopping(train,traintarget,val,valtarget,10,(unit,),0.25,True,0,\n                                      np.random.randint(0,12000))[1] for i in range(10)])\n    df[str(unit)]=[errors.mean(),errors.std(),errors.max(),errors.min()]\n    print(f\"Units done: {unit}\")\n</code></pre> <pre><code>Units done: 1\nUnits done: 2\nUnits done: 3\nUnits done: 5\nUnits done: 10\nUnits done: 25\n</code></pre> <pre><code>df\n</code></pre> <pre><code>|   | 1 | 2 | 3 | 5 | 10 | 25 |\n|---|---|---|---|---|---|---|\n|Mean Error |0.423110  |0.337027  |0.221558  |0.203000  |0.168734  |1.118023  |\n|Standard Deviation  |0.000311  |0.126717  |0.117290  |0.105529  |0.085867  |1.072914  |\n|Max Error  |0.423543  |0.429690  |0.405281  |0.421871  |0.422707  |3.377288  |\n|Min Error  |0.422477  |0.141333  |0.139736  |0.145992  |0.122757  |0.244395  |\n</code></pre> <p>Take a look at both the mean(or max) error and standard deviation of each model with their hidden nodes. We prefer the model with low error (mean and max) and low standard deviation. We can also change learning rate and test out the results. Also we can try more layers and optimize the nodes in each layer.</p>","tags":["neural networks","momentum"]},{"location":"neural_networks/multiayer-perceptron/improvements-to-mlp/#final-code","title":"Final Code","text":"<p>Before starting the next part we have to add a bunch of stuff to our code, implementing the cross-entropy error. We will also include the early stopping method with it. We will also fix the confusion matrix for multiple classes.</p> <pre><code>class MLP:\n    def init_weights(self,layer_sizes,random_state):\n        #save weights in a list of matrices\n        np.random.seed(random_state)\n        self.weights = [np.random.rand(layer_sizes[l-1]+1,layer_sizes[l])*(2/np.sqrt(layer_sizes[l-1]))-(1/np.sqrt(layer_sizes[l-1])) for l in range(1,len(layer_sizes))]\n    def sigmoid(self,x):\n        return 1/(1+np.exp(-x)) # keep beta = 1\n\n    def forward(self,A_0,is_regression,weights=None):\n        self.outputs=[]\n        A_l = A_0\n        self.outputs.append(A_0)\n        if weights is None:\n            weights = self.weights\n        for weight in weights:\n            A_lbias = np.concatenate(((-np.ones((A_l.shape[0],1)),A_l)),axis=1) # add bias to input data\n            H_l = np.matmul(A_lbias,weight) # compute the summation\n            A_l = self.sigmoid(H_l) # compute the activation\n            self.outputs.append(A_l)\n        if is_regression:\n            A_l = H_l\n            self.outputs.pop()\n            self.outputs.append(A_l)\n        return A_l # return the final output\n\n    def backward(self,T, learning_rate,is_regression,batch_size,momentum,delta_w,loss):\n        A_L = self.outputs[-1]\n        if is_regression:\n            delta_L = A_L-T\n        elif loss == 'sumofsquares':\n            delta_L = (A_L-T)*(A_L*(1-A_L)) # beta = 0\n        elif loss == 'crossentropy':\n#             delta_L = (T*(A_L-1)*A_L)\n            delta_L= A_L-T\n        delta_l_next = delta_L\n\n        for i in range(len(self.weights)-1,-1,-1):\n            A_l = self.outputs[i]\n            #compute error for previous layer\n            delta_l = A_l*(1-A_l)*(np.matmul(delta_l_next,np.transpose(self.weights[i][1:,:])))\n\n            # add bias output to output matrix\n            A_lbias = np.concatenate(((-np.ones((A_l.shape[0],1)),A_l)),axis=1)\n\n            # compute delta_w\n            delta_w[i] = (1-momentum)*(1/batch_size)*(np.matmul(np.transpose(A_lbias),delta_l_next)) + momentum*delta_w[i]\n\n\n            #update weights using the next errors\n            self.weights[i] = self.weights[i] - learning_rate*delta_w[i]\n\n            # change the next errors for next layer\n            delta_l_next = delta_l\n        return delta_w\n\n    def compute_error(self,A_L,T,loss):\n        if loss == 'sumofsquares':\n            return np.sum((A_L-T)**2)/T.shape[0]\n\n        if loss == 'crossentropy':\n            return -np.sum(T*(np.log(A_L))+(1-T)*(np.log(1-A_L)))/T.shape[0]\n\n\n\n\n    def train(self, input_data, input_target, epochs, layer_sizes=(100,), \n              learning_rate=0.01, batch_size=None,loss='sumofsquares', is_regression=False,\n              momentum=0.9, random_state=0,verbose=0, save_weights=False,warm_start=False):\n\n\n        X = np.array(input_data)\n        Target = np.array(input_target)\n        layer_sizes=list(layer_sizes)\n        layer_sizes.insert(0,X.shape[1])\n        n_outputs = Target.shape[1]\n        layer_sizes.append(n_outputs)\n\n        if not warm_start:\n            self.init_weights(layer_sizes, random_state=random_state)\n        if save_weights:\n            self.saved_weights = [self.weights.copy()]\n\n        if batch_size is None:\n            batch_size=X.shape[0]\n\n        # initialize delta_w to be zero for every layer\n        delta_w = [0*i for i in self.weights].copy()\n        for e in range(epochs):\n\n\n\n\n            # shuffle the input so we don't train on same sequences\n            idx = np.arange(0,Target.shape[0])\n            np.random.shuffle(idx)\n            X=X[idx]\n            Target=Target[idx]\n\n            b=0\n            while b&lt;X.shape[0]:\n                A_0=X[b:b+batch_size,:]\n                T=Target[b:b+batch_size,:]\n                A_L = self.forward(A_0,is_regression)\n                if e%((epochs//10)+1) == 0 and verbose:\n                    print(\"epoch:\",e)\n                    print(f\"Error: {self.compute_error(A_L,T,loss)}\")\n                delta_w = self.backward(T,learning_rate,is_regression,batch_size,momentum,delta_w,loss)\n\n\n                if save_weights:\n                    self.saved_weights.append(self.weights.copy())\n                b=b+batch_size\n        return self.compute_error(A_L,T,loss)\n\n    def early_stopping(self,train,traintarget,val,valtarget,epochs,layer_sizes,learning_rate,\n                       is_regression,momentum,batch_size=None, diff=0.01,random_state=0,loss='sumofsquares'):\n        error = self.train(train,traintarget,epochs,layer_sizes=layer_sizes,learning_rate=learning_rate,\n                           is_regression=is_regression,momentum=momentum,batch_size=batch_size,\n                           loss=loss,random_state=random_state)\n        old_val_error1 = 2000000000\n        old_val_error2 = 1999999999\n        new_error = self.compute_error(self.forward(val,is_regression=is_regression),valtarget,loss)\n\n        count = 0\n        while (((old_val_error1-new_error)&gt;diff) or ((old_val_error2-old_val_error1)&gt;diff)):\n            count+=epochs\n            error = self.train(train,traintarget,epochs,layer_sizes=layer_sizes,learning_rate=learning_rate,\n                is_regression=is_regression,momentum=momentum,random_state=random_state,loss=loss,warm_start=True)\n\n            old_val_error2=old_val_error1\n            old_val_error1=new_error\n            new_error = self.compute_error(self.forward(val,is_regression=is_regression),valtarget,loss)\n        return new_error\n\n\n    def predict(self,input_data,weights=None):\n        output = self.forward(np.array(input_data),weights)\n        #since this output is a realnumber(between 0 &amp; 1)\n        # we will have a threshold to predict its class for now 0.5\n        output = (output&gt;0.5)*1\n        return output\n\n    def confmat(self,input_data,targets):\n        '''returns the confusion matrix for binary classification'''\n        out = np.argmax(m.forward(input_data,is_regression=False),axis=1)\n        t = np.argmax(targets,axis=1)\n        mat = np.zeros([np.unique(t).shape[0]]*2)\n        for i in np.unique(t):\n            for j in np.unique(t):\n                mat[i,j]=np.sum((out==i) &amp; (t==j))\n        return mat,np.sum(mat*np.eye(mat.shape[0]))/np.sum(mat)\n</code></pre> <p>Let's save this model in our utils.py file.</p>","tags":["neural networks","momentum"]},{"location":"neural_networks/multiayer-perceptron/improvements-to-mlp/#42-a-classification-problem","title":"4.2 A Classification Problem","text":"<p>There are multiple ways to approach a classification problem. The inputs are the features normalized and there are a couple of choices for outputs.</p> <ol> <li>The first is to use a single linear node and then put some threshold for different outputs. For example, for a 4 class problem we can use:</li> </ol> <p>$$ C = \\begin{cases} C_1 &amp; \\text{if }  y \\leq -0.5 \\ C_2 &amp; \\text{if } -0.5 &lt; y \\leq 0 \\ C_3 &amp; \\text{if } 0 &lt; y \\leq 0.5 \\ C_4 &amp; \\text{if } y &gt; 0.5 \\end{cases} $$ But this gets impractical when the number of classes gets large and the boundaries are artificial; say what about the output 0.5? We classify it as \\(C_3\\), but the network gives us no information that how hard this example was to classify.</p> <ol> <li>A more suitable way is to use output is called \\(1\\text{-of-}N\\) encoding. A separate node is used to represent each posiible class and the target vector consists of zeros everywhere except for the class we are representing, e.g (0,0,1,0,0) means the third class out of 5. We are using binary output for each output node.</li> </ol> <p>We can use one of two ways to select a class. After all the output neurons have given their sigmoidal outputs, we can now choose the neuron to be 1 which has highest value and all others are Zero. This makes the result unambigious, since it is highly unlikely that two neurons have same values and they both happen to be the maximum values. This is called Hard Max method. We can also use the Soft Max version, where we first scale the the outputs to be comparable to each other and the sum also adds up to 1. It gives us the probability of that class(more or less!). So if there is a clear winner that neuron will be close to 1, otherwise if there are other similar values they will each have a value of \\(\\frac{1}{p}\\), where \\(p\\) is the number of output neurons that have similar values.</p> <p>There is another problem with the classification process. It is called class imablance. If we have, say, two class and we have 90% of training data for class 1, then the classifier will learn to always predict class 1, irrespective of the data. It will surely give it a 90% accuracy, but it is  a bad classifier. All the training examples from each classes should almost same. We may randomly throw off some data from the class which has larger data. There is another method called as novelty detection, where we train the whole model on just the over-represented data and then just predict if the data is different or similar to the training data. We will cover it in detail later.  </p> <p>The Iris Dataset: This data is concerned with classification of iris flower into different species. This data is already available in sklearn library. This data contains features of 'sepal length', 'sepal width', 'petal length', 'petal width'. There are three targets, the species, viz, 'setosa', 'versicolor', 'virginica'. These are encoded as 0,1,2 respectively.</p> <pre><code>from sklearn.datasets import load_iris\n</code></pre> <pre><code>data = load_iris()['data']\ntarget = load_iris()['target']\n</code></pre> <p>Upon inspection, you will find that this data has equal number of datapoints from each class, so we don't have to discard any datapoints. Let's first split the data into train, test and validation. Let's keep half of the data for training, a quarter for validation and a quarter for testing, but first we will randomise the dataset. Also we need to convert the target into 1-of-N encoding.</p> <pre><code>encoded_target = np.zeros((target.shape[0],3))\nencoded_target[np.arange(target.shape[0]),target]=1\n</code></pre> <pre><code>idx = np.arange(data.shape[0])\nnp.random.shuffle(idx)\ndata = data[idx,:]\nencoded_target = encoded_target[idx,:]\n\ntrain = data[::2,:]\ntraint = encoded_target[::2,:]\n\nval = data[1::4,:]\nvalt = encoded_target[1::4,:]\n\ntest = data[3::4]\ntestt = encoded_target[3::4,:]\n</code></pre> <p>Let's normalize the data. Let's divide by the maximum instead of standard deviation. We will normalize every set with mean and max(or std) from the training data. </p> <pre><code>mean = train.mean(axis=0)\nmaximum = train.max(axis=0)\nstd = train.std(axis=0)\ntrain = (train-mean)/maximum\nval = (val-mean)/maximum\ntest = (test-mean)/maximum\n</code></pre> <pre><code>m = MLP()\nm.early_stopping(train,traint,val,valt,10,(20,),15,False,0.9,loss='crossentropy',random_state=0)\n</code></pre> <pre><code>0.0798786726307614\n</code></pre> <pre><code>m.compute_error(m.forward(train,False),traint,'crossentropy')\n</code></pre> <pre><code>0.18322617355694956\n</code></pre> <pre><code>m.confmat(train,traint)\n</code></pre> <pre><code>(array([[28.,  0.,  0.],\n        [ 0., 23.,  1.],\n        [ 0.,  2., 21.]]), 0.96)\n</code></pre> <pre><code>m.confmat(val,valt)\n</code></pre> <pre><code>(array([[14.,  0.,  0.],\n        [ 0., 11.,  0.],\n        [ 0.,  0., 13.]]), 1.0)\n</code></pre> <pre><code>m.confmat(test,testt)\n</code></pre> <pre><code>(array([[ 8.,  0.,  0.],\n        [ 0., 13.,  0.],\n        [ 0.,  1., 15.]]), 0.972972972972973)\n</code></pre> <p>The confusion matrices reveal great results, on test set as well. You may even choose to change the activation of output nodes. (try softmax!)</p>","tags":["neural networks","momentum"]},{"location":"neural_networks/multiayer-perceptron/improvements-to-mlp/#43-time-series-problem","title":"4.3 Time Series Problem","text":"<p>Time Series data has a few problems:</p> <ol> <li> <p>Even if there is some regularities in the data, it can appear many different scales. There are local variations, which hide the general trend.</p> </li> <li> <p>How many datapoints of the past should we use to make a prediction? and at what intervals?</p> </li> </ol> <p>This comes to a choice of \\(\\tau\\) and \\(k\\) deciding the interval and the number respectively.</p> <p>For example, if \\(\\tau=2\\) and \\(k=3\\), then input data are elements 1,2,and 5 and 7 is the target. The next element is 2,4,6,and target is 8. Just make sure you don't use parameters such that data is picked systematically. e.g: If some feature is only visible at odd datapoints, then it will be completely missed in even ones.</p> <p>After done it is a simple regression problem.</p>","tags":["neural networks","momentum"]},{"location":"neural_networks/multiayer-perceptron/improvements-to-mlp/#364-the-auto-associative-network","title":"3.6.4 The Auto-Associative Network","text":"<p>This is an interesting use of neural networks. In this network the input and the targets are the same. It is also called auto-encoder at times. It looks useless at first, but if we have a small hidden layer(lesser nodes than input nodes), it forms a bottleneck condition, where the outer nodes (input and output) are equal to each other and more than the hidden ones. Once the network is trained to reproduce the the own result, the hidden outputs(activations) will represent a compressed version of the data. It reduces the dimensions. We can use it to compress the data and then use the second weights to regenerate the input again.</p> <p>It can also be used to denoise images. Once a model is trained, it can be used to pass noisy images to return clearer images.</p> <p>The hidden acivations, if they all have linear activations, will be the Principal Components of input data. Principal Component Analysis is a useful dimensionality reduction technique and will be discussed in detail later.</p>","tags":["neural networks","momentum"]},{"location":"neural_networks/multiayer-perceptron/multi-layer-perceptron/","title":"The Multi Layer Perceptron","text":"","tags":["neural","networks","perceptron","deep","learning","feedforward","networks","backpropagation","momentum","multiple","layers"]},{"location":"neural_networks/multiayer-perceptron/multi-layer-perceptron/#1-introduction","title":"1. Introduction","text":"<p>As we have seen, in the Basic perceptron, that a perceptron can only classify the Linearly Separable Data. We had two different approaches to get around this problem:</p> <ol> <li>The Higher Dimensions, which was discussed briefly and will be discussed in detail later.</li> <li>The Multiple Layers, which we will discuss now.</li> </ol> <p>So the concept here is to add an intermediate layer of neurons between the input nodes and the output layer of neurons(as you might have noticed, we are starting to use the term layer quite a lot. You can make a drinking game out of it!). This additional layer will have input from the input nodes, and will have it's own weights(and biases!). Then the outputs generated by this layer will be input for the next layer and so on till the final output layer. So this network can learn more complex functions than just linear ones.</p> <p>Now the question is how we train this network?</p>","tags":["neural","networks","perceptron","deep","learning","feedforward","networks","backpropagation","momentum","multiple","layers"]},{"location":"neural_networks/multiayer-perceptron/multi-layer-perceptron/#2-training-the-mlp","title":"2. Training the MLP","text":"","tags":["neural","networks","perceptron","deep","learning","feedforward","networks","backpropagation","momentum","multiple","layers"]},{"location":"neural_networks/multiayer-perceptron/multi-layer-perceptron/#21-introduction","title":"2.1 Introduction","text":"<p>It is the same as with the simple perceptron. We predict the outputs on a given data. We change the weights for wrong answers, until all the outputs are correct(or until epochs run out!).</p> <p>The prediction phase is quite simple. We compute the outputs of intermediate layer and use that as input for the final output layer.</p> <p>But the updation of weights is what makes it a bit tricky. If an error has occured, we don't know which weights to change. The error might be in the weights of the first layer or in the final output layer. Since the updation of weights is more complex(or lengthy!) we divide the training into two parts: 1. Forward Process, where we compute the outputs for the data to spot errors. It is also used in the Prediction Phase. 2. Backward Process, or updation of weights. We will know why it is called \"Backward\".</p>","tags":["neural","networks","perceptron","deep","learning","feedforward","networks","backpropagation","momentum","multiple","layers"]},{"location":"neural_networks/multiayer-perceptron/multi-layer-perceptron/#22-forward-process","title":"2.2 Forward Process","text":"<p>This process is pretty straight forward, the intermediate layer has its own weights which connect the input nodes to its neurons. The intermediate layer can have any number of neurons. We will just calculate if each neuron will fire or not in that layer. That vector will be the output of that layer and the input for next layer which will finally generate the final outputs.</p> <p>It is note here that it is not necessary to have just have one intermediate layer but as many layers as we want and they will work the same as passing on their outputs as inputs for next layer until the final output is generated.</p> <p>The intermediate layers are called hidden layers. As we had seen in the simple Perceptron that an additional input bias node was required. So while using the outputs of a hidden layer as inputs for a next layer we again add a bias node of constant value(-1,say) to the layer but while computing the outputs of any hidden layer from it's previous layer, we ignore the bias unit, as it is always constant.</p> <p></p> <p>Figure 1</p> <p>As you can see in the above network, a bias node is from input layer to hidden layer and another bias node from the hidden layer to output layer. There is no weight from the previous layer to the bias node of any layer as it is not computed but is constant.</p> <p>Let's say we have \\(L\\) number of layers(not including the input layer but output layer is included). So \\(L-1\\) layers are hidden layers. The input layer will have nodes as many as there are features in the data and the output layer will have as many neurons as the number of classes(or 1 if there are two classes or if we are using the network for regression). The rest of the network i.e all the hidden layers can have as many neurons as we want.</p> <p>Let: - \\(n_i\\): the number of neurons in \\(i^{th}\\) layer. - \\(a^{(i)}_j\\): the output generated by \\(j^{th}\\) neuron in \\(i^{th}\\) layer. - \\(w^{(i)}_{jk}\\): weight connecting the \\(j^{th}\\) neuron of \\((i-1)^{th}\\) layer to \\(k^{th}\\) neuron of \\(i^{th}\\) layer. - \\(g_{(i)}\\) is the activation functionof \\(i^{th}\\) layer.</p> <p>Note: We can also use the term \\(0^{th}\\) layer for the input layer. So \\(n_0\\) is the number of input features. \\(a^0_j=x_j\\) is the \\(j^{th}\\) input node. \\(a^0_0=x_0\\) is the bias node of input layer.</p> <p>We compute the activation of \\(j^{th}\\) neuron in \\(i^{th}\\) layer as:</p> \\[a^{(i)}_j = g_{(i)}\\bigg(\\sum_{k=0}^{n_{i-1}} a^{(i-1)}_kw^{(i)}_{kj}\\bigg)  \\qquad \\forall \\ j \\in [1,n_i] \\tag{1}\\] <p>where \\(g_{(i)}\\) is the activation of \\(i^{th}\\) layer.</p> <p>Note: We did not compute for \\(j=0\\) i.e we did not compute \\(a^{(i)}_0\\) as that will be the bias node and has constant value.</p> <p>To make things a bit more simpler, we introduce another term, \\(h^{(i)}_j\\), where:</p> \\[ h^{(i)}_j = \\sum_{k=0}^{n_{i-1}} a^{(i-1)}_kw^{(i)}_{kj} \\tag{2} \\] <p>So Equation 1 can be changed to:</p> \\[ a^{(i)}_j = g_{(i)}\\big(h^{(i)}_j\\big) \\tag{3} \\] <p>To get the final outputs, Repeat Equation 1 \\(\\forall i \\in [1,L]\\) sequentially.</p> <p>We will see it in action later in implementation.</p>","tags":["neural","networks","perceptron","deep","learning","feedforward","networks","backpropagation","momentum","multiple","layers"]},{"location":"neural_networks/multiayer-perceptron/multi-layer-perceptron/#23-backward-process-updation-of-weights","title":"2.3 Backward Process (Updation of weights)","text":"","tags":["neural","networks","perceptron","deep","learning","feedforward","networks","backpropagation","momentum","multiple","layers"]},{"location":"neural_networks/multiayer-perceptron/multi-layer-perceptron/#231-introduction","title":"2.3.1 Introduction","text":"<p>Now that we have generated outputs, it is time to check them against the ground truth targets and make changes in weights to correct the errors made. But the problem is we don't know which weights made the error, especially in which layer was the error.</p> <p>Now even if we follow the basic rule of updating weights as in simple perceptron, we will compute error in each output neuron and update the weights using the learning rate and the inputs. But how to update weights of the layer before? We don't know the ground truth values for hidden layers. How do we compute error in these layers?</p>","tags":["neural","networks","perceptron","deep","learning","feedforward","networks","backpropagation","momentum","multiple","layers"]},{"location":"neural_networks/multiayer-perceptron/multi-layer-perceptron/#232-the-backpropagation-algorithm","title":"2.3.2 The Backpropagation Algorithm","text":"","tags":["neural","networks","perceptron","deep","learning","feedforward","networks","backpropagation","momentum","multiple","layers"]},{"location":"neural_networks/multiayer-perceptron/multi-layer-perceptron/#2321-introduction","title":"2.3.2.1 Introduction","text":"<p>The Backpropagation Algorithm is the core of Multi Layer Neural Networks. It works on the principle of gradient descent and minimizing error.</p> <p>If you recall from the basic perceptron, we updated weights as:</p> \\[ w_{ij} \\leftarrow w_{ij} - \\eta (y_j-t_j)x_i \\] <p>Here we are doing the same but for each layer, but instead of using \\((y_j-t_j)\\) we will let the calculus decide the error in that hidden layer, as we don't know the ground truth targets for hidden layers(and that's why they are called so!) and \\(x_i\\) was the input node for that weight, where as in layer \\(l\\), the input comes from the layer before (\\(a^{(l-1)}_j\\)).</p> <p>So we can write for each layer,\\(l\\), we will update weights as:</p> \\[ w^{(l)}_{jk} \\leftarrow w^{(l)}_{jk} - \\eta \\delta^{(l)}_k a^{(l-1)}_j \\tag{4} \\] <p>where \\(\\delta^{(l)}_k\\) is the error in \\(k^{th}\\) neuron of \\(l^{th}\\) layer.</p> <p>Please make sure it is the error in a neuron and not the total error in the network, which we will define now.</p>","tags":["neural","networks","perceptron","deep","learning","feedforward","networks","backpropagation","momentum","multiple","layers"]},{"location":"neural_networks/multiayer-perceptron/multi-layer-perceptron/#2322-the-network-error","title":"2.3.2.2 The Network Error","text":"<p>Now to get around the problem of updating weights in hidden layers, we decide to formalize the error in the network and try to minimize it.</p> <p>As in Simple Perceptron, we had used the simple difference as our error function for each output neuron which was fine but now we would like to minimize the error of the network. There are multiple ways of doing it. We will see an example for that. But, for now let's say the total error in the network is \\(E\\).</p> <p>The error of the network has to be a function of final outputs and ground truth targets, that is how we will compute error.</p> <p>Actually we will have a loss function for each output neuron, which will be summed. Let the loss in \\(i^{th}\\) neuron be a function \\(\\mathcal{L}(y_i,t_i)\\).</p> <p>The total Error for an example will be the sum of loss functions in each final neuron. </p> <p>So,</p> \\[ E = \\sum_{i=1}^{n_{L}} \\mathcal{L}(y_i,t_i) \\tag{5} \\] <p>where \\(y_i = a^{(L)}_i\\) is the output of \\(i^{th}\\) output neuron in the final layer and \\(t_i\\) is the corresponding true value.</p> <p>You can even go ahead and sum this error for every training example and call that the error of the network and try to minimize that but we will define this as our network error.</p>","tags":["neural","networks","perceptron","deep","learning","feedforward","networks","backpropagation","momentum","multiple","layers"]},{"location":"neural_networks/multiayer-perceptron/multi-layer-perceptron/#2323-computing-gradients","title":"2.3.2.3 Computing Gradients","text":"<p>Now to minimize this error, we will use calculus, specifically differential calculus. We can compute the gradient of the error along each weight dimension in the network and try to minimize by changing weights along the minimizing gradient.</p> <p>So the gradient of the Error(\\(E\\)) w.r.t a weight \\(w^{(l)}_{jk}\\) in layer \\(l\\) is:</p> \\[ \\frac{\u2202E}{\u2202w^{(l)}_{jk}} \\] <p>The negative of this gradient is the direction along which the Error is minimum along this weight dimension. Now we know what direction to change the weights in and for the amount we need to change we will use the learning rate.</p> <p>So weight update can also be defined for any weight from neuron \\(p\\) of layer \\((l-1)\\) to neuron \\(q\\) of layer \\(l\\):</p> \\[ w^{(l)}_{pq} \\leftarrow w^{(l)}_{pq} - \\eta \\frac{\u2202E}{\u2202w^{(l)}_{pq}} \\tag{6} \\] <p>Comparing Equation 4 and 6,</p> \\[ \\frac{\u2202E}{\u2202w^{(l)}_{pq}} = \\delta^{(l)}_q a^{(l-1)}_p \\tag{7} \\] <p>Now to compute the gradient of weights, we will use the chain rule:</p> \\[ \\frac{\u2202E}{\u2202w^{(l)}_{pq}} = \\frac{\u2202E}{\u2202h^{(l)}_{q}}\\frac{\u2202h^{(l)}_{q}}{\u2202w^{(l)}_{pq}} \\tag{8} \\] <p>Recalling Equation 2:</p> \\[ \\begin{align} h^{(l)}_q &amp;= \\sum_{k=0}^{n_{l-1}} a^{(l-1)}_kw^{(i)}_{kq}\\\\ \\implies \\frac{\u2202h^{(l)}_{q}} {\u2202w^{(l)}_{pq}}&amp;= a^{(l-1)}_p \\end{align} \\tag{9} \\] <p>The above equation holds by the fact that it is a partial derivative and all other weights and outputs are constant and their differentiation is zero except the weight we are differentiating with.</p> <p>Using Equation 9 in 8:</p> \\[ \\frac{\u2202E}{\u2202w^{(l)}_{pq}} =  \\frac{\u2202E}{\u2202h^{(l)}_{q}} a^{(l-1)}_p \\tag{10} \\] <p>Comparing Equation 10 and 7:</p> \\[ \\delta^{(l)}_q =\\frac{\u2202E}{\u2202h^{(l)}_{q}} \\tag{11} \\] <p>The above equation is one of the most important equations in Backpropagation algorithm. It defines the error in \\(q^{th}\\) neuron of \\(l^{th}\\) layer. </p>","tags":["neural","networks","perceptron","deep","learning","feedforward","networks","backpropagation","momentum","multiple","layers"]},{"location":"neural_networks/multiayer-perceptron/multi-layer-perceptron/#2324-computing-gradients-final-layer","title":"2.3.2.4 Computing Gradients (Final Layer)","text":"<p>Let's try to compute the weight updates for weights in final output layer.</p> <p>Using Equation 4 for the final layer:</p> \\[ w^{(L)}_{pq} \\leftarrow w^{(L)}_{pq} - \\eta \\delta^{(L)}_qa^{(L-1)}_p \\tag{12}\\] <p>Now let's compute \\(\\delta^{(L)}_q\\)(i.e error in \\(q^{th}\\) neuron of final layer):</p> <p>From Equation 11:</p> \\[ \\delta^{(L)}_q=\\frac{\u2202E}{\u2202h^{(L)}_{q}} \\tag{13} \\] <p>As we know the error is the sum of loss functions on each output neuron:</p> \\[ E = \\sum_{i=1}^{n_{L}} \\mathcal{L}(y_i,t_i) \\tag{14} \\] <p>where \\(y_i = a^{(L)}_i\\), the output of \\(i^{th}\\) neuron in the final layer.</p> <p>Using Equation 3:</p> <p>$$ E = \\sum_{i=1}^{n_{L}} \\mathcal{L}\\big(g_{(L)}(h^{(L)}_i),t_i\\big) \\tag{15} $$ where \\(g_{(L)}\\) is the activation of final layer.</p> <p>Coming back to Equation 13:</p> \\[ \\begin{align} \\delta^{(L)}_q&amp;= \\frac{\u2202}{\u2202h^{(L)}_{q}} \\bigg( \\sum_{i=1}^{n_{L}}\\mathcal{L}\\big(g_{(L)} (h^{(L)}_i),t_i\\big) \\bigg)\\\\ &amp;= \\sum_{i=1}^{n_{L}} \\frac{\u2202}{\u2202h^{(L)}_{q}} \\bigg( \\mathcal{L}\\big(g_{(L)} (h^{(L)}_i),t_i\\big) \\bigg) \\tag{16} \\end{align} \\] <p>Since it is a partial derivative, all other functions except for those of \\(h^{(L)}_q\\) are considered constant and hence differentiation is zero. i.e:</p> \\[ \\frac{\u2202}{\u2202h^{(L)}_{q}} \\bigg( \\mathcal{L}\\big(g_{(L)}(h^{(L)}_i),t_i\\big) \\bigg) = \\begin{cases} g_{(L)}'(h^{(L)}_i)\\ \\mathcal{L}' \\big(g_{(L)}(h^{(L)}_i),t_i\\big) &amp; \\text{if } i = q \\\\ 0 &amp; \\text{if } i \\neq q \\end{cases} \\tag{17} \\] \\[ \\begin{align} \\implies \\delta^{(L)}_q&amp;=g_{(L)}'\\big(h^{(L)}_q\\big)\\ \\mathcal{L}'\\big(g_{(L)}(h^{(L)}_q),t_q\\big) \\\\ &amp;=g_{(L)}'\\big(h^{(L)}_q\\big)\\ \\mathcal{L}'\\big(y_q,t_q\\big) \\tag{18} \\end{align} \\] <p>Equation 18 above calculates the error in neuron number \\(q\\) in layer \\(L\\)(the final layer).</p> <p>Using the above equation in Equation 13,</p> \\[ w^{(L)}_{pq} \\leftarrow w^{(L)}_{pq} -  \\eta \\Big[g_{(L)}'\\big(h^{(L)}_q\\big)\\ \\mathcal{L}'\\big(y_q,t_q \\big)\\Big] a^{(L-1)}_p \\tag{19}\\] <p>This looks very confusing at first, but have a look at the indices and you'll get the hang of it. Besides we will have an example for a specific loss function later.</p> <p>Also you might have noticed that we perform differentiation of the activation function, so our current activation function (the threshold function) is no good here as that is non-differentiable at 0. We will use some new activation functions later.</p>","tags":["neural","networks","perceptron","deep","learning","feedforward","networks","backpropagation","momentum","multiple","layers"]},{"location":"neural_networks/multiayer-perceptron/multi-layer-perceptron/#2325-computing-gradients-final-hidden-layer","title":"2.3.2.5 Computing Gradients (Final Hidden Layer)","text":"<p>Now, that we have computed error in the final layer and updated weights, let's try updating weights in the final hidden layer (i.e 2nd last layer, \\(L-1\\)). We will proceed the same as we did with the final output layer before.</p> <p>Again Using Equation 4 for the final hidden layer:</p> \\[ w^{(L-1)}_{pq} \\leftarrow w^{(L-1)}_{pq} - \\eta \\delta^{(L-1)}_qa^{(L-2)}_p \\tag{20}\\] <p>Now let's compute \\(\\delta^{(L-1)}_q\\)(i.e error in \\(q^{th}\\) neuron of final hidden layer):</p> <p>From Equation 11:</p> \\[ \\delta^{(L-1)}_q=\\frac{\u2202E}{\u2202h^{(L-1)}_{q}} \\tag{21} \\] <p>Now this is where the chain rule comes to propagate the error,</p> \\[ \\begin{align} \\delta^{(L-1)}_q&amp;=\\frac{\u2202E}{\u2202h^{(L-1)}_{q}}\\\\ &amp;=\\frac{\u2202E}{\u2202h^{(L)}_{i}}\\frac{\u2202(h^{(L)}_{i})}{\u2202h^{(L-1)}_{q}}\\\\ &amp;\\ \\forall\\  i \\in [1,n_L] \\tag{22} \\end{align} \\] \\[ \\delta^{(L-1)}_q=\\sum_{i=1}^{n_L}\\frac{\u2202E}{\u2202h^{(L)}_{i}}\\frac{\u2202(h^{(L)}_{i})}{\u2202h^{(L-1)}_{q}} \\tag{23} \\] <p>Using Equation 13,</p> \\[ \\delta^{(L-1)}_q=\\sum_{i=1}^{n_L}\\delta^{(L)}_i\\frac{\u2202(h^{(L)}_{i})}{\u2202h^{(L-1)}_{q}} \\tag{24} \\] <p>Now let's compute \\(\\frac{\u2202(h^{(L)}_{i})}{\u2202h^{(L-1)}_{q}}\\), using Equation 2:</p> \\[ \\frac{\u2202(h^{(L)}_{i})}{\u2202h^{(L-1)}_{q}} = \\frac{\u2202}{\u2202h^{(L-1)}_{q}} \\Big( \\sum_{k=0}^{n_{L-1}} a^{(L- 1)}_kw^{(L)}_{ki} \\Big) \\tag{25} \\] <p>Using Equation 3,</p> \\[ \\begin{align} \\frac{\u2202(h^{(L)}_{i})}{\u2202h^{(L-1)}_{q}} &amp;= \\frac{\u2202}{\u2202h^{(L-1)}_{q}} \\Big(\\sum_{k=0}^{n_{L-1}} g_{(L- 1)}\\big(h^{(L-1)}_k\\big)w^{(L)}_{ki} \\Big)\\\\ &amp;=  \\sum_{k=0}^{n_{L-1}} w^{(L)}_{ki} \\frac{\u2202}{\u2202h^{(L-1)}_{q}} \\Big(  g_{(L-1)}\\big(h^{(L-1)}_k\\big) \\Big) \\tag{26} \\end{align} \\] <p>Again it is a partial derivative, so just the functions of \\(h^{(L-1)}_q\\) are considered as variables, all others are constant and differentiate to Zero.</p> \\[ \\frac{\u2202}{\u2202h^{(L-1)}_{q}} \\Big( g_{(L-1)}\\big(h^{(L-1)}_k\\big) \\Big) = \\begin{cases} g_{(L-1)}'\\big(h^{(L- 1)}_k\\big) &amp; \\text{if } k = q \\\\ 0 &amp; \\text{if } k \\neq 0 \\end{cases} \\tag{27} \\] \\[\\implies \\frac{\u2202(h^{(L)}_{i})}{\u2202h^{(L-1)}_{q}} =  w^{(L)}_{qi} \\  g_{(L-1)}'\\big(h^{(L-1)}_q\\big)\\tag{28} \\] <p>Since the Equations 25 - 28 could have worked for any layer \\(l\\) and not just layer \\(L-1\\), we can also generalize the above result, for any layer \\(l\\), as:</p> \\[ \\frac{\u2202(h^{(l)}_{i})}{\u2202h^{(l-1)}_{q}} = w^{(l)}_{qi} \\  g_{(l-1)}'\\big(h^{(l-1)}_q\\big)  \\tag{29} \\] <p>The above equation is also important as we will see, in a moment.</p> <p>Using equation 29 in Equation 24, we get:</p> \\[\\begin{align} \\delta^{(L-1)}_q&amp;=\\sum_{i=1}^{n_L}\\delta^{(L)}_iw^{(L)}_{qi} g_{(L-1)}'\\big(h^{(L-1)}_q\\big)\\\\ &amp;= g_{(L-1)}'\\big(h^{(L-1)}_q\\big) \\sum_{i=1}^{n_L}\\delta^{(L)}_iw^{(L)}_{qi} \\tag{30} \\end{align} \\] <p>Now, coming back to Equation 20:</p> \\[ w^{(L-1)}_{pq} \\leftarrow w^{(L-1)}_{pq} - \\eta \\delta^{(L-1)}_qa^{(L-2)}_p \\tag{20} \\] <p>We can now update the weights of the final hidden layer as the error from the final layer has propagated to the final hidden layer.</p>","tags":["neural","networks","perceptron","deep","learning","feedforward","networks","backpropagation","momentum","multiple","layers"]},{"location":"neural_networks/multiayer-perceptron/multi-layer-perceptron/#2326-summarizing-the-important-points","title":"2.3.2.6 Summarizing the important points.","text":"<p>Let's list all the important results that we derived uptill this point to avoid confusion.</p> <ol> <li>We update the weights of any layer \\(l\\) as Equation 4:</li> </ol> \\[ w^{(l)}_{jk} \\leftarrow w^{(l)}_{jk} - \\eta \\delta^{(l)}_k a^{(l-1)}_j \\tag{4} \\] <p>where:</p> <ul> <li>\\(\\eta\\) is the learning rate.</li> <li>\\(a^{(l-1)}_j\\) is the output generated by \\(j^{th}\\) neuron in the previous layer</li> <li> <p>\\(\\delta^{(l)}_k\\) is the error generated in the \\(k^{th}\\) neuron of \\(l^{th}\\) layer explained below.</p> </li> <li> <p>The output of \\(j^{th}\\) neuron in the \\(i^{th}\\) layer is given by Equation 3:</p> </li> </ul> \\[ a^{(i)}_j = g_{(i)}\\big(h^{(i)}_j\\big) \\tag{3} \\] <p>where:     * \\(g_{(i)}\\) is the activation function of layer \\(i\\).     * \\(h^{(i)}_j\\) is the summation output of the \\(j^{th}\\) neuron of \\(i^{th}\\) layer explained below.</p> <ol> <li>The summation output has been assigned a new term as it makes calculations easier and leads to definition of the error in a neuron. The summation output in \\(j^{th}\\) neuron of \\(i^{th}\\) layer is as Equation 2:</li> </ol> \\[ h^{(i)}_j = \\sum_{k=0}^{n_{i-1}} a^{(i-1)}_kw^{(i)}_{kj} \\tag{2} \\] <p>where: * \\(a^{(i-1)}_k\\) is the output from the \\(k^{th}\\) neuron of the previous layer \\((i-1)^{th}\\) layer. *  \\(w^{(i)}_{kj}\\) is the weight joining the \\(k^{th}\\) neuron of previous (\\((i-1)^{th}\\)) layer to the \\(j^{th}\\) neuron of current (\\(i^{th}\\)) layer.</p> <ol> <li>The error in \\(q^{th}\\) neuron in \\(l^{th}\\) layer is defined by Equation 11:</li> </ol> \\[ \\delta^{(l)}_q =\\frac{\u2202E}{\u2202h^{(l)}_{q}} \\tag{11} \\] <p>where: * \\(E\\) is the total error of the network. * \\(h^{(l)}_q\\) is the summation output of \\(q^{th}\\) neuron in \\(l^{th}\\) layer.</p> <ol> <li>The derivative of summation output of \\(i^{th}\\) neuron in the next (\\(l^{th}\\)) layer w.r.t \\(q^{th}\\) neuron in current (\\((l-1)^{th}\\)) layer given in Equation 29:</li> </ol> \\[ \\frac{\u2202(h^{(l)}_{i})}{\u2202h^{(l-1)}_{q}} =  w^{(l)}_{qi} \\cdot g_{(l-1)}'\\big(h^{(l-1)}_q\\big) \\tag{29} \\] <p>where: * \\(g_{(l-1)}'\\big(h^{(l-1)}_q\\big)\\) is the derivative of the activation function of layer \\((l-1)\\) at \\(h^{(l-1)}_q\\), the summation output.</p>","tags":["neural","networks","perceptron","deep","learning","feedforward","networks","backpropagation","momentum","multiple","layers"]},{"location":"neural_networks/multiayer-perceptron/multi-layer-perceptron/#2327-computing-gradients-any-hidden-layer","title":"2.3.2.7 Computing Gradients (Any hidden Layer)","text":"<p>Now, let's try to develop a generalized equation for the error in layer \\(l\\).</p> <p>We update the weights of any layer \\(l\\) as Equation 4:</p> \\[ w^{(l)}_{jk} \\leftarrow w^{(l)}_{jk} - \\eta \\delta^{(l)}_k a^{(l-1)}_j \\tag{4} \\] <p>where: - \\(\\eta\\) is the learning rate. -  \\(a^{(l-1)}_j\\) is the output generated by \\(j^{th}\\) neuron in the previous layer -  \\(\\delta^{(l)}_k\\) is the error generated in the \\(k^{th}\\) neuron of \\(l^{th}\\) layer explained below.  </p> <p>Now we just need to compute the error \\(\\delta^{(l)}_k\\):</p> <p>By Equation 11:</p> \\[ \\delta^{(l)}_k =\\frac{\u2202E}{\u2202h^{(l)}_{k}} \\tag{31} \\] <p>where:</p> <ul> <li>\\(E\\) is the total error of the network.</li> <li>\\(h^{(l)}_q\\) is the summation output of \\(q^{th}\\) neuron in \\(l^{th}\\) layer.</li> </ul> \\[ \\delta^{(l)}_k =\\frac{\u2202E}{\u2202h^{(l+1)}_{i}}\\cdot\\frac{\u2202h^{(l+1)}_{i}}{\u2202h^{(l)}_{k}} \\qquad \\forall \\ i \\ \\in  [1,n_{l+1}] \\tag{32} \\] <p>doing it for all \\(i\\):</p> \\[\\implies \\delta^{(l)}_k = \\sum_{i=1}^{n_{l+1}}\\Bigg[\\frac{\u2202E}{\u2202h^{(l+1)}_{i}}\\cdot\\frac{\u2202h^{(l+1)}_{i}} {\u2202h^{(l)}_{k}}\\Bigg] \\tag{33} \\] <p>Using Equation 11,</p> \\[\\implies \\delta^{(l)}_k = \\sum_{i=1}^{n_{l+1}}\\Bigg[ \\delta^{(l+1)}_i \\cdot \\frac{\u2202h^{(l+1)}_{k}}{\u2202h^{(l)}_{k}} \\Bigg] \\tag{34} \\] <p>Now according to Equation 29:</p> \\[ \\frac{\u2202(h^{(l)}_{i})}{\u2202h^{(l-1)}_{q}} = w^{(l)}_{qi} \\cdot g_{(l-1)}'\\big(h^{(l-1)}_q\\big) \\tag{35} \\] <p>Using above equation in Equation 34:</p> \\[ \\begin{align}\\implies \\delta^{(l)}_k &amp;= \\sum_{i=1}^{n_{l+1}}\\Big[ \\delta^{(l+1)}_i \\cdot w^{(l+1)}_{ki} \\cdot g_{(l)}'\\big(h^{(l)}_k\\big)\\Big]\\\\ &amp;= g_{(l)}'\\big(h^{(l)}_k\\big) \\sum_{i=1}^{n_{l+1}}\\Big[ \\delta^{(l+1)}_i w^{(l+1)}_{ki} \\Big] \\tag{36} \\end{align} \\] <p>Equation 36 above is the general way of how we backpropagate the error layer to layer. This is at the core of Backpropagation Algorithm which works at the heart of neural networks, not just MLP. </p> <p>Note: It is advised to compute the error in each neuron of previous layer first and then update the weights of current layer as they are used to calculate the error of previous layer. </p>","tags":["neural","networks","perceptron","deep","learning","feedforward","networks","backpropagation","momentum","multiple","layers"]},{"location":"neural_networks/multiayer-perceptron/multi-layer-perceptron/#2328-the-activations","title":"2.3.2.8 The Activations","text":"<p>As you can see in the Equation 36, that the activations we use should be differentiable at all points and the threshold activation was not differentiable at 0 and had derivative of 0 at all other points, making the error in every neuron to be zero, which isn't what we want. So we introduce a couple more activation functions. We will visualize each function and how they vary.</p> <pre><code>from plotly import graph_objects as go\nimport numpy as np\n\ndef plot_activation(func):\n    x = np.linspace(-100,100,400)\n    #compute the activation\n    y = func(x)\n    fig = go.Figure(data=[go.Scatter(x=x,y=y,mode=\"lines\")],\n                    layout=dict(height=500,width=500,title=f\"{func.__name__} activation\".title()))\n    fig.show()\n</code></pre> <p>Our original threshold function was:</p> \\[ g(x) = \\begin{cases} 1 &amp; \\text{if } x&gt;0\\\\ 0 &amp; \\text{if } x \\leq 0 \\end{cases} \\tag{37} \\] <p>It can be coded as:</p> <pre><code>def threshold(x):\n    return (x&gt;0)*1\n</code></pre> <p><pre><code># plot the threshold \nplot_activation(threshold)\n</code></pre> {% include mlp/plot1.html %}</p> <p>1. The Sigmoidal Activation: It is an S shaped activation. This activation is given by:</p> \\[ a = g(x) = \\frac{1}{1+\\exp(-\\beta x)} \\tag{38} \\] <p>where \\(\\beta\\) is a positive parameter, preferably 1.</p> <p>It's derivative is:</p> \\[ g'(x) = g(x)\\beta(1-g(x)) = \\beta a(1-a) \\tag{39} \\] <pre><code>def sigmoid(x,beta=1):\n    return 1/(1+np.exp(-beta*x))\n</code></pre> <pre><code>plot_activation(sigmoid)\n</code></pre> <p>{% include mlp/plot2.html %}</p> <p>Let's compare the threshold and sigmoid together</p> <pre><code>from plotly.subplots import make_subplots\nfig = make_subplots(rows=1,cols=2, shared_yaxes=True, subplot_titles=[\"Threshold Activation\", \"Sigmoid Activation\"])\nx = np.linspace(-10,10,400)\ny1 = threshold(x)\nfig.add_trace(go.Scatter(x=x,y=y1),row=1,col=1)\ny2 = sigmoid(x)\nfig.add_trace(go.Scatter(x=x,y=y2),row=1,col=2)\nfig.update_layout(showlegend=False)\nfig.show()\n</code></pre> <p>{% include mlp/plot3.html %}</p> <p>As we can see both the functions are almost the same, just that the sigmoid is more smooth at edges, making it differentiable. It can be used as a replacement for threshold. Although it will output values other than 0 and 1 for values close to zero.</p> <p>2. The tanh activation: It is the similar to sigmoid but it outputs range between -1 and 1, instead of 0 and 1. It is not used much in the MLP. It is given by:</p> \\[ a = g(x) = tanh(x) = \\frac{\\exp(x) - \\exp(-x)}{\\exp(x) + \\exp(-x)} \\tag{40} \\] <p>It's derivative is rather simple:</p> \\[ g'(x) = 1-a^2 \\tag{41} \\] <p>It is already defined in numpy as <code>np.tanh</code> function.</p> <pre><code>plot_activation(np.tanh)\n</code></pre> <p>{% include mlp/plot4.html %}</p> <p>Let's compare all the functions in one go.</p> <pre><code>x = np.linspace(-10,10,100)\nfig=go.Figure(data=[\n                    go.Scatter(x=x,y=threshold(x),name=\"Threshold\"),\n                    go.Scatter(x=x,y=sigmoid(x),name=\"Sigmoid\"),\n                    go.Scatter(x=x,y=np.tanh(x),name=\"Tanh\"),\n                   ],\n             layout=dict(width=600,height=600))\nfig.show()\n</code></pre> <p>{% include mlp/plot5.html %}</p> <p>3. The Softmax Activation: This activation is given by:</p> \\[ a_i = g(x_i) = \\frac{\\exp(x_i)}{\\sum_{k=1}^{n_l}\\exp(x_k)} \\tag{42} \\] <p>As all the outputs in output nodes are independent of each other. The sigmoid layer may make more than one output neuron to fire. But sometimes we just want to classify a datapoint in just one class. That is where softmax is good. It sums up all the outputs to 1 and the maximum one is set to 1 and others to 0.</p> <p>Before computing its derivative, we have two cases 1. Derivative w.r.t the summation of the node, i.e:</p> \\[\\frac{\\partial (g(x_i))}{\\partial x_i} \\] <ol> <li>Derivative w.r.t the summation of some other node, i.e:</li> </ol> \\[\\frac{\\partial (g(x_i))}{\\partial x_j} \\] \\[ \\frac{\\partial (g(x_i))}{\\partial x_i} = a_i(1-a_i) \\tag{43} \\] <p>and </p> \\[\\frac{\\partial (g(x_i))}{\\partial x_j} = - a_ia_j \\tag{44} \\] <p>3. The Linear Activation This activation is given by:</p> \\[g(x) = x \\tag{45}\\] <p>This activation is used for regression purposes.</p>","tags":["neural","networks","perceptron","deep","learning","feedforward","networks","backpropagation","momentum","multiple","layers"]},{"location":"neural_networks/multiayer-perceptron/multi-layer-perceptron/#233-the-equations-in-mlp","title":"2.3.3 The Equations in MLP","text":"<p>We saw the equations in the light of general cases. Let's now see them in an example to clear out the doubts if any. The activations will depend on the type of problem we are solving. For Regression problems we won't use any activation in the final layer as we need real values, but we will use sigmoid activation in hidden layers at all times for the concept of firing and non firing of neurons. For Classification we will use sigmoid for final layer as well. We can also use softmax for the final layer, if we just need to predict one class. Let's say we are currently using sigmoid in all cases.</p> <p>We haven't defined any Loss function for the final output neurons to compute the network error. We can use the simple difference between the outputs and ground truth as the Loss function, but for the network error we sum all the losses(errors) in the final output neurons,</p> <p>From Equation 5,</p> \\[ E = \\sum_{i=1}^{n_{L}} \\mathcal{L}(y_i,t_i) \\tag{5} \\] <p>So if we use just the difference, it will be positive for some neurons and negative for some which may sum to zero or a very small number, meaning small network error which isn't the case. So we need to change the sign of all errors to the same before summing up. We can use absolute values but that has differentiation problems. Instead we can use the squares of difference as our loss function.</p> <p>So for neuron \\(i\\) in the final output layer, the loss is</p> \\[ \\mathcal{L}(y_i,t_i) = \\frac{1}{2}(y_i-t_i)^2 \\tag{46} \\] <p>We have added the half at the front to make further calculations easier.</p> <p>It's derivative is:</p> \\[ \\mathcal{L}'\\big(y_i,t_i\\big) = y_i-t_i \\tag{47} \\] <p>Now using  above equations, equation 5 becomes:</p> \\[ E = \\frac{1}{2}\\sum_{i=1}^{n_{L}} (y_i-t_i)^2 \\tag{48} \\] <p>Now let's compute the error in final layer.</p> <p>Recalling Equations 18, </p> \\[\\implies  \\delta^{(L)}_q=g_{(L)}'(h^{(L)}_q)\\ \\mathcal{L}'\\big(y_q,t_q\\big)  \\tag{18}\\] <p>Since our activation is sigmoid and it's derivative is given in Equation 39.</p> \\[\\implies  \\delta^{(L)}_q=\\beta a^{(L)}_q(1-a^{(L)}_q)(y_q-t_q) \\tag{49}\\] <p>Since, \\(a^{(L)}_i = y_i\\)</p> \\[\\implies  \\delta^{(L)}_q=\\beta^{(L)} (y_q-t_q)y_q(1-y_q) \\tag{50}\\] <p>That is how we compute the error in the final layer.</p> <p>Now let's compute in any hidden layer.</p> <p>We know from Equation 36,</p> \\[\\implies \\delta^{(l)}_k = g_{(l)}'\\big(h^{(l)}_k\\big) \\sum_{i=1}^{n_{l+1}}\\Big[ \\delta^{(l+1)}_i w^{(l+1)}_{ki} \\Big] \\tag{36} \\] <p>Now since we are using sigmoid activation,</p> \\[\\implies \\delta^{(l)}_k = \\beta^{(l)} a^{(l)}_k(1-a^{(l)}_k) \\sum_{i=1}^{n_{l+1}}\\Big[ \\delta^{(l+1)}_i w^{(l+1)}_{ki} \\Big] \\tag{51} \\] <p>The equations will change with change in loss function as well as the activation function of each layer.</p> <p>Side Note: Another great Loss function for Classification is:</p> \\[ \\mathcal{L}(y_i,t_i) = -t_i\\log(y_i) \\tag{52} \\]","tags":["neural","networks","perceptron","deep","learning","feedforward","networks","backpropagation","momentum","multiple","layers"]},{"location":"neural_networks/multiayer-perceptron/multi-layer-perceptron/#24-the-multi-layer-perceptron-algorithm","title":"2.4 The Multi-Layer Perceptron Algorithm","text":"","tags":["neural","networks","perceptron","deep","learning","feedforward","networks","backpropagation","momentum","multiple","layers"]},{"location":"neural_networks/multiayer-perceptron/multi-layer-perceptron/#241-the-algorithm","title":"2.4.1 The Algorithm","text":"<p>With the knowledge of Backpropagation Algorithm, we can now update weights. </p> <p>We will consider the sum of squares of as our network error and sigmoid as the activation in every layer.</p> <ol> <li> <p>Initialization:</p> <p>Initialize the weights for every neuron of every layer to small(positive and negative) random values.</p> </li> <li> <p>Training:</p> <ul> <li>============repeat===========:</li> <li> <p>for each input vector:</p> <ul> <li> <p>forward phase:</p> <ul> <li> <p>for each layer \\(l\\) in the network:</p> <ul> <li>compute the activation of each neuron \\(j\\) using:</li> </ul> \\[ \\begin{align} &amp;h^{(l)}_j = \\sum_{k=0}^{n_{l-1}}a^{(l-1)}_k w^{(l)}_{kj}\\\\ &amp;a^{(l)}_j = g_{(l)}\\big(h^{(l)}_j\\big) = \\frac{1}{1+\\exp\\big(-\\beta h^{(l)}_j\\big)} \\end{align} \\] <p>where \\(a^{0}_i = x_i\\), i.e the input node</p> <p>and \\(a^{(l)}_0 = -1\\), the bias node for every layer.</p> </li> </ul> </li> <li> <p>backward phase:</p> </li> <li>compute the error at the output node \\(j\\) using:       $$       \\delta^{(L)}_j = \\beta^{(L)}(y_j-t_j)y_j(1-y_j)       $$</li> <li> <p>for each layer \\(l\\) in the network starting from the final hidden layer:</p> <ol> <li> <p>compute the error in the layer \\(l\\) using</p> \\[\\delta^{(l)}_k = \\beta^{(l)} a^{(l)}_k(1-a^{(l)}_k) \\sum_{i=1}^{n_{l+1}}\\Big( \\delta^{(l+1)}_i w^{(l+1)}_{ki} \\Big) \\] </li> <li> <p>update the every weight in the layer \\((l+1)\\) using:</p> \\[w^{(l+1)}_{pq} \\leftarrow w^{(l+1)}_{pq} - \\eta \\delta^{(l+1)}_q a^{(l)}_p\\\\  \\forall \\ p \\in [0,n_l] \\text{  and  } q \\in [1,n_{l+1}]  \\] </li> </ol> </li> <li> <p>finally update every weight of first hidden layer using:</p> \\[w^{(1)}_{pq} \\leftarrow w^{(1)}_{pq} - \\eta \\delta^{(1)}_q a^{(0)}_p\\\\       \\forall \\ p \\in [0,n_0] \\text{  and  } q \\in [1,n_{1}]   \\] <p>where \\(a^{(0)}_i = x_i\\), the input node.</p> </li> </ul> </li> <li> <p>====until learning stops or epochs run out============</p> </li> </ul> </li> </ol> <p>Note: In batch implementation, we should randomise the input so that we don't train on the same sequences for every iteration. 3. Recall     - Use the forward phase in training section above.              </p>","tags":["neural","networks","perceptron","deep","learning","feedforward","networks","backpropagation","momentum","multiple","layers"]},{"location":"neural_networks/multiayer-perceptron/multi-layer-perceptron/#242-initialization-of-weights","title":"2.4.2 Initialization of weights","text":"<p>The weights should be initialized to be small random numbers. It is because the sigmoid activation flattens at the large(positive and negative) values and the slope becomes closer zero and so it takes longer to train the network. The sigmoid starts flattening at input of 1, so that will be the max input and similarly -1 will be the minimum input.</p> <p>So for a neuron,</p> \\[ \\begin{align} &amp;-1 \\leq h^{(l)}_i \\leq 1\\\\ \\implies &amp;-1 \\leq \\sum_{k=0}^{n_{l-1}}a^{(l-1)}_kw^{(l)}_{ki}\\leq 1\\\\ \\end{align} \\tag{53} \\] <p>Now since we have to figure out all the weights, we will keep all the weights to be same and equal to \\(w^{(l)}\\).</p> \\[ \\implies -1 \\leq w^{(l)}\\sum_{k=0}^{n_{l-1}}a^{(l-1)}_k\\leq 1\\\\ \\tag{54} \\] <p>Note: \\(w^{(l)}\\) is the general representation of every weight of layer \\(l\\) and not the weight matrix of layer \\(l\\).</p> <p>We assume that all the inputs come from a stadard normal distribution zero mean and unit variance, i.e:</p> \\[ \\sigma^2 = \\frac{1}{n_{l-1}}\\sum_{i=0}^{n_{l-1}}\\big(a^{(l-1)}_i - \\mu^{(l-1)}\\big)^2 \\tag{55} \\] <p>where \\(\\mu^{(l)}\\) and \\(\\sigma^2\\) is the mean and variance of all output values in layer \\(l\\).</p> <p>Since mean is ZERO and variance is unity,</p> \\[ \\begin{align} &amp;\\frac{1}{n_{l-1}}\\sum_{i=0}^{n_{l-1}}\\big(a^{(l)}_i\\big)^2 = 1\\\\ \\implies &amp;\\sum_{i=0}^{n_{l- }}\\big(a^{(l)}_i\\big)^2 = n_{l-1} \\tag{56}   \\end{align} \\] <p>Using the above equation in Equation 54, we get:</p> \\[\\begin{align} &amp; \\implies -1 \\leq w^{(l)}\\sqrt{n_{l-1}}\\leq 1\\\\ &amp;\\implies \\frac{-1}{\\sqrt{n_{l-1}}} \\leq w^{(l)}  \\leq \\frac{1}{\\sqrt{n_{l-1}}} \\tag{57} \\end{align} \\] <p>So we know how to initialize weights. However still weights from different initialization can have different effects on the outputs. You should try to train the network on different random seeds and figure out the best one.</p> <p>Also note that the above equation came from an assumption as well as approximation.</p> <p>Also, as we know that it is better to have small inputs to a sigmoid to give better results, so \\(\\beta\\) should also be small(\\(\\beta \\leq 3.0\\)).</p>","tags":["neural","networks","perceptron","deep","learning","feedforward","networks","backpropagation","momentum","multiple","layers"]},{"location":"neural_networks/multiayer-perceptron/multi-layer-perceptron/#25-speeding-up-the-code","title":"2.5 Speeding up the code","text":"","tags":["neural","networks","perceptron","deep","learning","feedforward","networks","backpropagation","momentum","multiple","layers"]},{"location":"neural_networks/multiayer-perceptron/multi-layer-perceptron/#251-speeding-up-the-forward-process","title":"2.5.1 Speeding Up the  forward process","text":"<p>We can compute the activations of multiple examples at once using matrix operations. </p> <p>Let's say we have \\(k\\) number of training examples and each example has \\(m\\) features with \\(n\\) types of outputs.</p> <p>Now we can store our inputs in a matrix where each row represents a training example and a column represents a feature.</p> <p>So our training example is like:</p> <p>$$X= \\begin{bmatrix} x_{11} &amp; x_{12} &amp; x_{13} &amp; \\cdots &amp; x_{1m} \\ x_{21} &amp; x_{22} &amp; x_{23} &amp; \\cdots &amp; x_{2m} \\ x_{31} &amp; x_{32} &amp; x_{33} &amp; \\cdots &amp; x_{3m} \\ \\vdots &amp; \\vdots &amp; \\vdots &amp; &amp; \\vdots \\ x_{k1} &amp; x_{k2} &amp; x_{k3} &amp; \\cdots &amp; x_{km} \\ \\end{bmatrix} \\tag{58} $$ The matrix shape is \\(k \\times m\\).</p> <p>We will use <code>numpy</code> library to store inputs. </p> <pre><code>import numpy as np\n</code></pre> <p>Let's say we want to train the Logical-XOR function. So our input should look like:</p> <p>So our input should look like:</p> \\[X= \\begin{bmatrix} 0 &amp; 0 \\\\ 0 &amp; 1 \\\\ 1 &amp; 0 \\\\ 1 &amp; 1 \\\\ \\end{bmatrix} \\] <p>We can make it make it using <code>np.array</code> method:</p> <pre><code>X = np.array([[0,0],\n             [0,1],\n             [1,0],\n             [1,1]])\nX\n</code></pre> <pre><code>array([[0, 0],\n       [0, 1],\n       [1, 0],\n       [1, 1]])\n</code></pre> <p>Now our input is ready, let's figure out how to store the target values. Each input has target values to denote which neuron to fire and which not to(1s and 0s). This type of output is for classification process.</p> <p>So with \\(k\\) examples and \\(n\\) output neurons, the target matrix should be \\([t_{ij}]\\) which is the target for \\(i^{th}\\) example and \\(j^{th}\\) neuron.</p> <p>So,</p> \\[ T=\\begin{bmatrix} t_{11} &amp; t_{12} &amp; t_{13} &amp; \\cdots &amp; t_{1n}\\\\ t_{21} &amp; t_{22} &amp; t_{23} &amp; \\cdots &amp; t_{2n}\\\\ t_{31} &amp; t_{32} &amp; t_{33} &amp; \\cdots &amp; t_{3n}\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ t_{k1} &amp; t_{k2} &amp; t_{k3} &amp; \\cdots &amp; t_{kn}\\\\ \\end{bmatrix}\\\\ \\text{where $t_{ij} \\in \\{0,1\\}$}\\tag{59} \\] <p>For binary outputs, like in our example, we can just use one output neuron, which will fire for one output and not fire for other which means \\(n=1\\).</p> <p>It is same for regression problems just instead of output being 1 or 0, it is real valued.</p> <p>So, </p> \\[ T_{bin/reg} = \\begin{bmatrix} t_1\\\\ t_2\\\\ \\vdots \\\\ t_k \\end{bmatrix}\\tag{60} \\] <p>and in our example,</p> \\[ T = \\begin{bmatrix} 0\\\\ 1\\\\ 1\\\\ 0 \\end{bmatrix}\\tag{61} \\] <p>We can do it in numpy in the same way:</p> <pre><code>T = np.array([[0],[1],[1],[0]]);T\n</code></pre> <pre><code>array([[0],\n       [1],\n       [1],\n       [0]])\n</code></pre> <p>Now since we have multiple layers and each have their own output(of zeroes and ones), we will have an activation matrix for each layer, where \\([a^{(l)}_{ij}]\\) is the ouput for \\(i^{th}\\) example from \\(j^{th}\\) neuron in \\(l^{th}\\) layer.</p> <p>Now since there will be many matrices, we will save them in a python <code>list</code>. It will be a list of numpy arrays.</p> \\[A^{(l)} = \\begin{bmatrix} a^{(l)}_{11} &amp; a^{(l)}_{12} &amp; a^{(l)}_{13} &amp; \\cdots &amp; a^{(l)}_{1n_l}\\\\ a^{(l)}_{21} &amp;  a^{(l)}_{22} &amp; a^{(l)}_{23} &amp; \\cdots &amp; a^{(l)}_{2n_l}\\\\ a^{(l)}_{31} &amp; a^{(l)}_{32} &amp; a^{(l)}_{33} &amp; \\cdots &amp;  a^{(l)}_{3n_l}\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ a^{(l)}_{k1} &amp; a^{(l)}_{k2} &amp; a^{(l)}_{k3} &amp; \\cdots &amp; a^{(l)}_{kn_l}\\\\ \\end{bmatrix} \\\\\\tag{62} \\] <p>This matrix will be \\(k \\times n_l\\).</p> <p>Or</p> \\[A^{(l)} = g_{(l)}\\Bigg(\\begin{bmatrix} h^{(l)}_{11} &amp; h^{(l)}_{12} &amp; h^{(l)}_{13} &amp; \\cdots &amp; h^{(l)}_{1n_l}\\\\ h^{(l)}_{21} &amp; h^{(l)}_{22} &amp; h^{(l)}_{23} &amp; \\cdots &amp; h^{(l)}_{2n_l}\\\\ h^{(l)}_{31} &amp; h^{(l)}_{32} &amp; h^{(l)}_{33} &amp;  \\cdots &amp; h^{(l)}_{3n_l}\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ h^{(l)}_{k1} &amp; h^{(l)}_{k2} &amp; h^{(l)}_{k3} &amp; \\cdots &amp; h^{(l)}_{kn_l}\\\\ \\end{bmatrix}\\Bigg)\\tag{63} \\] <p>where </p> <ul> <li>\\(g_{(l)}\\) is the activation of layer \\(l\\).</li> <li>\\(h^{(l)}_{pq} = \\sum_{c=0}^{n_{l-1}}a^{(l-1)}_{pc}w^{(l)}_{cq}\\)</li> </ul> \\[ \\implies A^{(l)} = g_{(l)}\\big(H^{(l)}\\big)\\tag{64} \\] \\[A^{(l)} = g_{(l)}\\Bigg( \\begin{bmatrix} \\sum_{c=0}^{n_{l-1}}a^{(l-1)}_{1c}w^{(l)}_{c1} &amp;\\sum_{c=0}^{n_{l- 1}}a^{(l-1)}_{1c}w^{(l)}_{c2} &amp;\\cdots &amp;\\sum_{c=0}^{n_{l-1}}a^{(l-1)}_{1c}w^{(l)}_{cn_{l}}\\\\\\\\ \\sum_{c=0}^{n_{l- 1}}a^{(l-1)}_{2c}w^{(l)}_{c1} &amp;\\sum_{c=0}^{n_{l-1}}a^{(l-1)}_{2c}w^{(l)}_{c2} &amp;\\cdots &amp;\\sum_{c=0}^{n_{l-1}}a^{(l- 1)}_{2c}w^{(l)}_{cn_{l}}\\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ \\sum_{c=0}^{n_{l-1}}a^{(l-1)}_{kc}w^{(l)}_{c1} &amp;\\sum_{c=0}^{n_{l-1}}a^{(l- 1)}_{kc}w^{(l)}_{c2} &amp;\\cdots &amp;\\sum_{c=0}^{n_{l-1}}a^{(l-1)}_{kc}w^{(l)}_{cn_{l}}\\\\ \\end{bmatrix} \\Bigg)\\tag{65} \\] <p>The above matrix is a matrix multiplication of two matrices.</p> \\[A^{(l)} = g_{(l)}\\Bigg( \\begin{bmatrix} a^{(l-1)}_{10} &amp; a^{(l-1)}_{11} &amp; \\cdots &amp; a^{(l-1)}_{1n_{l-1}}\\\\  a^{(l- 1)}_{20} &amp; a^{(l-1)}_{21} &amp; \\cdots &amp; a^{(l-1)}_{2n_{l-1}}\\\\  \\vdots &amp; \\vdots &amp; &amp; \\vdots\\\\ a^{(l-1)}_{k0} &amp; a^{(l-1)}_{k1} &amp; \\cdots &amp;  a^{(l-1)}_{kn_{l-1}} \\end{bmatrix}  \\times  \\begin{bmatrix} w^{(l)}_{01} &amp; w^{(l)}_{02} &amp; \\cdots &amp; w^{(l)}_{0n_l}\\\\ w^{(l)}_{11} &amp; w^{(l)}_{12} &amp; \\cdots &amp; w^{(l)}_{1n_l}\\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots\\\\ w^{(l)}_{n_{l-1}1}&amp; w^{(l)}_{n_{l-1}2} &amp; \\cdots &amp;  w^{(l)}_{n_{l-1}n_l}\\\\ \\end{bmatrix} \\Bigg)\\tag{66} \\] <p>The left matrix looks like an activation matrix for the previous layer with extra bias column at the front.</p> <p>We can generate a column of -1 using the <code>np.ones</code> method and then concatenate it with our matrix using <code>np.concatenate</code> to form this matrix. We can define an activation matrix with bias as of layert \\(l\\) as:</p> \\[ A^{(l)}_{bias}= \\begin{bmatrix} a^{(l)}_{10} &amp; a^{(l)}_{11} &amp; \\cdots &amp; a^{(l)}_{1n_{l}}\\\\  a^{(l- 1)}_{20} &amp; a^{(l)}_{21} &amp; \\cdots &amp; a^{(l)}_{2n_{l}}\\\\  \\vdots &amp; \\vdots &amp; &amp; \\vdots\\\\ a^{(l)}_{k0} &amp; a^{(l- 1)}_{k1} &amp; \\cdots &amp; a^{(l)}_{kn_{l}} \\end{bmatrix}\\tag{67} \\] <p>and the right matrix of Equatio 66 is the weight matrix of layer \\(l\\), where \\(w^{(l)}_{ij}\\) is the weight from \\(i^{th}\\) node of layer \\((l-1)\\) to \\(j^{th}\\) node of layer \\(l\\).</p> \\[ W^{(l)}= \\begin{bmatrix} w^{(l)}_{01} &amp; w^{(l)}_{02} &amp; \\cdots &amp; w^{(l)}_{0n_l}\\\\ w^{(l)}_{11} &amp; w^{(l)}_{12} &amp;  \\cdots &amp; w^{(l)}_{1n_l}\\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ w^{(l)}_{n_{l-1}1}&amp; w^{(l)}_{n_{l-1}2} &amp; \\cdots &amp; w^{(l)}_{n_{l-1}n_l}\\\\ \\end{bmatrix}\\tag{68} \\] <p>this is a \\(\\big((n_{l-1}+1) \\times n_{l}\\big)\\) matrix</p> <p>so we can say,</p> \\[A^{(l)} = g_{(l)}\\big(A^{(l-1)}_{bias} \\times W^{(l)}\\big)\\tag{69}\\] <p>and $$A^{(l)}_{bias} = \\begin{bmatrix} -1 &amp; A^{(l)} \\end{bmatrix}\\tag{70} $$</p> <p>where: - \\(\\times\\) is the matrix multiplication - \\(\\begin{bmatrix} M &amp; N \\end{bmatrix}\\) is the horizontal concatenation of matrix \\(M\\) and \\(N\\). - \\(g_{(l)}\\) is the activation of layer \\(l\\).</p> <p>We will use the <code>np.matmul</code> function to perform a matrix multiplication and we will use the numpy broadcasting to compute activations.</p>","tags":["neural","networks","perceptron","deep","learning","feedforward","networks","backpropagation","momentum","multiple","layers"]},{"location":"neural_networks/multiayer-perceptron/multi-layer-perceptron/#252-speeding-up-the-backward-process","title":"2.5.2 Speeding Up the backward process","text":"<p>Now that we have completed the forward process, let's remind of the backward equations.</p> <p>We update weights of any layer \\(l\\) using:</p> \\[ w^{(l)}_{pq} \\leftarrow w^{(l)}_{pq} - \\eta \\delta^{(l)}_qa^{(l-1)}_p\\tag{71} \\] <p>For \\(k\\) examples, we can</p> \\[ w^{(l)}_{pq} \\leftarrow w^{(l)}_{pq} - \\eta \\sum_{i=1}^{k}\\delta^{(l)}_{iq}a^{(l-1)}_{ip}\\tag{72} \\] <p>We can turn it into a matrix operation as:</p> \\[W^{(l)} \\leftarrow  W^{(l)} - \\eta \\Delta W^{(l)} \\tag{73} \\] \\[ \\Delta W^{(l)} =  \\begin{bmatrix} \\Delta w^{(l)}_{01} &amp; \\Delta w^{(l)}_{02} &amp; \\cdots &amp; \\Delta w^{(l)}_{0n_l}\\\\ \\Delta w^{(l)}_{11} &amp; \\Delta w^{(l)}_{12} &amp; \\cdots &amp; \\Delta w^{(l)}_{1n_l}\\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ \\Delta w^{(l)}_{n_{l-1}1} &amp;  \\Delta w^{(l)}_{n_{l-1}2} &amp; \\cdots &amp; \\Delta w^{(l)}_{n_{l-1}n_l} \\end{bmatrix} \\tag{74} \\] <p>where \\(\\Delta w^{(l)}_{pq} = \\sum_{i=1}^{k}\\delta^{(l)}_{iq}a^{(l-1)}_{ip}\\)</p> \\[ \\implies \\Delta W^{(l)} =  \\begin{bmatrix} \\sum_{i=1}^{k}\\delta^{(l)}_{i1}a^{(l-1)}_{i0} &amp; \\sum_{i=1}^{k} \\delta^{(l)}_{i2}a^{(l-1)}_{i0} &amp; \\cdots &amp; \\sum_{i=1}^{k}\\delta^{(l)}_{in_l}a^{(l-1)}_{i0}\\\\\\\\ \\sum_{i=1}^{k} \\delta^{(l)}_{i1}a^{(l-1)}_{i1} &amp; \\sum_{i=1}^{k}\\delta^{(l)}_{i2}a^{(l-1)}_{i1} &amp; \\cdots &amp; \\sum_{i=1}^{k} \\delta^{(l)}_{in_l}a^{(l-1)}_{i1}\\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots\\\\ \\sum_{i=1}^{k}\\delta^{(l)}_{i1}a^{(l- 1)}_{in_{l-1}} &amp; \\sum_{i=1}^{k}\\delta^{(l)}_{i2}a^{(l-1)}_{in_{l-1}} &amp; \\cdots &amp; \\sum_{i=1}^{k} \\delta^{(l)}_{in_l}a^{(l-1)}_{in_{l-1}} \\end{bmatrix} \\tag{75} \\] <p>The matrix above looks like a matrix multiplication, let's open it</p> \\[\\implies \\Delta W^{(l)} = \\begin{bmatrix} a^{(l-1)}_{10} &amp; a^{(l-1)}_{20} &amp; \\cdots &amp; a^{(l-1)}_{k0}\\\\ a^{(l- 1)}_{11} &amp; a^{(l-1)}_{21} &amp; \\cdots &amp; a^{(l-1)}_{k1}\\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ a^{(l-1)}_{1n_{l-1}} &amp; a^{(l- 1)}_{2n_{l-1}} &amp; \\cdots &amp; a^{(l-1)}_{kn_{l-1}}\\\\ \\end{bmatrix} \\times \\begin{bmatrix} \\delta^{(l)}_{11} &amp;  \\delta^{(l)}_{12} &amp; \\cdots &amp; \\delta^{(l)}_{1n_l}\\\\ \\delta^{(l)}_{21} &amp; \\delta^{(l)}_{22} &amp; \\cdots &amp;  \\delta^{(l)}_{2n_l}\\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ \\delta^{(l)}_{k1} &amp; \\delta^{(l)}_{k2} &amp; \\cdots &amp;  \\delta^{(l)}_{kn_l}\\\\ \\end{bmatrix} \\tag{76} \\] <p>where \\(\\times\\) is the matrix multiplication.</p> <p>Using Equation 67,</p> \\[\\implies \\Delta W^{(l)}= (A^{(l-1)}_{bias})^T \\times \\begin{bmatrix} \\delta^{(l)}_{11} &amp; \\delta^{(l)}_{12} &amp;  \\cdots &amp; \\delta^{(l)}_{1n_l}\\\\ \\delta^{(l)}_{21} &amp; \\delta^{(l)}_{22} &amp; \\cdots &amp; \\delta^{(l)}_{2n_l}\\\\ \\vdots &amp;  \\vdots &amp; &amp; \\vdots \\\\ \\delta^{(l)}_{k1} &amp; \\delta^{(l)}_{k2} &amp; \\cdots &amp; \\delta^{(l)}_{kn_l}\\\\ \\end{bmatrix} \\Bigg) \\tag{77} \\] \\[ \\implies W^{(l)} \\leftarrow W^{(l)}- \\eta \\big( (A^{(l-1)}_{bias})^T \\times \\Delta^{(l)} \\big) \\tag{78} \\] <p>where,</p> \\[ \\Delta^{(l)} =  \\begin{bmatrix} \\delta^{(l)}_{11} &amp; \\delta^{(l)}_{12} &amp; \\cdots &amp; \\delta^{(l)}_{1n_l} \\\\ \\delta^{(l)}_{21} &amp; \\delta^{(l)}_{22} &amp; \\cdots &amp; \\delta^{(l)}_{2n_l} \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ \\delta^{(l)}_{k1} &amp; \\delta^{(l)}_{k2} &amp; \\cdots &amp; \\delta^{(l)}_{kn_l} \\\\ \\end{bmatrix}\\tag{79} \\] <p>It is called the error matrix of layer \\(l\\) where \\(\\delta^{(l)}_{ij}\\) is the error in \\(j^{th}\\) neuron of layer \\(l\\) for \\(i^{th}\\) training example.</p> <p>The matrix is \\(\\big(k \\times n_l\\big)\\).</p> <p>For the final output layer,</p> \\[ \\Delta^{(L)} =  \\begin{bmatrix} \\delta^{(L)}_{11} &amp; \\delta^{(L)}_{12} &amp; \\cdots &amp; \\delta^{(L)}_{1n_L} \\\\ \\delta^{(L)}_{21} &amp; \\delta^{(L)}_{22} &amp; \\cdots &amp; \\delta^{(L)}_{2n_L} \\\\ \\vdots &amp; \\vdots &amp; 7 \\vdots \\\\  \\delta^{(L)}_{k1} &amp; \\delta^{(L)}_{k2} &amp; \\cdots &amp; \\delta^{(L)}_{kn_L} \\\\ \\end{bmatrix}\\tag{80} \\] <p>The matrix is \\(\\big(k \\times n_L)\\).</p> <p>We know from Equations 18, </p> \\[\\implies\\delta^{(L)}_q=g_{(L)}'(h^{(L)}_q) \\ \\mathcal{L}'\\big(y_q,t_q\\big) \\tag{18}\\] <p>for a training example, \\(i\\),</p> \\[\\implies  \\delta^{(L)}_{iq}=g_{(L)}'(h^{(L)}_{iq}) \\ \\mathcal{L}'\\big(y_{iq},t_{iq}\\big) \\tag{81}\\] <p>and for our chosen loss function, </p> \\[ \\mathcal{L}'\\big(y_{iq},t_{iq}\\big) = y_{iq} - t_{iq} \\] <p>Let's first compute the \\(\\Delta^{(L)}\\),</p> <p>Using Equation 81,</p> \\[ \\Delta^{(L)}= \\begin{bmatrix} (y_{11} - t_{11}) g_{(L)}'(h^{(L)}_{11}) &amp; (y_{12} - t_{12})  g_{(L)}'(h^{(L)}_{12}) &amp; \\cdots &amp; (y_{1n_L} - t_{1n_L}) g_{(L)}'(h^{(L)}_{1n_L})\\\\\\\\ (y_{21} - t_{21})  g_{(L)}'(h^{(L)}_{21}) &amp; (y_{22} - t_{22}) g_{(L)}'(h^{(L)}_{22}) &amp; \\cdots &amp; (y_{2n_L} - t_{2n_L})  g_{(L)}'(h^{(L)}_{2n_L})\\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ (y_{k1} - t_{k1}) g_{(L)}'(h^{(L)}_{k1}) &amp; (y_{k2} -  t_{k2}) g_{(L)}'(h^{(L)}_{k2}) &amp; \\cdots &amp; (y_{kn_L} - t_{kn_L}) g_{(L)}'(h^{(L)}_{kn_L})\\\\ \\end{bmatrix}\\tag{82} \\] <p>Opening up,</p> \\[\\implies \\Delta^{(L)}= \\begin{bmatrix} y_{11} - t_{11} &amp; y_{12} - t_{12} &amp; \\cdots &amp; y_{1n_L} - t_{1n_L}\\\\ y_{21} - t_{21} &amp; y_{22} - t_{22} &amp; \\cdots &amp; y_{2n_L} - t_{2n_L}\\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ y_{k1} - t_{k1} &amp;  y_{k2} - t_{k2} &amp; \\cdots &amp; y_{kn_L} - t_{kn_L}\\\\ \\end{bmatrix} * g_{(L)}'\\Bigg( \\begin{bmatrix} h^{(L)}_{11}  &amp;h^{(L)}_{12} &amp; \\cdots &amp; h^{(L)}_{1n_L}\\\\ h^{(L)}_{21} &amp;h^{(L)}_{22} &amp; \\cdots &amp; h^{(L)}_{2n_L}\\\\ \\vdots &amp; \\vdots  &amp; &amp; \\vdots \\\\ h^{(L)}_{k1} &amp;h^{(L)}_{k2} &amp; \\cdots &amp; h^{(L)}_{kn_L}\\\\ \\end{bmatrix} \\Bigg)\\tag{83} \\] <p>since \\(y_i = a^{(L)}_i\\),</p> \\[\\implies \\Delta^{(L)}= \\begin{bmatrix} a^{(L)}_{11} - t_{11} &amp; a^{(L)}_{12} - t_{12} &amp; \\cdots &amp; a^{(L)}_{1n_L} -  t_{1n_L}\\\\ a^{(L)}_{21} - t_{21} &amp; a^{(L)}_{22} - t_{22} &amp; \\cdots &amp; a^{(L)}_{2n_L} - t_{2n_L}\\\\ \\vdots &amp; \\vdots &amp; &amp;  \\vdots\\\\ a^{(L)}_{k1} - t_{k1} &amp; a^{(L)}_{k2} - t_{k2} &amp; \\cdots &amp; a^{(L)}_{kn_L} - t_{kn_L}\\\\ \\end{bmatrix} * g_{(L)}' \\Bigg( \\begin{bmatrix} h^{(L)}_{11} &amp;h^{(L)}_{12} &amp; \\cdots &amp; h^{(L)}_{1n_L}\\\\ h^{(L)}_{21} &amp;h^{(L)}_{22} &amp;  \\cdots &amp; h^{(L)}_{2n_L}\\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots\\\\ h^{(L)}_{k1} &amp;h^{(L)}_{k2} &amp; \\cdots &amp; h^{(L)}_{kn_L}\\\\ \\end{bmatrix} \\Bigg)\\tag{84} \\] \\[\\implies \\Delta^{(L)} = (A^{(L)} - T)*g_{(L)}'(H^{(L)})\\tag{85} \\] <p>where \\(*\\) is the elementwise multiplication and not matrix multiplication.</p> <p>Now let's compute \\(\\Delta^{(l)}\\),</p> <p>Recalling Equation 51,</p> \\[ \\delta^{(l)}_k = \\beta^{(l)} a^{(l)}_k(1-a^{(l)}_k) \\sum_{i=1}^{n_{l+1}}\\Big[ \\delta^{(l+1)}_i w^{(l+1)}_{ki} \\Big] \\tag{51} \\] <p>for training example \\(j\\),</p> \\[ \\delta^{(l)}_{jk} = \\beta^{(l)} a^{(l)}_{jk}(1-a^{(l)}_{jk}) \\sum_{i=1}^{n_{l+1}}\\Big[ \\delta^{(l+1)}_{ji} w^{(l+1)}_{ki} \\Big] \\tag{86} \\] <p>Using Equation 86,</p> \\[ \\Delta^{(l)} =  \\begin{bmatrix} \\beta^{(l)} a^{(l)}_{11}(1-a^{(l)}_{11}) \\sum_{c=1}^{n_{l+1}}\\Big[ \\delta^{(l+1)}_{1c} w^{(l+1)}_{1c}  \\Big] &amp; \\cdots &amp; \\beta^{(l)} a^{(l)}_{1n_l}(1-a^{(l)}_{1n_l}) \\sum_{c=1}^{n_{l+1}}\\Big[ \\delta^{(l+1)}_{1c} w^{(l+1)}_{n_lc} \\Big] \\\\\\\\  \\beta^{(l)} a^{(l)}_{21}(1-a^{(l)}_{21}) \\sum_{c=1}^{n_{l+1}}\\Big[ \\delta^{(l+1)}_{2c} w^{(l+1)}_{1c} \\Big] &amp;  \\cdots &amp; \\beta^{(l)} a^{(l)}_{2n_l}(1- a^{(l)}_{2n_l}) \\sum_{c=1}^{n_{l+1}}\\Big[ \\delta^{(l+1)}_{2c} w^{(l+1)}_{n_lc} \\Big] \\\\ \\vdots  &amp; &amp; \\vdots  \\\\ \\beta^{(l)} a^{(l)}_{k1}(1-a^{(l)}_{k1}) \\sum_{c=1}^{n_{l+1}}\\Big[ \\delta^{(l+1)}_{kc} w^{(l+1)}_{1c} \\Big] &amp; \\cdots &amp; \\beta^{(l)} a^{(l)}_{kn_l}(1-a^{(l)}_{kn_l}) \\sum_{c=1}^{n_{l+1}}\\Big[ \\delta^{(l+1)}_{kc} w^{(l+1)}_{n_lc} \\Big] \\end{bmatrix} \\tag{87} \\] <p>Using Equation 67,</p> \\[\\implies \\Delta^{(l)}= \\beta^{(l)} A^{(l)}*(I_1-A^{(l)})* \\begin{bmatrix} \\sum_{c=1}^{n_{l+1}}\\Big[ \\delta^{(l+1)}_{1c} w^{(l+1)}_{1c} \\Big] &amp; \\cdots &amp; \\sum_{c=1}^{n_{l+1}}\\Big[ \\delta^{(l+1)}_{1c} w^{(l+1)}_{n_lc} \\Big] \\\\\\\\ \\sum_{c=1}^{n_{l+1}}\\Big[ \\delta^{(l+1)}_{2c} w^{(l+1)}_{1c} \\Big] &amp; \\ \\cdots &amp; \\sum_{c=1}^{n_{l+1}}\\Big[ \\delta^{(l+1)}_{2c} w^{(l+1)}_{n_lc} \\Big] \\\\ \\vdots &amp;  &amp; \\vdots\\\\ \\sum_{c=1}^{n_{l+1}}\\Big[ \\delta^{(l+1)}_{kc} w^{(l+1)}_{1c} \\Big] &amp; \\cdots &amp; \\sum_{c=1}^{n_{l+1}}\\Big[ \\delta^{(l+1)}_{kc} w^{(l+1)}_{n_lc} \\Big] \\end{bmatrix} \\tag{88} \\] <p>where \\(I_1\\) is the matrix full of 1 of the same shape as \\(A^{(l)}\\) and \\(*\\) represents elementwise multiplication.</p> <p>the matrix on the right looks like a matrix multiplication of two matrices. Let's open it up.</p> \\[ \\Delta^{(l)}= \\beta^{(l)} A^{(l)}*(I_1-A^{(l)})* \\Bigg( \\begin{bmatrix} \\delta^{(l+1)}_{11} &amp;  \\delta^{(l+1)}_{12} &amp; \\cdots &amp; \\delta^{(l+1)}_{1n_{l+1}}\\\\\\\\  \\delta^{(l+1)}_{21} &amp; \\delta^{(l+1)}_{22} &amp; \\cdots &amp;  \\delta^{(l+1)}_{2n_{l+1}}\\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots\\\\ \\delta^{(l+1)}_{k1} &amp; \\delta^{(l+1)}_{k2} &amp; \\cdots &amp;  \\delta^{(l+1)}_{kn_{l+1}}\\\\ \\end{bmatrix} \\times \\begin{bmatrix} w^{(l+1)}_{11}&amp;w^{(l+1)}_{21} &amp; \\cdots &amp;  w^{(l+1)}_{n_l1}\\\\\\\\ w^{(l+1)}_{12}&amp;w^{(l+1)}_{22} &amp; \\cdots &amp; w^{(l+1)}_{n_l2}\\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots\\\\ w^{(l+1)}_{1n_{l+1}}&amp;w^{(l+1)}_{2n_{l+1}} &amp; \\cdots &amp; w^{(l+1)}_{n_ln_{l+1}}\\\\ \\end{bmatrix} \\Bigg) \\tag{89} \\] <p>Using Equation 68 and 79,</p> \\[ \\Delta^{(l)}= \\beta^{(l)} A^{(l)}*(I_1-A^{(l)})* (\\Delta^{(l+1)} \\times (W^{(l+1)}_{biasless})^T) \\tag{79} \\] <p>where  - \\(*\\) is the elementwise multiplication  - \\(\\times\\) is the matrix multiplication. - \\(W^{(l)}_{biasless}\\) is the weight matrix for \\(l^{th}\\) layer without the weights from the bias nodes of previous layer.</p> <p>Side Note: We can produce a biasless weight matrix from a complete matrix by simple slicing.</p>","tags":["neural","networks","perceptron","deep","learning","feedforward","networks","backpropagation","momentum","multiple","layers"]},{"location":"neural_networks/multiayer-perceptron/multi-layer-perceptron/#26-code","title":"2.6 Code","text":"<pre><code>class MLP:\n    def init_weights(self,layer_sizes,random_state):\n        #save weights in a list of matrices\n        np.random.seed(random_state)\n        self.weights = [np.random.rand(layer_sizes[l-1]+1,layer_sizes[l])*(2/np.sqrt(layer_sizes[l-1]))-(1/np.sqrt(layer_sizes[l-1])) for l in range(1,len(layer_sizes))]\n    def sigmoid(self,x):\n        return 1/(1+np.exp(-x)) # keep beta = 1\n\n    def forward(self,A_0,weights=None):\n        self.outputs=[]\n        A_l = A_0\n        self.outputs.append(A_0)\n        if not weights:\n            weights = self.weights\n        for weight in weights:\n            A_lbias = np.concatenate(((-np.ones((A_l.shape[0],1)),A_l)),axis=1) # add bias to input data\n            H_l = np.matmul(A_lbias,weight) # compute the summation\n            A_l = self.sigmoid(H_l) # compute the activation\n            self.outputs.append(A_l)\n        return A_l # return the final output\n\n    def backward(self,T, learning_rate):\n        A_L = self.outputs[-1]\n        delta_L = (A_L-T)*(A_L*(1-A_L)) # beta = 0\n        delta_l_next = delta_L\n\n        for i in range(len(self.weights)-1,-1,-1):\n#             print(i)\n            A_l = self.outputs[i]\n            #compute error for previous layer\n            delta_l = A_l*(1-A_l)*(np.matmul(delta_l_next,np.transpose(self.weights[i][1:,:])))\n#             A_0 A_1 A_2\n#             W_1 W_2\n#             0   1    2\n            # add bias output to output matrix\n            A_lbias = np.concatenate(((-np.ones((A_l.shape[0],1)),A_l)),axis=1)\n            #update weights using the next errors\n\n\n            self.weights[i] = self.weights[i]- (1/T.shape[0])*(learning_rate*(np.matmul(np.transpose(A_lbias),delta_l_next)))\n            # change the next errors for next layer\n            delta_l_next = delta_l\n\n\n\n\n\n    def train(self,input_data,input_target, epochs,layer_sizes=(100,), \n              learning_rate=0.01,random_state=0,verbose=0, save_weights=False):\n        A_0 = np.array(input_data)\n        T = np.array(input_target)\n        layer_sizes=list(layer_sizes)\n        layer_sizes.insert(0,A_0.shape[1])\n        n_outputs = np.unique(T).shape[0] if np.unique(T).shape[0] != 2 else 1\n        layer_sizes.append(n_outputs)\n\n\n        self.init_weights(layer_sizes, random_state=random_state)\n        if save_weights:\n            self.saved_weights = [self.weights.copy()]\n        for e in range(epochs):\n\n\n#             print(\"epoch\",e)\n\n\n            # shuffle the input so we don't train on same sequences\n            idx = np.arange(0,T.shape[0])\n            np.random.shuffle(idx)\n            A_0=A_0[idx]\n            T=T[idx]\n\n            A_L = self.forward(A_0)\n#             print(e)\n            if e%(epochs//10) == 0 and verbose:\n                print(\"epoch:\",e)\n                print(f\"Error: {np.sum((A_L-T)**2)/T.shape[0]}\")\n                print(f\"out: {A_L}\")\n#                 print(\"weights\",*self.weights,sep='\\n',end='\\n\\n')\n            self.backward(T,learning_rate)\n            if save_weights:\n                self.saved_weights.append(self.weights.copy())\n        print(f\"Error: {np.sum((A_L-T)**2)/T.shape[0]}\")\n\n    def predict(self,input_data,weights=None):\n        output = self.forward(np.array(input_data),weights)\n        #since this output is a realnumber(between 0 &amp; 1)\n        # we will have a threshold to predict its class for now 0.5\n        output = (output&gt;0.5)*1\n        return output\n\n    def confmat(self,input_data,targets):\n        '''returns the confusion matrix for binary classification'''\n        outputs = self.predict(np.array(input_data))\n        T = np.array(targets).reshape(outputs.shape)\n        tp = ((T==1)&amp;(outputs==1)).sum()\n        tn = ((T==0)&amp;(outputs==0)).sum()\n        fp = ((T==0)&amp;(outputs==1)).sum()\n        fn = ((T==1)&amp;(outputs==0)).sum()\n        return np.array([[tp,fp],\n                        [fn,tn]])\n</code></pre> <p>Let's try to train the model for XOR data.</p> <pre><code>XOR_inp = np.array([[0, 0],\n       [0, 1],\n       [1, 0],\n       [1, 1]])\n\nXOR_target = np.array([[0],\n       [1],\n       [1],\n       [0]])\n</code></pre> <pre><code>m = MLP()\nm.train(XOR_inp,XOR_target,5001,layer_sizes=(2,),learning_rate=1, random_state=0)\n</code></pre> <pre><code>Error: 0.008145708816099705\n</code></pre> <p>Let's check the confusion matrix for this model</p> <pre><code>m.confmat(XOR_inp,XOR_target)\n</code></pre> <pre><code>array([[2, 0],\n       [0, 2]])\n</code></pre> <p>As we can see all examples have been correctly classified. But it took almost 5000 iterations to do so.</p> <p>Let's see for AND data.</p> <pre><code>AND_inp = np.array([[0,0],\n                   [0,1],\n                   [1,0],\n                   [1,1]])\nAND_target = np.array([[0],\n                      [0],\n                      [0],\n                      [1]])\n\nm2 = MLP()\nm2.train(AND_inp,AND_target,layer_sizes=(2,),learning_rate=1,epochs=5001,random_state=27)\nm2.confmat(AND_inp,AND_target)\n</code></pre> <pre><code>Error: 0.0004818608199957538\narray([[1, 0],\n       [0, 3]])\n</code></pre> <p>We were able to classify this as well, but it took a lot of iterations to do so. So we can classify complex data with multilayer perceptrons, but they come with a computing cost and take a lot of iterations to classify even the linearly separable data.</p> <p>Let's check the decision boundary.</p> <pre><code>xx,yy=np.meshgrid(np.arange(X[:,0].min()-0.1,X[:,0].max()+0.1,(X[:,0].max()-X[:,0].min())/500),\n                      np.arange(X[:,1].min()-0.1,X[:,1].max()+0.1,(X[:,1].max()-X[:,1].min())/500))\nZ = m.predict(np.c_[xx.ravel(),yy.ravel()])\nZ = Z.reshape(xx.shape)*1\n\n\nfig = go.Figure(layout=dict(width=600,height=600,title=\"Decision Boundary for XOR Data\",\n                            xaxis_title=\"Input 1\", yaxis_title=\"Input 2\"))\n\nfig.add_trace(\n    go.Heatmap(\n        x=xx[0],\n        y=yy[:,1],\n        z=Z,\n        colorscale=\"Viridis\",\n        showscale=False\n))\nfig.add_trace(\n    go.Scatter(\n        x=X[:,0],y=X[:,1],mode=\"markers\",\n        marker=dict(\n            size=20,\n            color=T[:,0],\n            colorscale=\"Viridis\",\n            line=dict(color=\"black\",width=2))\n    )\n)\nfig.show()\n</code></pre> <p>{% include mlp/plot6.html %}</p> <pre><code>m = MLP()\nm.train(XOR_inp,XOR_target,5001,layer_sizes=(2,),learning_rate=1, random_state=0,save_weights = True)\n</code></pre> <pre><code>Error: 0.008145708816099705\n</code></pre> <pre><code>xx,yy=np.meshgrid(np.arange(X[:,0].min()-0.1,X[:,0].max()+0.1,(X[:,0].max()-X[:,0].min())/200),\n                      np.arange(X[:,1].min()-0.1,X[:,1].max()+0.1,(X[:,1].max()-X[:,1].min())/200))\n\nZ = [m.predict(np.c_[xx.ravel(),yy.ravel()],weights).reshape(xx.shape)*1 for weights in m.saved_weights[::50]]\n\nnb_frames = 5001//50\n\nfig = go.Figure(frames=[go.Frame(data=[go.Heatmap(x=xx[0],y=yy[:,1],z=Z[k],\n            colorscale=\"Viridis\",showscale=False),],name=str(k))\n                        for k in range(nb_frames)])\n\nfig.add_trace(go.Scatter(x=X[:,0],y=X[:,1],mode=\"markers\",marker=dict(size=20,\n                color=T[:,0],colorscale=\"Viridis\",line=dict(color=\"black\",width=2))))\n\nfig.add_trace(go.Scatter(x=X[:,0],y=X[:,1],mode=\"markers\",marker=dict(\n                size=20,color=T[:,0],colorscale=\"Viridis\",line=dict(color=\"black\",width=2))))\n\nfig.add_trace(go.Scatter(x=X[:,0],y=X[:,1],mode=\"markers\",marker=dict(\n                size=20,color=T[:,0],colorscale=\"Viridis\",line=dict(color=\"black\",width=2))))\n\n\n\ndef frame_args(duration):\n    return {\n            \"frame\": {\"duration\": duration},\n            \"mode\": \"immediate\",\n            \"fromcurrent\": True,\n            \"transition\": {\"duration\": duration, \"easing\": \"linear\"},\n        }\n\nsliders = [\n            {\n                \"pad\": {\"b\": 10, \"t\": 60},\n                \"len\": 0.9,\n                \"x\": 0.1,\n                \"y\": 0,\n                \"steps\": [\n                    {\n                        \"args\": [[f.name], frame_args(0)],\n                        \"label\": str(50*k),\n                        \"method\": \"animate\",\n                    }\n                    for k, f in enumerate(fig.frames)\n                ],\n            }\n        ]\n\n# Layout\nfig.update_layout(title='Change in Decision Boundary with Weight Update',width=600, height=600,\n                  xaxis_title=\"Input 1\", yaxis_title=\"Input 2\",\n                  scene=dict( zaxis=dict(range=[-0.1, 6.8], autorange=False), aspectratio=dict(x=1, y=1, z=1),),\n         updatemenus = [\n            {\n                \"buttons\": [\n                    {\n                        \"args\": [None, frame_args(50)],\n                        \"label\": \"&amp;#9654;\", # play symbol\n                        \"method\": \"animate\",\n                    },\n                    {\n                        \"args\": [[None], frame_args(0)],\n                        \"label\": \"&amp;#9724;\", # pause symbol\n                        \"method\": \"animate\",\n                    },\n                ],\n                \"direction\": \"left\",\n                \"pad\": {\"r\": 10, \"t\": 70},\n                \"type\": \"buttons\",\n                \"x\": 0.1,\n                \"y\": 0,\n            }\n         ],\n         sliders=sliders,\n    showlegend=False\n)\n\nfig.show()\n</code></pre> <p>{% include mlp/plot7.html %}</p>","tags":["neural","networks","perceptron","deep","learning","feedforward","networks","backpropagation","momentum","multiple","layers"]},{"location":"neural_networks/multiayer-perceptron/perceptron-convergence-theorem/","title":"Perceptron Convergence Theorem","text":"","tags":["neural networks","perceptron","deep learning","feedforward networks","proof","mathematics"]},{"location":"neural_networks/multiayer-perceptron/perceptron-convergence-theorem/#introduction","title":"Introduction","text":"<p>The Perceptron Convergence Theorem is an important result as it proves the ability of a perceptron to achieve its result. This proof will be purely mathematical. There are some geometrical intuitions that need to be cleared first. This proof requires some prerequisites - concept of vectors, dot product of two vectors. We will use the <code>train</code> function that we developed in the Mathematics Behind Perceptron  post. It will be included in a <code>utils.py</code> file which you can download  here. Also, ignore the visualization code, if that seems too complex.</p>","tags":["neural networks","perceptron","deep learning","feedforward networks","proof","mathematics"]},{"location":"neural_networks/multiayer-perceptron/perceptron-convergence-theorem/#planes","title":"Planes","text":"<p>Since, we know that a Perceptron classifies only linearly separable data with a linear hyper-plane, let's get some things clear about planes.</p> <p>The equation of an n-dimensional hyper plane is:</p> \\[a_1x_1 + a_2x_2 + \\dots + a_nx_n + d = 0 \\tag{1}\\] <p>where \\(x_1,x_2,\\dots,x_n\\) are orthogonal axes.</p> <p>Another way of describing a plane is in the form of a vector \\(\\mathbf{n} = (a_1,a_2, \\dots , a_n)\\) (which is normal to that plane) and a point(position vector) \\(\\mathbf{x_0}\\) which resides on that plane.</p> <p>Now any vector between point, \\(\\mathbf{x_0}\\), and any other general point \\(\\mathbf{x} = (x_1,x_2,\\dots,x_n)\\) on the plane is perpendicular to that plane, i.e:</p> \\[\\begin{align} \\mathbf{n} \\cdot \\mathbf{(x-x_0)}&amp;=0 \\\\ \\implies  (\\mathbf{n} \\cdot \\mathbf{x}) - (\\mathbf{n} \\cdot \\mathbf{x_0}) &amp;= 0\\end{align} \\tag{2}\\] <p>According to Eq. 1 and 2, using the dot product rule,</p> \\[d = - \\mathbf{n} \\cdot \\mathbf{x_0} \\tag{3}\\] <p>The shortest distance(with sign) of the plane from the origin, \\(p\\) is given by:</p> \\[p=\\frac{d}{\\lVert \\mathbf{n} \\rVert}\\tag{4}\\] <p>If we divide Eq.1 by \\(\\lVert \\mathbf{n} \\rVert\\), then,</p> \\[ \\frac{a_1x_1 + a_2x_2 + \\dots + a_nx_n}{\\lVert \\mathbf{n} \\rVert} = -\\frac{d}{\\lVert \\mathbf{n} \\rVert}\\\\ \\implies \\frac{\\mathbf{n} \\cdot \\mathbf{x}}{\\lVert \\mathbf{n} \\rVert} = -\\frac{d}{\\lVert \\mathbf{n} \\rVert} \\] <p>Using \\(\\frac{\\mathbf{n}}{\\lVert \\mathbf{n} \\rVert}=\\hat{\\mathbf{n}}\\) and Eq. 4, we have</p> \\[\\hat{\\mathbf{n}} \\cdot \\mathbf{x} = - p \\tag{5}\\] <p>The above equation is called the Hessian Normal Form of a plane.</p> <p>In this form, we need a unit vector perpendicular to plane and the distance of the plane from the origin to define the plane.</p>","tags":["neural networks","perceptron","deep learning","feedforward networks","proof","mathematics"]},{"location":"neural_networks/multiayer-perceptron/perceptron-convergence-theorem/#planes-with-perceptron","title":"Planes with Perceptron","text":"<p>The Perceptron prediction rule is as follows:</p> \\[\\sigma=g(h)=\\begin{cases}1&amp;\\text{if }h&gt;0 \\\\ 0&amp;\\text{if }h\\leq0 \\\\ \\end{cases}\\tag{6}\\] <p>and,</p> \\[h=\\sum_{i=0}^m{w_ix_i}\\tag{7}\\] <p>where \\(w_0\\) is the threshold of the neuron and \\(x_0\\) can be any constant, except Zero (we use -1). </p> <p>The decison boundary is formed, when \\(h=0\\), so the equation of the decision hyper plane is:</p> \\[\\sum_{i=0}^m{w_ix_i}=0\\tag{8}\\] <p>Comapring the above equation with the general equation of a plane (Eq. 1), the normal vector, which we will represent by \\(\\mathbf{w}\\), is:</p> \\[ \\mathbf{n} = \\mathbf{w} = (w_1,w_2,\\dots,w_m) \\tag{9}\\] <p>which means the constant term in the general equtaion, \\(d\\), will be:</p> \\[ d = w_0x_0 \\tag{10}\\] <p>which means the perpendicular distance of the plane from origin is,\\(p\\), (from Equation 4):</p> \\[ p = \\frac{w_0x_0}{\\lVert \\mathbf{w} \\rVert} \\tag{11}\\] <p>Now we have the perpendicular vector as well as the perpendicular distance. </p> <p>Let's see an example of the OR dataset</p> <pre><code>import numpy as np\nfrom utils import train\nnp.set_printoptions(precision=2)\n# the dataset\nX = np.array([[0,0],\n             [0,1],\n             [1,0],\n             [1,1]])\nT=np.array([[0],[1],[1],[1]])\n</code></pre> <pre><code># fit the data\nweights = train(X,T,0.25,15,random_state=42)\n</code></pre> <pre><code>Accuracy: 1.0\n</code></pre> <pre><code>#plot decision boundaries, weight vector\n\ninp1 = np.linspace(0,0.5,10000)\n\nfig  = go.Figure(layout=dict(height=800,width=800,\n                            xaxis_title=\"Input 1\", yaxis_title=\"Input 2\",\n                            title=\"Decision Boundary with Scatter\",\n                            autosize=False,\n                            legend=dict(x=0.5,y=1),\n                            annotations=[dict(x=weights[1,0],y=weights[2,0],ax=0,ay=0,arrowhead=1,\n                                             startarrowsize=4,axref=\"x\",ayref=\"y\"),\n                                        dict(x=weights[1,0],y=weights[2,0],showarrow=False,text=f\"Weight Vector {weights[1:,0]}\",\n                                             yanchor=\"bottom\"),\n                                        dict(x=0.07,y=0.14,showarrow=False,text=\"Distance, p\",\n                                             yanchor=\"middle\", xanchor=\"center\", textangle=-40),\n\n                                        ],\n                             shapes=[go.layout.Shape(type=\"path\",path=\"M 0,0 L 0,0.04 L 0.18673,0.21764 L 0.22673,0.21764\",\n                                                     line_color=\"MediumPurple\",),\n\n\n    ]\n                            )\n                )\nfig.add_trace(go.Scatter(x=X[:,0:1][T==1],y=X[:,1:][T==1],name= \"Output: 1\",mode=\"markers\",marker=dict(size=20)))\nfig.add_trace(go.Scatter(x=X[:,0:1][T==0],y=X[:,1:][T==0],name= \"Output: 1\",mode=\"markers\",marker=dict(size=20)))\nfig.add_trace(go.Scatter(x=inp1,y=(-(weights[1,0]*inp1) + weights[0,0])/weights[2,0],name= \"Decision Boundary\",mode=\"lines\",marker=dict(size=20)))\n\nfig.show()\n</code></pre> <p>{% include perceptron-convergence-theorem/fig1.html %}</p> <p>So we can represent the decision boundary with the weight vector and the bias term. </p> <p>The next step is to develop a comparison technique between two decision boundaries. The idea is to show that the decision boundary that our model is creating is close to the actual decision boundary. We will talk more about it in a moment.</p> <p>We can just compare the unit vectors of the two weight vectors and the constant bias terms to find if the decision boundaries are similar or even same.</p> <p>But this solution needs multiple comparisons, a vector comparison and a bias weight comparison. A better way is to consider this is to consider the bias weight as part of the weight vector. </p> <p>So, in our OR example, the data will be considered 3-dimensional, with the 3rd dimension being constant(we have chosen -1).</p> <p>For 3-dimensions, the decision boundary will be a plane, whose general equation is:</p> \\[a_1x_1+a_2x_2+a_3x_3+d=0\\] <p>and expanding Equation 8 for our OR data set, equation of our decision boundary is:</p> \\[w_1x_1+w_2x_2+w_0x_0=0\\] <p>As we we can see, our bias input \\(x_0\\) is the 3rd dimension in this equation, which is constant (-1) in our dataset. The constant term \\(d\\) is Zero.</p> <p>Since \\(d=0\\), for our decision boundary, it means the distance between the hyper-plane and the origin, \\(p=\\frac{d}{\\lVert \\mathbf{w} \\rVert} = 0\\). Which implies the plane always passes through the origin of that higher coordinate system. </p> <p>The above result can be generalized for any linearly separable n-dimensional data, that if we consider the bias inputs as part of the data, the resulting n+1 dimensional data will have its decision boundary passing through origin of the new coordinate system.</p> <p>The above fact stands because in the higher dimension, the constant term is Zero, and so the origin satisfies the new decision boundary.</p> <p>Let's try to verify this on our linearly separable OR-dataset. Our bias input is constant(-1). Let's plot the data along with the decision boundary represented by our weights (now including the threshold/bias weight as well).</p> <pre><code>xx,yy=np.meshgrid(np.arange(-2,1.25,0.1),\n                  np.arange(-2,1.25,0.1))\nZ = (-xx.ravel()*weights[1,0] - yy.ravel()*weights[1,0])/weights[0,0]\nZ = Z.reshape(xx.shape)\n\nfig = go.Figure()\n\n\n#scatter points\nfig.add_trace(\n    go.Scatter3d(x=X[:,0], # First input\n                y=X[:,1], # second input\n                z=[-1]*X.shape[0], # bias additional input\n                 mode=\"markers\",\n                marker=dict(size=10,color=T.squeeze(),colorscale=\"Viridis\"),name=\"Data Point\"\n                )\n\n)\n\n\n#decision boundary\nfig.add_trace(\n    go.Surface(x=xx[0],y=yy[:,0],z=Z,showscale=False,colorscale=\"Viridis\",opacity=0.8\n              ,name=\"Decision Boundary\")\n)\n\n\n# weight vector\nfig.add_trace(\n    go.Cone(x=weights[1], y=weights[2], z=weights[0], u=weights[1], v=weights[2], w=weights[0], sizemode=\"scaled\",\n        sizeref=0.15, anchor=\"tip\", showscale=False, name=\"Weight Vector\", cmax=0, cmin=0, colorscale=[[0,'rgb(0,0,0)'],[1,'rgb(0,0,0)'],]\n    )\n)\nfig.add_trace(\n    go.Scatter3d(x=[0,weights[1,0]], y=[0,weights[2,0]], z=[0,weights[0,0]], mode=\"lines\", line=dict(width=4,color=\"black\"), name=\"Weight Vector\"\n    )\n\n)\n\n\n#origin\nfig.add_trace(\n    go.Scatter3d(\n        x=[0],y=[0],z=[0],name=\"origin\", mode=\"markers\", marker=dict(symbol=\"diamond\", size=5,color=\"green\")\n    )\n)\n\n\nfig.update_layout(\n    scene=go.layout.Scene(\n        camera=dict(\n            eye=dict(\n                x=0,\n                y=0,\n                z=2.2\n            ),\n            up=dict(\n                x=1,\n                y=0,\n                z=0\n            )\n        ),\n        dragmode=\"turntable\",\n        xaxis=dict(\n            title_text=\"Input 1\",\n            range=[-0.3,1.5],\n        ),\n        yaxis=dict(\n            title_text=\"Input 2\",\n            range=[-0.3,1.5],\n\n        ),\n        zaxis=dict(\n            title_text=\"Bias Input\",\n            range=[-1.3,0.5],\n        ),\n    ),\n    showlegend=False,\n    height=700,\n    width=700\n)\n\nfig.show()\n</code></pre> <p>{% include perceptron-convergence-theorem/fig2.html %}</p> <p>If you look through the top of the bias axis, you can see the data points are same as they were in the 2D figure, and the weight vector looks the same as well. And the moment you rotate the above figure you can see this new higher dimensional decision boundary also classifies the data and infact passes through the previous decision boundary(the lower dimensional). Further more, you can also see that this plane has also passed through the origin of the coordinate axis. </p> <p>With this intuition in mind, we can conclude that a higher dimensional decision boundary achieves the same goal, and to represent this higher dimensional hyper-plane, we just need the weight vector(which includes the bias weight as well!), since we already know it's distance from the origin will be Zero. </p> <p>Now that we have established, that only the complete weight vector(it's unit vector) defines the decision hyper-plane, two same vectors(or vectors in the same direction), will define the same hyper plane.</p>","tags":["neural networks","perceptron","deep learning","feedforward networks","proof","mathematics"]},{"location":"neural_networks/multiayer-perceptron/perceptron-convergence-theorem/#the-proof","title":"The Proof","text":"<p>The Perceptron Convergence theorem proved by Rosenblatt in 1962, states that:</p> <p>\"given a linearly separable data, the perceptron will converge to a solution within \\(R/ \\gamma^2\\) iterations, where \\(\\gamma\\) is the distance between the separating hyperplane and the closest datapoint to it.\"</p> <p>However there are some assumptions about it: 1. The data should be linearly separable. 2. For every input vector \\(x\\), \\(\\lVert \\mathbf{x} \\rVert\\) is bound by some constant R. In our proof we will assume \\(\\lVert \\mathbf{x} \\rVert \\leq R\\). 3. Also the learning rate is chosen to be 1.</p> <p>Now to the proof,</p> <p>We know that the data is linearly separable, which means there exists a set of weights which represent the the seperating hyperplane. Let's say these weights are \\(\\mathbf{w^*}\\).</p> <p>Our learning algorithm tries to find some vector \\( \\mathbf{w} \\) that is parallel to \\( \\mathbf{w^*} \\). To see if the vectors are parallel we use the inner product (also called the dot product) \\( \\mathbf{w^*} \\cdot \\mathbf{w} \\).</p> <p>So,</p> \\[ \\mathbf{w^*} \\cdot \\mathbf{w} = \\lVert \\mathbf{w^*}\\rVert \\   \\lVert \\mathbf{w}\\rVert \\cos\\theta \\tag{12}\\] <p>now if two vectors are parallel the angle is \\(0\\), and \\(\\cos{0}=1\\) and so the inner product is maximum. If we show that after each update \\(\\mathbf{w^*} \\cdot \\mathbf{w}\\) increases, we have shown that the perceptron converges. However we need to be a bit more careful as \\(\\mathbf{w^*} \\cdot \\mathbf{w}\\) can also increase if \\(\\lVert \\mathbf{w} \\rVert\\) increases, we also need to check the length of \\(\\mathbf{w}\\) doesn't increase too much.</p> <p>Keeping all that in mind, let's move on.</p> <p>Suppose at the \\(i^{th}\\) iteration of the algorithm, the network sees a particular input vector \\(\\mathbf{x}\\) that has a target(ground truth) \\(t\\), and the output(prediction) was \\(y\\). </p> <p>Now, if the output was wrong, then</p> \\[(t-y)(\\mathbf{w^{(i-1)}} \\cdot \\mathbf{x}) &lt; 0 \\tag{13}\\] <p>To see this result clearly, look at a few examples:</p> <p>e.g, if the target was 1 and output was 0, then,</p> \\[t-y = 1\\] <p>and </p> \\[\\begin{align}\\mathbf{w^{(i-1)}} \\cdot \\mathbf{x} &lt; 0\\\\ \\implies (t-y)(\\mathbf{w^{(i-1)}} \\cdot \\mathbf{x}) &lt; 0 \\end{align} \\] <p>or if the target was 0 and output was 1, then,</p> \\[t-y=-1\\] <p>and </p> \\[\\begin{align}\\mathbf{w^{(i-1)}} \\cdot \\mathbf{x} &gt; 0 \\\\ \\implies (t-y)(\\mathbf{w^{(i-1)}} \\cdot \\mathbf{x}) &lt; 0\\end{align}\\] <p>Now that there is error, the weights need to be updated according to the perceptron updation rule,</p> \\[ \\mathbf{w^{(i)}} = \\mathbf{w^{(i-1)}} - \\eta (y-t)\\mathbf{x} \\tag{14}\\] <p>Note: Please clear it that the above equations have been generalised as \\(\\mathbf{w^{(i)}}\\) is not a single weight, but a vector of weights to a neuron from the input nodes at \\(i^{th}\\) iteration and \\(\\mathbf{x}\\) is not a single input but the input vector(including the bias input), \\(y\\) and \\(t\\) are output and target of a single neuron and the symbol \\(\\cdot\\) represents inner product of two vectors.</p> <p>Coming back to above equation, our proof assumes the learning rate of 1 and let's use \\(t-y\\) instead of \\(y-t\\)(which changed sign, ofcourse!).</p> \\[\\implies \\mathbf{w^{(i)}} = \\mathbf{w^{(i-1)}} + (t-y)\\mathbf{x} \\tag{15}\\] <p>Now to show that \\(w^* \\cdot w\\) increases with iterations, using Equation 15,</p> \\[ \\begin{align} \\mathbf{w^*} \\cdot \\mathbf{w^{(i)}} &amp;= \\mathbf{w^*} \\cdot (\\mathbf{w^{(i-1)}} + (t-y)\\mathbf{x})\\\\ &amp;= \\mathbf{w^*} \\cdot \\mathbf{w^{(i-1)}} + \\mathbf{w^*} \\cdot (t-y)\\mathbf{x}\\\\ &amp;= \\mathbf{w^*} \\cdot \\mathbf{w^{(i-1)}} + (t-y)(\\mathbf{w^*} \\cdot \\mathbf{x})\\\\ \\end{align} \\tag{16} \\] <p>This is where we take a break  and think what \\(\\mathbf{w^*} \\cdot \\mathbf{x}\\) represents.</p> <p>If you have any idea about the signed distance of a point \\(\\mathbf{x}\\) from a hyper-plane(which passes through origin) with coefficient vector \\(\\mathbf{a}\\) is:</p> \\[\\begin{align} D = \\frac{\\mathbf{a} \\cdot \\mathbf{x}}{\\lVert \\mathbf{a} \\rVert} \\\\ \\implies D \\lVert \\mathbf{a} \\rVert = \\mathbf{a} \\cdot \\mathbf{x} \\end{align} \\tag{17}\\] <p>Similarly \\(\\mathbf{w^*} \\cdot \\mathbf{x}\\) represents \\(\\lVert \\mathbf{w^*} \\rVert\\) times the signed distance between the point \\(\\mathbf{x}\\) and the plane represented by our vector \\(\\mathbf{w^*}\\) (which is the correct decision boundary)</p> <p>By signed distance, I mean the sign regarding to what side of the plane the data point lies. Now if we have made an error and misclassified the point, then \\((t-y)\\) will be the opposite the sign of the sign of the distance given by the correct boundary. Give it a little thought, work out on different cases, you'll get it.</p> <p>So \\((t-y)(\\mathbf{w^*} \\cdot \\mathbf{x})\\) represents \\(\\lVert \\mathbf{w^*} \\rVert\\) times the magnitude of distance between the point \\(\\mathbf{x}\\) and the plane represented by our vector \\(\\mathbf{w^*}\\) (which is the correct decision boundary). And the smallest distance between the correct decision boundary and any datapoint is \\(\\gamma\\).</p> <p>So,</p> \\[(t-y)\\mathbf{w^*} \\cdot \\mathbf{x} \\geq \\gamma \\lVert \\mathbf{w^*} \\rVert \\tag{18}\\] <p>Using the above equation in Equation 16,</p> \\[ \\mathbf{w^*} \\cdot \\mathbf{w^{(i)}} \\geq \\mathbf{w^*} \\cdot \\mathbf{w^{(i-1)}} + \\gamma \\lVert \\mathbf{w^*} \\rVert \\tag{19} \\] <p>where \\(\\gamma\\) is the minimum distance between the optimal hyperplane defined by \\(\\mathbf{w^*}\\) and the closest datapoint to it.</p> <p>Now according to above equation, \\(\\mathbf{w^*} \\cdot \\mathbf{w^{(i)}}\\) always increases by at least \\(\\gamma\\) and we initialize the weights to be small random numbers(positive and negative), so after \\(i\\) iterations</p> \\[\\mathbf{w^*} \\cdot \\mathbf{w^{(i)}} \\geq i\\gamma \\lVert \\mathbf{w^*} \\rVert \\tag{20}\\] <p>So we have proved that \\(\\mathbf{w^*} \\cdot \\mathbf{w^{(i)}}\\) increases as iterations increase.</p> <p>Since the R.H.S of the above equation is positive,</p> \\[\\lvert \\mathbf{w^*} \\cdot \\mathbf{w^{(i)}} \\rvert \\geq i\\gamma \\lVert \\mathbf{w^*} \\rVert \\tag{21}\\] <p>Also we use Cauchy\u2013Schwarz inequality,</p> \\[ \\lvert \\mathbf{w^*}  \\cdot \\mathbf{w^{(i)}} \\rvert \\leq \\lVert \\mathbf{w^*} \\rVert \\ \\lVert \\mathbf{w^{(i)}} \\rVert \\tag{22} \\] <p>Using Equations 21 and 22,</p> \\[\\begin{align} &amp; i\\gamma \\lVert \\mathbf{w^*} \\rVert \\leq \\lvert \\mathbf{w^*} \\cdot \\mathbf{w^{(i)}} \\rvert \\leq \\lVert \\mathbf{w^*} \\rVert \\ \\lVert \\mathbf{w^{(i)}} \\rVert \\\\ \\implies &amp; i\\gamma \\lVert \\mathbf{w^*} \\rVert \\leq \\lVert \\mathbf{w^*} \\rVert \\ \\lVert \\mathbf{w^{(i)}} \\rVert \\end{align} \\tag{22}\\] <p>\\(\\lVert \\mathbf{w^*} \\rVert\\) is positive and can be cancelled without affecting the inequality.</p> \\[ i\\gamma \\leq \\lVert \\mathbf{w^{(i)}} \\rVert \\tag{23} \\] <p>Here we have put a lower limit to \\(\\lVert \\mathbf{w^{(i)}}\\rVert\\). Now we have to make sure that \\(\\lVert \\mathbf{w^{(i)}} \\rVert\\) does not increase too much. </p> <p>For that we will use the Equation 15 and square the norm on both sides and use the Vector addition rules: $$ \\begin{align} \\lVert \\mathbf{w^{(i)}}\\rVert ^2 &amp;= \\lVert \\mathbf{w^{(i-1)}} + (t-y)\\mathbf{x} \\rVert ^2\\ &amp;= \\lVert \\mathbf{w^{(i-1)}}\\rVert ^2 + (t-y)^2 \\lVert \\mathbf{x}\\rVert ^2 + 2(t-y)\\mathbf{w^{(i-1)}} \\cdot \\mathbf{x}  \\tag{24} \\end{align} $$</p> <p>Now using Perceptron rule, assumption 2(i.e, \\( \\lVert \\mathbf{x} \\rVert \\) is bound by some constant \\( R \\)) and Equation 13 repectively, we have</p> \\[\\begin{align}(t-y)^2 = 1\\\\  \\lVert \\mathbf{x} \\rVert \\leq R\\\\  (t-y)(\\mathbf{w^{(i-1)}} \\cdot \\mathbf{x}) &lt; 0 \\end{align} \\tag{25}\\] <p>Remember, the last equation here is when the output is incorrect.</p> <p>So using the above equations (25):</p> \\[(t-y)^2 \\lVert \\mathbf{x}\\rVert ^2 + 2(t-y)\\mathbf{w^{(i-1)}} \\cdot \\mathbf{x} \\leq R\\] <p>Using the above equation in Equation 24,</p> \\[ \\lVert \\mathbf{w^{(i)}}\\rVert ^2 \\leq \\lVert \\mathbf{w^{(i-1)}}\\rVert ^2 + R \\tag{26} \\] <p>Which shows \\(\\lVert \\mathbf{w^{(i)}}\\rVert ^2\\) does not increase more than what the input data is bound by. If we normalize the input before training, then the data will be bound by 1.</p> <p>Now according to Equation 26, after i iterations,</p> \\[ \\lVert \\mathbf{w^{(i)}}\\rVert ^2 \\leq iR \\tag{27} \\] <p>Here we have put an upper limit on \\(\\lVert \\mathbf{w^{(i)}}\\rVert\\).</p> <p>We have shown that \\(\\mathbf{w^*} \\cdot \\mathbf{w^{(i)}}\\) increases by atleast \\(i\\gamma \\lVert \\mathbf{w^*}\\rVert\\) and \\(\\lVert \\mathbf{w^{(i)}}\\rVert\\) does not increase more than \\(iR\\). Which means the angle between the vectors is decreasing and so the predicting decision boundary is getting close to the actual decision boundary, and after certain weight updates, the algorithm will converge to the actual boundary.</p> <p>Using Equation 23 and 27,</p> \\[i\\gamma \\leq \\lVert \\mathbf{w^{(i)}}\\rVert  \\leq \\sqrt{iR}\\\\ \\implies i \\leq \\frac{R}{\\gamma^2} \\tag{28}\\] <p>So within \\(R/\\gamma^2\\) iterations, the algorithm must have converged.</p> <p>We have shown that if the data is linearly separable, then the algorithm will converge, and the time it will take is a function distance between the separating hyperplane and the nearest point. This is actually called margin.</p> <p>Note: The perceptron stops learning as soon as it gets all the data correctly classified, and so there is no guarantee that it will find the largest margin, just that if there is a separator, it will find it.</p> <p>Note: By weight update we mean just update weight when the algorithm makes an error and so only those weight updates count to the iterations. We do not count the examples which the algorithm classifies correctly.</p>","tags":["neural networks","perceptron","deep learning","feedforward networks","proof","mathematics"]},{"location":"neural_networks/multiayer-perceptron/perceptron/","title":"Mathematics Behind Perceptron","text":"","tags":["neural","networks","perceptron","deep","learning","feedforward","networks"]},{"location":"neural_networks/multiayer-perceptron/perceptron/#introduction","title":"Introduction","text":"<p>This is actually a notebook made by me during the internet shutdown in Kashmir (since 5th Aug 2019). This notebook, and others, are heavily inspired by  Machine Learning: An Algorithmic Perspective  . This notebook is essentially the notes of chapter 3 of this book. I highly recommend this book for the basic understanding of Machine Learning Algorithms. This post covers mathematics, implementation of the basic perceptron algorithm. I would suggest to practice on the code for perceptron. Although stay away from the visualization code, if that seems too complex.</p>","tags":["neural","networks","perceptron","deep","learning","feedforward","networks"]},{"location":"neural_networks/multiayer-perceptron/perceptron/#hebbs-rule","title":"Hebb's Rule","text":"<p>It states that \"the changes in the strength of synaptic connections are proportional to the correlation in the firing of two connecting neurons\"</p>","tags":["neural","networks","perceptron","deep","learning","feedforward","networks"]},{"location":"neural_networks/multiayer-perceptron/perceptron/#mcculloch-and-pitts-neurons","title":"McCulloch and Pitts Neurons","text":"","tags":["neural","networks","perceptron","deep","learning","feedforward","networks"]},{"location":"neural_networks/multiayer-perceptron/perceptron/#1-introduction","title":"1. Introduction","text":"<p>This was a mathematical model of a neuron. It extracts the basic essentials to accurately represent the entity being studied(the neurons that make our nervous system!), removing all the extraneous details.</p> <p>It is modelled as: 1. a set of weighted inputs, \\(w_i\\), that correspond to synapses. 2. an adder, that sums the input signals. 3. an activation function(usually a threshold function), that decides whether the neuron fires(spikes!) for the current inputs.</p> <p>So,</p> <p>\\(\\(h=\\sum_{i=1}^m{w_ix_i}\\tag{1}\\)\\)</p> <p>where \\(w_i\\) is the weight at the synapse for \\(i^{th}\\) input and \\(x_i\\) is the input from \\(i^{th}\\) neuron into synapse. </p> <p>Now to decide if the neuron fires or not, we need a threshold (\\(\\theta\\)). If the weighted sum of the inputs(\\(h\\)) is greater than the threshold, the neuron fires(i.e output is 1).</p> <p>So the activation function is: $$\\sigma=g(h)=\\begin{cases} 1&amp;\\text{if }h&gt;\\theta\\ 0&amp;\\text{if }h\\leq\\theta\\ \\end{cases} \\tag{2} $$</p>","tags":["neural","networks","perceptron","deep","learning","feedforward","networks"]},{"location":"neural_networks/multiayer-perceptron/perceptron/#2-limitations","title":"2. Limitations","text":"<ul> <li>The inputs to a real neuron aren't necessarily summed linearly: there may be non-linear summations.</li> <li>The real neurons do not output a single response, instead a train of spikes like a sequence of pulses is produced which encodes information.</li> <li>The neurons do not update themselves sequentially according to a computer clock but do it asynchronously.</li> <li>The weights in our model can be positive and negative, implying the presence of excitation or inhibitance property, which is also possesed by the real neurons but unlike real neurons our model change change from exciting to inhibitory after weight updates while real neurons stay the way they are(exciting or inhibitory).</li> </ul>","tags":["neural","networks","perceptron","deep","learning","feedforward","networks"]},{"location":"neural_networks/multiayer-perceptron/perceptron/#the-perceptron","title":"The Perceptron","text":"","tags":["neural","networks","perceptron","deep","learning","feedforward","networks"]},{"location":"neural_networks/multiayer-perceptron/perceptron/#1-introduction_1","title":"1. Introduction","text":"<p>The perceptron is nothing more than a collection of McCulloch and Pitts neurons together with a set of inputs and some weights to fasten the inputs to the neurons. The neurons in the Perceptron are completely independent of each other.</p> <p> Figure 1: The Perceptron Network(the orange nodes are inputs, not neurons)</p> <p>Each neuron has its own weights which it multiplies with its input and adds them to decide whether to fire or not depending on its own threshold. The inputs are the number of features(usually columns) our data has. The number of neurons can be varied and is usually the total unique classifying target values.</p> <p>We represent a particular weight as \\(w_{ij}\\) where \\(i\\) is the input it is coming from and \\(j\\) is the neuron it is going into. So \\(w_{32}\\) is the weight that connects the input node 3 to neuron 2. In the implementation, we will save the weights in a two dimensional array.</p> <p>The input will be stored as a vector and so the output.</p>","tags":["neural","networks","perceptron","deep","learning","feedforward","networks"]},{"location":"neural_networks/multiayer-perceptron/perceptron/#2-implementation","title":"2. Implementation","text":"","tags":["neural","networks","perceptron","deep","learning","feedforward","networks"]},{"location":"neural_networks/multiayer-perceptron/perceptron/#21-introduction","title":"2.1 Introduction","text":"<p>In supervised learning, we already have a ground truth target.</p> <ol> <li>For an input vector, we apply Equation 1 for each neuron to decide if each neuron will fire or not, generating its the output vector. </li> <li>For an output vector(a vector of 0s and 1s which determine if the corresponding neurons have fired or not!), we compare it to the target vector(the actual value for that input) to identify which neurons got the answer right and which did not.</li> <li>Those neurons with correct outputs are fine but those with wrong outputs(i.e they fire when they didn't have to or didn't when they had to), their weights need to be changed so that they fire correctly.</li> </ol>","tags":["neural","networks","perceptron","deep","learning","feedforward","networks"]},{"location":"neural_networks/multiayer-perceptron/perceptron/#22-learning-the-weights","title":"2.2 Learning the Weights","text":"","tags":["neural","networks","perceptron","deep","learning","feedforward","networks"]},{"location":"neural_networks/multiayer-perceptron/perceptron/#221-introduction","title":"2.2.1 Introduction","text":"<p>We'll talk more about it later, but for now let's deploy a simple learning system.</p> <p>\\(m:\\) no. of input features.</p> <p>\\(n:\\) no. of output neurons.</p> <p>\\(x_i,  i \\in [0,m]:\\) the input vector of m features.</p> <p>\\(y_i,   i \\in [0,n]:\\) the output vector of n neurons.</p> <p>\\(t_i,  i \\in [0,n]:\\) the actual target vector.</p> <p>Suppose \\(k^{th}\\) neuron gets the wrong answer, it has \\(m\\) weights connected to it(one for each input node). The weights we need to change is \\(w_{ik}\\) where \\(i\\) runs from \\(1\\) to \\(m\\).</p> <p>Now we know which weights to change, but by how much to change them by?</p> <p>Before answering that, let's figure out if a weight is too high or low(i.e do we need to increase it or decrease it?).</p> <p>At the first glance, we can say that bigger weights tend a neuron to fire(as they help it get over the threshold) and smaller weights tend to not fire. So if a neuron fires when it wasn't supposed to, there are some weights which are bigger than they should be, or if doesn't fire when it should(some weights are too small!).</p> <p>For that neuron we can calculate: \\(\\(y_k-t_k \\tag{3}\\)\\) i.e the difference between the output of the nepuron and the actual truth.</p> <p>If the above equation is Zero, then neuron has the correct output. However if it is positive(specifically \\(1\\)), then the neuron has fired when it shouldn't have, i.e weights are big and if it is negative(specifically \\(-1\\)), then the neuron hasn't fired when it should have, i.e weights are small. The above equation can act as a possible error function.</p> <p>So until now we know:</p> <p>the weights \\(w_{ij}\\) will be changed by \\( \\Delta w_{ij}\\)</p> <p>where,</p> \\[\\Delta w_{ij}  = \\begin{cases} &gt; 0 &amp; \\text{if } y_j - t_j &lt;0\\\\ &lt; 0 &amp; \\text{if } y_j - t_j &gt;0\\\\ = 0 &amp; \\text{if } y_j - t_j =0\\\\ \\end{cases}\\\\ \\implies \\Delta w_{ij} = - (y_j-t_j)k \\] <p>where \\(k\\) is the constant which gives the amount by which each weight needs to be changed.</p> <p>While this all seems right, we have missed something. What if the inputs are negative? </p> <p>If the inputs are negative, then switch values, we'll need to reduce the weights to fire and increase to not fire.</p> <p>To get around that we make a change:</p> \\[ \\Delta w_{ij}=-k(y_j - t_j)x_i\\] <p>if \\(x_i\\) is negative it will automatically change the direction of weight change and \\(k\\) decides how much the weight changes by. The parameter \\(k\\) is called the learning rate and is often represented by \\(\\eta\\) instead of k.</p> <p>so </p> \\[\\Delta w_{ij}= - \\eta(y_j-t_j) x_i\\] <p>Finally we update the weights,</p> \\[w_{ij} \\leftarrow w_{ij}-\\eta(y_j-t_j) x_i \\tag{4}\\] <p>We can update these for an optimum learning rate for some predefined \\(T\\) iterations. However we will later see other stopping methods.</p>","tags":["neural","networks","perceptron","deep","learning","feedforward","networks"]},{"location":"neural_networks/multiayer-perceptron/perceptron/#222-learning-rate","title":"2.2.2 Learning Rate","text":"<p>The learning rate is an important parameter which needs to be tuned to get better accuracies. Too high value of it might change the weights more than they were needed to and a too low will take too long to train. It is often used in the range of \\( 10^{-4} &lt; \\eta &lt; 10 \\). But feel free to check out of these bounds. Also for perceptron this parameter is way less important and can be set to anything. However for other models it is the most crucial parameter.</p>","tags":["neural","networks","perceptron","deep","learning","feedforward","networks"]},{"location":"neural_networks/multiayer-perceptron/perceptron/#223-bias-node","title":"2.2.3 Bias node","text":"<p>Now that we have figured out the weights, but we haven't discussed another important parameter, the threshold. What threshold to choose for what problem? That is where a Bias node comes into play.</p> <p>We can show Equation 1 for a certain neuron as:</p> \\[ h = w_{1j} x_1 + w_{2j}  x_2 + w_{3j}  x_3 + \\cdots +w_{mj}  x_m  \\] <p>where \\(x_k\\) is \\(k^{th}\\) input feature and \\(w_{kj}\\) is the weight from \\(k^{th}\\) input node to \\(j^{th}\\) neuron.</p> <p>Now, if the threshold is \\(\\theta\\), Equation 2 shows that:</p> \\[ \\begin{align} \\sigma &amp;= \\begin{cases}1 &amp; \\text{if }w_{1j} x_1 + w_{2j} x_2 + w_{3j} x_3 + \\cdots +w_{mj} x_m &gt; \\theta_j \\\\ 0 &amp; \\text{if }w_{1j}  x_1 + w_{2j}  x_2 + w_{3j} x_3 + \\cdots +w_{mj} x_m &lt; \\theta_j \\end{cases}\\\\ \\\\ &amp;= \\begin{cases} 1 &amp; \\text{if }w_{1j} x_1 + w_{2j} x_2 + w_{3j} x_3 + \\cdots +w_{mj} x_m - \\theta_j &gt;0 \\\\ 0 &amp; \\text{if }w_{1j}  x_1 + w_{2j}  x_2 + w_{3j} x_3 + \\cdots +w_{mj} x_m - \\theta_j&lt;0 \\end{cases}\\\\ \\\\ &amp;=\\begin{cases} 1 &amp; \\text{if }w_{1j} x_1 + w_{2j} x_2 + w_{3j} x_3 + \\cdots +w_{mj} x_m + (-1) \\theta_j &gt;0 \\\\ 0 &amp; \\text{if }w_{1j}  x_1 + w_{2j}  x_2 + w_{3j} x_3 + \\cdots +w_{mj} x_m + (-1) \\theta_j&lt;0 \\end{cases} \\end{align} \\] <p>so \\(\\theta\\) can be learned as another weight, if we consider an extra input feature which is always \\(-1\\) and our new threshold is \\(0\\).(However \\(\\theta_j\\) is the actual threshold which instead of defining, we let the neuron to learn like any other weight.)</p> <p>The Bias Node also helps us overcome the all-zero input problem. If all the inputs in an example are Zero then no matter how the weights change, it won't change the output, but the bias node will change and make changes necessary for correct output. </p> <p>So to make it work we will also need an extra but constant input node reserved for -1.</p> <p>Note : Actually, it is not necessary to use -1 as the bias input. Any constant will do. People mostly use +1 but we will use -1 to make it stand out.</p> <p>The Bias Node is considered the \\(x_0\\) which is constant(-1) and the weight to it is \\(w_{0j}\\)(which is actually \\(\\theta_j\\)).</p> <p>So the new Structure is like: </p> <p>Figure 2: Perceptron with Bias Node</p>","tags":["neural","networks","perceptron","deep","learning","feedforward","networks"]},{"location":"neural_networks/multiayer-perceptron/perceptron/#23-putting-everything-together","title":"2.3 Putting Everything Together","text":"<p>Now that we have all the things cleared out, it is time to put everything together.</p> <p>The algorithm is separated into two parts: a training phase and a recall phase. The recall is essentially used after training is finished to actually use the model.</p> <p>The Algorithm is as:</p>","tags":["neural","networks","perceptron","deep","learning","feedforward","networks"]},{"location":"neural_networks/multiayer-perceptron/perceptron/#1-initialization","title":"1. Initialization:","text":"<ul> <li>set all the weights \\(w_{ij}\\) to small random numbers(both positive and negative).</li> </ul>","tags":["neural","networks","perceptron","deep","learning","feedforward","networks"]},{"location":"neural_networks/multiayer-perceptron/perceptron/#2-training","title":"2. Training:","text":"<ul> <li>for \\(T\\) iterations or untill all outputs are correct:<ul> <li> <p>for each input vector:</p> <ol> <li>compute the activation of each neuron \\(j\\) using:</li> </ol> <p>\\(\\(y=g \\bigg(\\sum_{i=0}^mw_{ij}x_i\\bigg)=\\begin{cases}1 &amp; \\text{if } \\sum_{i=0}^mw_{ij}x_i &gt; 0 \\\\ 0 &amp; \\text{if } \\sum_{i=0}^mw_{ij}x_i \\leq 0 \\end{cases}\\)\\)</p> <p>where \\(x_0\\) is -1 (the bias node) and \\(m\\) is the number of features our data has.</p> <ol> <li>update each of the weights individually using:     \\(\\(w_{ij} \\leftarrow w_{ij} - \\eta(y_j-t_j)\\cdot x_i\\)\\)</li> </ol> </li> </ul> </li> </ul>","tags":["neural","networks","perceptron","deep","learning","feedforward","networks"]},{"location":"neural_networks/multiayer-perceptron/perceptron/#3-recall","title":"3. Recall:","text":"<ul> <li>compute the activation of each neuron \\(j\\) using:</li> </ul> <p>\\(\\(y=g \\Bigg(\\sum_{i=0}^mw_{ij}x_i\\Bigg)=\\begin{cases}1 &amp; \\text{if } \\sum_{i=0}^mw_{ij}x_i &gt; 0 \\\\ 0 &amp; \\text{if } \\sum_{i=0}^mw_{ij}x_i \\leq 0 \\end{cases}\\)\\)</p>","tags":["neural","networks","perceptron","deep","learning","feedforward","networks"]},{"location":"neural_networks/multiayer-perceptron/perceptron/#24-speeding-up-the-code","title":"2.4 Speeding Up the code","text":"","tags":["neural","networks","perceptron","deep","learning","feedforward","networks"]},{"location":"neural_networks/multiayer-perceptron/perceptron/#241-speeding-up-the-computation-of-activations","title":"2.4.1 Speeding Up the computation of activations","text":"<p>The code for it has multiple loops for training. Loops for multiplying the weights with inputs and loops while updating weights. The simple loops can take a lot of time, but many languages, like python, have libraries(numpy, tensorflow, pytorch) to perform matrix perations much quicker than simple loops. We will put these to our use to speed up the training as well as the recall process as well.</p> <p>Let's say we have \\(k\\) number of training examples and each example has \\(m\\) features with \\(n\\) types of outputs.</p> <p>Now instead of taking one example at a time and updating weights and doing the same for \\(T\\) iterations, we can store all our training examples in a matrix where each example is a row and each column is a feature.</p> <p>So the 4th feature of 6th training example will look like \\(x_{64}\\).</p> <p>So our input matrix \\(X\\) should look like:</p> \\[X= \\begin{bmatrix} x_{11} &amp; x_{12} &amp; x_{13} &amp; \\cdots &amp; x_{1m} \\\\ x_{21} &amp; x_{22} &amp; x_{23} &amp; \\cdots &amp; x_{2m} \\\\ x_{31} &amp; x_{32} &amp; x_{33} &amp; \\cdots &amp; x_{3m} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ x_{k1} &amp; x_{k2} &amp; x_{k3} &amp; \\cdots &amp; x_{km} \\\\ \\end{bmatrix} \\tag{5} \\] <p>This matrix will be \\(k \\times m\\).</p> <p>In python we would do this using a library called <code>numpy</code></p> <pre><code>import numpy as np\n</code></pre> <p>let's say we want to train to learn the Logical-OR function. So our input should look like:</p> \\[X= \\begin{bmatrix} 0 &amp; 0 \\\\ 0 &amp; 1 \\\\ 1 &amp; 0 \\\\ 1 &amp; 1 \\\\ \\end{bmatrix} \\] <p>we can make it make it using <code>np.array</code> method:</p> <pre><code>X = np.array([[0,0],\n             [0,1],\n             [1,0],\n             [1,1]])\nX\n</code></pre> <pre><code>array([[0, 0],\n       [0, 1],\n       [1, 0],\n       [1, 1]])\n</code></pre> <p>now our input is ready, let's figure out how to store the target values. Each input has target values for which neuron to fire and which not to(1s and 0s).</p> <p>So with \\(k\\) examples and \\(n\\) output neurons, the target matrix should be \\([t_{ij}]\\) which is the target for \\(i^{th}\\) example and \\(j^{th}\\) neuron.</p> <p>So, $$ T=\\begin{bmatrix} t_{11} &amp; t_{12} &amp; t_{13} &amp; \\cdots &amp; t_{1n}\\ t_{21} &amp; t_{22} &amp; t_{23} &amp; \\cdots &amp; t_{2n}\\ t_{31} &amp; t_{32} &amp; t_{33} &amp; \\cdots &amp; t_{3n}\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; &amp; \\vdots \\ t_{k1} &amp; t_{k2} &amp; t_{k3} &amp; \\cdots &amp; t_{kn}\\ \\end{bmatrix}\\ \\tag{6}$$ where \\(t_{ij} \\in {0,1}\\) </p> <p>For binary outputs, like in our example, we can just use one output neuron, which will fire for one output and not fire for other which means \\(n=1\\).</p> <p>So, </p> \\[ T_{binary} = \\begin{bmatrix} t_1\\\\ t_2\\\\ \\vdots \\\\ t_k \\end{bmatrix} \\] <p>and in our example,</p> \\[ T = \\begin{bmatrix} 0\\\\ 1\\\\ 1\\\\ 1 \\end{bmatrix} \\] <p>we can do it in numpy in the same way:</p> <pre><code>T = np.array([[0],[1],[1],[1]]);T\n</code></pre> <pre><code>array([[0],\n       [1],\n       [1],\n       [1]])\n</code></pre> <p>Moving on to the outputs generated by our neurons. For \\(n\\) neurons and \\(k\\) examples, we can store that in a matrix \\([y_{ij}]\\) for \\(i^{th}\\) example and \\(j^{th}\\) neuron like:</p> <p>\\(\\(Y= \\begin{bmatrix} y_{11} &amp; y_{12} &amp; y_{13} &amp; \\cdots &amp; y_{1n}\\\\ y_{21} &amp; y_{22} &amp; y_{23} &amp; \\cdots &amp; y_{2n}\\\\ y_{31} &amp; y_{32} &amp; y_{33} &amp; \\cdots &amp; y_{3n}\\\\ \\vdots \\\\ y_{k1} &amp; y_{k2} &amp; y_{k3} &amp; \\cdots &amp; y_{kn}\\\\ \\end{bmatrix}\\\\\\tag{7}\\)\\) where \\(y_{ij} \\in {0,1}\\)</p> <p>So, $$ y_{ij} = g(h_{ij})=\\begin{cases} 1 &amp; \\text{if } x_{i1}w_{1j}+x_{i2}w_{2j}+\\cdots + x_{im}w_{mj} + (-1)w_{0j} &gt; 0 \\ 0 &amp; \\text{if } x_{i1}w_{1j}+x_{i2}w_{2j}+\\cdots + x_{im}w_{mj} + (-1)w_{0j} \\leq 0 \\ \\end{cases} \\tag{8} $$</p> <p>if we replace all of it in the matrix,</p> <p>$$Y=g\\Bigg( \\begin{bmatrix} h_{11} &amp; h_{12} &amp; h_{13} &amp; \\cdots &amp; h_{1n}\\ h_{21} &amp; h_{22} &amp; h_{23} &amp; \\cdots &amp; h_{2n}\\ h_{31} &amp; h_{32} &amp; h_{33} &amp; \\cdots &amp; h_{3n}\\ \\vdots \\ h_{k1} &amp; h_{k2} &amp; h_{k3} &amp; \\cdots &amp; h_{kn}\\ \\end{bmatrix}\\Bigg) \\tag{9} $$ where \\(h_{ij} =(-1)w_{0j}+\\sum_{a=1}^{m}x_{ia}w_{aj}\\)</p> <p>or if we keep \\(\\(x_{j0} = -1  \\  \\forall j \\in [1,k] \\tag{10}\\\\\\)\\)</p> \\[ \\implies Y=g\\Bigg( \\begin{bmatrix} \\sum_{a=0}^{m}x_{1a}w_{a1} &amp; \\sum_{a=0}^{m}x_{1a}w_{a2} &amp; \\cdots &amp; \\sum_{a=0}^{m}x_{1a}w_{an}\\\\ \\\\ \\sum_{a=0}^{m}x_{2a}w_{a1} &amp; \\sum_{a=0}^{m}x_{2a}w_{a2} &amp; \\cdots &amp; \\sum_{a=0}^{m}x_{2a}w_{an}\\\\ \\\\ \\sum_{a=0}^{m}x_{3a}w_{a1} &amp; \\sum_{a=0}^{m}x_{3a}w_{a2} &amp; \\cdots &amp; \\sum_{a=0}^{m}x_{3a}w_{an}\\\\ \\vdots &amp; \\vdots &amp;   &amp; \\vdots\\\\ \\sum_{a=0}^{m}x_{ka}w_{a1} &amp; \\sum_{a=0}^{m}x_{ka}w_{a2}  &amp; \\cdots &amp; \\sum_{a=0}^{m}x_{ka}w_{an}\\\\ \\end{bmatrix}\\Bigg) \\tag{11} \\] <p>The above matrix looks like a multiplication of two matrices. Let's open it up:</p> \\[Y=g\\Bigg( \\begin{bmatrix} x_{10} &amp; x_{11} &amp;  \\cdots &amp; x_{1m}\\\\ x_{20} &amp; x_{21}  &amp; \\cdots &amp; x_{2m}\\\\ x_{30} &amp; x_{31} &amp; \\cdots &amp; x_{3m}\\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ x_{k0} &amp; x_{k1} &amp; \\cdots &amp; x_{km}\\\\ \\end{bmatrix}\\times \\begin{bmatrix} w_{01} &amp; w_{02}  &amp; \\cdots &amp; w_{0n}\\\\ w_{11} &amp; w_{12} &amp;  \\cdots &amp; w_{1n}\\\\ w_{21} &amp; w_{22}  &amp; \\cdots &amp; w_{2n}\\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ w_{m1} &amp; w_{m2}  &amp; \\cdots &amp; w_{mn}\\\\ \\end{bmatrix} \\Bigg) \\tag{12} \\] <p>if we look at the Left Matrix it is the input matrix (\\(X\\)) with extra column on far left and we know the far left column is always -1, so we can actually redefine the input matrix to include this extra column.</p> \\[\\implies X=\\begin{bmatrix} x_{10} &amp; x_{11} &amp; x_{12} &amp; \\cdots &amp; x_{1m}\\\\ x_{20} &amp; x_{21} &amp; x_{22} &amp; \\cdots &amp; x_{2m}\\\\ x_{30} &amp; x_{31} &amp; x_{32} &amp; \\cdots &amp; x_{3m}\\\\ \\vdots \\\\ x_{k0} &amp; x_{k1} &amp; x_{k2} &amp; \\cdots &amp; x_{km}\\\\ \\end{bmatrix} \\tag{13}\\] <p>This matrix is now \\(k \\times (m+1)\\).</p> <p>we can generate a column of -1 using the <code>np.ones</code> method and then concatenate it with our input matrix using <code>np.concatenate</code> to form the new input matrix, like:</p> <pre><code>X = np.concatenate((-np.ones((X.shape[0],1)),X),axis=1)\nX\n</code></pre> <pre><code>array([[-1.,  0.,  0.],\n       [-1.,  0.,  1.],\n       [-1.,  1.,  0.],\n       [-1.,  1.,  1.]])\n</code></pre> <p>Now that we have fixed the input matrix, let's move to the second matrix in Equation 12, the weight matrix.</p> <p>$$ W = \\begin{bmatrix} w_{01} &amp; w_{02} &amp; w_{03} &amp; \\cdots &amp; w_{0n}\\ w_{11} &amp; w_{12} &amp; x_{13} &amp; \\cdots &amp; w_{1n}\\ w_{21} &amp; w_{22} &amp; x_{23} &amp; \\cdots &amp; w_{2n}\\ \\vdots \\ w_{m1} &amp; w_{m2} &amp; x_{m3} &amp; \\cdots &amp; w_{mn}\\ \\end{bmatrix}\\ \\tag{14}$$ where \\(w_{ij}\\) is the weight from \\(i^{th}\\) input node to \\(j^{th}\\) output neuron</p> <p>This matrix is \\( (m+1) \\times n\\).</p> <p>with Equation 13 can be rewriten as:</p> <p>$$ Y=g(X \\times W)\\ \\tag{15} $$ where: 1. \\(X\\) is the input matrix(Equation 13 with bias nodes. 2. \\(W\\) is the weight matrix (Equation 14). 3. \\(\\times\\) represents matrix multiplication. 4. and the function \\(g\\)(Equation 8) is applied elementwise to the resultant matrix to generate outputs for every neuron for every training example.</p> <p>We will use the <code>np.matmul</code> function to perform a matrix multiplication and we will use the numpy boolean mask broadcasting to compute activations. Since we have to repeat this step, we will make a function for this named <code>compute_activations</code>. so <code>Y=compute_activations(X,W)</code> whenever needed.</p> <pre><code>def compute_activations(X,W):\n    activation = np.matmul(X,W) &gt; 0\n    return activation\n</code></pre> <p>This completes the first part of the Training Algorithm (i.e compute activation of each neuron for each input example). Now let's move to the Second Part of the Training, Updating Weights.</p>","tags":["neural","networks","perceptron","deep","learning","feedforward","networks"]},{"location":"neural_networks/multiayer-perceptron/perceptron/#242-speeding-up-the-the-updation-of-weights","title":"2.4.2 Speeding up the the updation of weights","text":"<p>The updation of weights is given by Equation 4, which is:</p> <p>\\(\\(w_{ij}\\leftarrow w_{ij} - \\eta(y_j-t_j)\\cdot x_i\\tag{4}\\)\\) where \\(w_{ij}\\) is the weight from \\(i^{th}\\) input node to \\(j^{th}\\) output neuron. </p> <p>We can vectorize this operation, instead of updating every weight using a loop, we update the whole weight matrix at once. </p> <p>$$ W \\leftarrow W -\\eta \\Delta W \\tag{16} $$ where \\(W\\) is the weight matrix and \\( \\Delta W \\) is the matrix having the corresponding \\((y_j - t_j) \\cdot x_i\\) for each weight.</p> <p>Now before we move on figure out how \\(\\Delta W\\) should be computed, let's put a detour to see how the weight changes for different examples.</p> <p>Let's say we had three examples. Each weight will be updated three times with their corresponding \\(x_i\\).</p> <p>Let's denote \\(x_{ij}\\) as the \\(j^{th}\\) input feature of \\(i^{th}\\) example and \\(t_{ia}\\) as the target for \\(i^{th}\\) example and \\(a^{th}\\) output neuron. So, \\(1\\leq i \\leq 3\\).</p> <p>Now after each example, weights will change, like:</p> \\[ w_{ja} \\leftarrow w_{ja} - \\eta(y_{1a} - t_{1a})  x_{1j} \\tag{for ex. 1}\\\\ \\] \\[ w_{ja} \\leftarrow w_{ja} - \\eta(y_{2a} - t_{2a})  x_{2j} \\tag{for ex. 2}\\\\ \\] \\[ w_{ja} \\leftarrow w_{ja} - \\eta(y_{3a} - t_{3a})  x_{3j} \\tag{for ex. 3} \\] <p>each of the above change will occur to each weight one after the another. so these can be summed as:</p> \\[ w_{ja} \\leftarrow w_{ja} - \\eta \\{(y_{1a} - t_{1a})  x_{1j} + (y_{2a} - t_{2a})  x_{2j} + (y_{3a} - t_{3a})  x_{3j}\\} \\tag{17} \\] <p>It can be generalized as, for \\(k\\) examples:</p> \\[ w_{ja} \\leftarrow w_{ja}- \\eta \\Big(\\sum_{i=1}^k(y_{ia} - t_{ia}) x_{ij}\\Big) \\tag{18} \\] <p>So \\(\\Delta W\\) is just a matrix of the \\(\\sum_{i=1}^k(y_{ia} - t_{ia}) x_{ij}\\) for every neuron and for every example.</p> <p>So, for \\(k\\) examples with \\(m+1\\) input features (the first being \\(x_{i0} = -1\\)) and \\(n\\) output neurons</p> \\[ \\Delta W = \\begin{bmatrix} \\sum_{i=1}^k  x_{i0}  (y_{i1} - t_{i1})  &amp; \\sum_{i=1}^k  x_{i0}  (y_{i2} - t_{i2})  &amp; \\cdots &amp; \\sum_{i=1}^k  x_{i0}  (y_{in} - t_{in}) \\\\ \\sum_{i=1}^k  x_{i1}  (y_{i1} - t_{i1})  &amp; \\sum_{i=1}^k  x_{i1}  (y_{i2} - t_{i2})  &amp; \\cdots &amp; \\sum_{i=1}^k  x_{i1}  (y_{in} - t_{in}) \\\\ \\vdots\\\\ \\sum_{i=1}^k  x_{im}  (y_{i1} - t_{i1})  &amp; \\sum_{i=1}^k  x_{im}  (y_{i2} - t_{i2})  &amp; \\cdots &amp; \\sum_{i=1}^k  x_{im} (y_{in} - t_{in}) \\\\ \\end{bmatrix} \\tag{19} \\] <p>This matrix is \\((m+1) \\times n\\) same as the weights matrix.</p> <p>Take some time to write it down and look at every entry to make it clear for yourself.</p> <p>Let's unpack this bad boy! So it also looks like a matrix multiplication of two matrices.</p> \\[ \\Delta W = \\begin{bmatrix} x_{10} &amp; x_{20} &amp; \\cdots &amp; x_{k0}\\\\ x_{11} &amp; x_{21} &amp; \\cdots &amp; x_{k1}\\\\ \\vdots \\\\ x_{1m} &amp; x_{2m} &amp; \\cdots &amp; x_{km}\\\\ \\end{bmatrix} \\times  \\begin{bmatrix} y_{11} - t_{11} &amp; y_{12} - t_{12} &amp; \\cdots &amp; y_{1n} - t_{1n} \\\\ y_{21} - t_{21} &amp; y_{22} - t_{22} &amp; \\cdots &amp; y_{2n} - t_{2n} \\\\ \\vdots \\\\ y_{k1} - t_{k1} &amp; y_{k2} - t_{k2} &amp; \\cdots &amp; y_{kn} - t_{kn} \\\\ \\end{bmatrix} \\] <p>The right matrix is basically the subtraction of target matrix \\(T\\) ([Equation 6]) subtracted from the output matrix \\(Y\\) ([Equation 7]).</p> \\[ \\implies \\Delta W =  \\begin{bmatrix} x_{10} &amp; x_{20} &amp; \\cdots &amp; x_{k0}\\\\ x_{11} &amp; x_{21} &amp; \\cdots &amp; x_{k1}\\\\ \\vdots \\\\ x_{1m} &amp; x_{2m} &amp; \\cdots &amp; x_{km}\\\\ \\end{bmatrix} \\times \\Bigg( \\begin{bmatrix} y_{11} &amp; y_{12} &amp; \\cdots &amp; y_{1n} \\\\ y_{21} &amp; y_{22} &amp; \\cdots &amp; y_{2n} \\\\ \\vdots \\\\ y_{k1} &amp; y_{k2} &amp; \\cdots &amp; y_{kn} \\\\ \\end{bmatrix} -  \\begin{bmatrix} t_{11} &amp; t_{12} &amp; \\cdots &amp; t_{1n} \\\\ t_{21} &amp; t_{22} &amp; \\cdots &amp; t_{2n} \\\\ \\vdots \\\\ t_{k1} &amp; y_{k2} &amp; \\cdots &amp; t_{kn} \\\\ \\end{bmatrix} \\Bigg) \\] <p>The left matrix is the transpose of the input matrix with bias values \\(X\\) (Equation 13).</p> \\[ \\implies \\Delta W =  \\begin{bmatrix} x_{10} &amp; x_{11} &amp; \\cdots &amp; x_{1m}\\\\ x_{20} &amp; x_{21} &amp; \\cdots &amp; x_{2m}\\\\ \\vdots \\\\ x_{k0} &amp; x_{k1} &amp; \\cdots &amp; x_{km}\\\\ \\end{bmatrix}^T \\times \\Bigg( \\begin{bmatrix} y_{11} &amp; y_{12} &amp; \\cdots &amp; y_{1n} \\\\ y_{21} &amp; y_{22} &amp; \\cdots &amp; y_{2n} \\\\ \\vdots \\\\ y_{k1} &amp; y_{k2} &amp; \\cdots &amp; y_{kn} \\\\ \\end{bmatrix} -  \\begin{bmatrix} t_{11} &amp; t_{12} &amp; \\cdots &amp; t_{1n} \\\\ t_{21} &amp; t_{22} &amp; \\cdots &amp; t_{2n} \\\\ \\vdots \\\\ t_{k1} &amp; y_{k2} &amp; \\cdots &amp; t_{kn} \\\\ \\end{bmatrix} \\Bigg) \\tag{20} \\] <p>Finally,</p> \\[ \\Delta W = X^T \\times (Y-T) \\tag{21} \\] <p>Using the above equation, Equation 16 becomes:</p> \\[ W \\leftarrow W - \\eta \\{X^T \\times (Y-T)\\}\\\\ \\tag{22} \\] <p>where: - \\(X^T\\): is the transpose of input matrix. - \\(Y\\): is the output matrix. - \\(T\\):  is the target matrix. - \\( \\eta \\):  is the learning rate. - \\(\\times\\):  represents matrix multiplication.</p> <p>now to compute the transpose of a matrix, we use <code>np.transpose</code> function. we can use the simple minus operator to perform subtraction in matrices. Also to multiply each element by \\(\\eta\\), we use the broadcasting property. Since this step is also going to be used multiple times, we will turn it into a function, like:</p> <pre><code>def update_weights(weights, input_matrix, output_matrix, target_matrix, learning_rate):\n    delta_w = np.matmul(np.transpose(input_matrix),(output_matrix-target_matrix))\n    weights = weights - (learning_rate*delta_w) # elementwise multiplication using broadcasting\n    return weights\n</code></pre> <p>but before updating the weight matrix, we need to initialize a weight matrix with small random numbers. we can use the <code>np.random.rand</code> to generate random numbers between 0 and 1, then multiply by 0.1 (to make them small) and subtract 0.05 to get some negative weights.</p> <pre><code>def initialize_weights(n_input,n_out):\n    # the input shape should be including the bias inputs\n    weights = np.random.rand(n_input, n_out)*0.1 - 0.05\n    return weights\n</code></pre>","tags":["neural","networks","perceptron","deep","learning","feedforward","networks"]},{"location":"neural_networks/multiayer-perceptron/perceptron/#25-final-code","title":"2.5 Final Code","text":"<p>now that we have finished all the functions for training as well as initialization, for recall, we can again use the <code>compute_activations</code> function.</p> <p>Let's put the code together and run for an example. while training, we can print weights and output after each iteration.</p> <pre><code>def compute_activations(X,W):\n    activation = (np.matmul(X,W) &gt; 0)\n    return activation\n\ndef initialize_weights(n_input,n_out, random_state):\n    np.random.seed(random_state)\n    # the input shape should be including the bias inputs\n    weights = np.random.rand(n_input, n_out)*0.1 - 0.05\n    return weights\n\ndef update_weights(weights, input_matrix, output_matrix, target_matrix, learning_rate):\n    delta_w = np.matmul(np.transpose(input_matrix),(output_matrix-target_matrix))\n    weights = weights - (learning_rate*delta_w) # elementwise multiplication using broadcasting\n    return weights\n\ndef train(input_data, target, learning_rate, epochs,random_state=0,init_weights=None, save_weights=False, verbose=False):\n    # add the bias values to input_matrix\n    X = np.concatenate((-np.ones((input_data.shape[0],1)),input_data),axis=1)\n    #set the shapes\n    n_input = X.shape[1]\n    n_out = target.shape[1]\n\n    #initialize the weights\n    if init_weights is None:\n        W = initialize_weights(n_input,n_out, random_state)\n    else:\n        W = init_weights\n\n    if save_weights:\n        weight_array=[W]\n\n    for it in range(epochs):\n        # compute outputs\n        Y = compute_activations(X,W)\n\n        if verbose:\n            #print the output\n            print(f\"Iteration: {it}\\n{W}\\nOutput:\\n{Y[:10,:10]}\\nAccuracy: {(Y==target).sum()/X.shape[0]}\")\n\n        # update weights\n        W = update_weights(W, X, Y, target, learning_rate)\n\n        if save_weights:\n            weight_array.append(W)\n    if save_weights:\n        return W, weight_array\n    else:\n        return W\n\ndef recall(input_data, weights):\n    # add the bias values to input_matrix\n    X = np.concatenate((-np.ones((input_data.shape[0],1)),input_data),axis=1)\n    # compute activations\n    Y = compute_activations(X,weights)\n    return Y\n</code></pre> <p>This is all we need for a perceptron code. After a good time, you can feel the simplicity and elegancy of it.</p> <p>Let's try it for an OR data. Let's see if it can learn the parameters.</p> <pre><code> prepare the data\nX = np.array([[0,0],\n             [0,1],\n             [1,0],\n             [1,1]])\n\nT = np.array([[0],[1],[1],[1]])\n\ntrain the data\nweights = train(input_data=X,target=T,learning_rate=0.25,epochs=6, random_state=42,verbose=True)\n</code></pre> <pre><code>Iteration: 0\n[[-0.01254599]\n [ 0.04507143]\n [ 0.02319939]]\nOutput:\n[[ True]\n [ True]\n [ True]\n [ True]]\nAccuracy: 0.75\nIteration: 1\n[[0.23745401]\n [0.04507143]\n [0.02319939]]\nOutput:\n[[False]\n [False]\n [False]\n [False]]\nAccuracy: 0.25\nIteration: 2\n[[-0.51254599]\n [ 0.54507143]\n [ 0.52319939]]\nOutput:\n[[ True]\n [ True]\n [ True]\n [ True]]\nAccuracy: 0.75\nIteration: 3\n[[-0.26254599]\n [ 0.54507143]\n [ 0.52319939]]\nOutput:\n[[ True]\n [ True]\n [ True]\n [ True]]\nAccuracy: 0.75\nIteration: 4\n[[-0.01254599]\n [ 0.54507143]\n [ 0.52319939]]\nOutput:\n[[ True]\n [ True]\n [ True]\n [ True]]\nAccuracy: 0.75\nIteration: 5\n[[0.23745401]\n [0.54507143]\n [0.52319939]]\nOutput:\n[[False]\n [ True]\n [ True]\n [ True]]\nAccuracy: 1.0\n</code></pre> <p>and with a learning rate of 0.25, it can learn the parameters for OR gate in just 6 iterations.</p>","tags":["neural","networks","perceptron","deep","learning","feedforward","networks"]},{"location":"neural_networks/multiayer-perceptron/perceptron/#3-visualization","title":"3. Visualization","text":"<p>Now that we have completely studied the basic perceptron. Let's visualize some things to get the better understanding.</p> <p>we will use the <code>plotly</code>'s <code>express</code> and <code>graph_objects</code> for plotting.</p> <p>Let's first plot our input data</p> <pre><code>import plotly.express as px\nimport plotly.graph_objects as go\n</code></pre> <pre><code>fig = px.scatter(x=X[:,0], y=X[:,1], color=T.astype(\"str\"), size=[8]*X.shape[0],\n                 labels={\"x\":\"Input 1\",\"y\":\"Input 2\",\"color\":\"Output\"},\n                 title=\"Scatter of Data Points\", height=600, width=600, opacity=1)\nfig.show()\n</code></pre> <p>{% include perceptron/plot_1.html %}</p> <p>Now let's see what the weights represent.</p> <p>We know that \\(w_{ij}\\) is the weight from \\(i^{th}\\) input node to \\(j^{th}\\) output node.</p> <p>Since we have only one output neuron and 2 input nodes(3 with bias node).</p> <p>Our Weights compute, for each example: \\(w_{01}(-1) + w_{11}\\cdot inp1+w_{12}\\cdot inp2\\) and then check if it is greater than or less than Zero.</p> <p>For the boundary case, </p> \\[ w_{01}(-1) + w_{11}\\cdot inp1+w_{12}\\cdot inp2=0 \\] <p>if we were to plot this threshold line with \\(inp2\\) as the y-axis and \\(inp1\\) on the x-axis, then:</p> <p>$$ inp2=\\frac{-w_{11}inp1+w_{01}}{w_{12}} $$ So,in the previous example, our final weights were:</p> <pre><code>weights\n</code></pre> <pre><code>array([[0.23745401],\n       [0.54507143],\n       [0.52319939]])\n</code></pre> <pre><code>inp1 = np.linspace(0,0.5,100)\n\nfig  = go.Figure(layout=dict(height=600,width=600,\n                            xaxis_title=\"Input 1\", yaxis_title=\"Input 2\",\n                            title=\"Decision Boundary with Scatter\"))\nfig.add_trace(go.Scatter(x=X[:,0:1][T==1],y=X[:,1:][T==1],name= \"Output: 1\",mode=\"markers\",marker=dict(size=20)))\nfig.add_trace(go.Scatter(x=X[:,0:1][T==0],y=X[:,1:][T==0],name= \"Output: 1\",mode=\"markers\",marker=dict(size=20)))\nfig.add_trace(go.Scatter(x=inp1,y=(-(weights[1,0]*inp1) + weights[0,0])/weights[2,0],name= \"Decision Boundary\",mode=\"lines\",marker=dict(size=20)))\n\nfig.show()\n</code></pre> <p>{% include perceptron/plot_2.html %}</p> <p>a more sophisticated  decision boundary</p> <pre><code>xx,yy=np.meshgrid(np.arange(X[:,0].min()-0.1,X[:,0].max()+0.1,(X[:,0].max()-X[:,0].min())/500),\n                      np.arange(X[:,1].min()-0.1,X[:,1].max()+0.1,(X[:,1].max()-X[:,1].min())/500))\nZ = recall(np.c_[xx.ravel(),yy.ravel()],weights)\nZ = Z.reshape(xx.shape)*1\n\n\nfig = go.Figure(layout=dict(width=600,height=600))\n\nfig.add_trace(\n    go.Heatmap(\n        x=xx[0],\n        y=yy[:,1],\n        z=Z,\n        colorscale=\"Viridis\",\n        showscale=False\n))\nfig.add_trace(\n    go.Scatter(\n        x=X[:,0],y=X[:,1],mode=\"markers\",\n        marker=dict(\n            size=20,\n            color=T[:,0],\n            colorscale=\"Viridis\",\n            line=dict(color=\"black\",width=2))\n    )\n)\nfig.show()\n</code></pre> <p>{% include perceptron/plot_3.html %}</p> <p>all the points above this line will cause the fire of the current neuron, and all the below won't.</p> <p>Let's see the updation of weights and it's impact on the decision boundary.</p> <p>we will train the model while saving the weights to plot it. We will set the learning rate to be small so that the animation is smooth and we get a good idea of what is happening. You can change it. I have also increased the number of epochs as the learning rate is small now. Also play with different <code>random_state</code> to start from initial different weights.</p> <pre><code>W, weight_array = train(input_data=X,target=T,learning_rate=0.001,epochs=100,save_weights=True,random_state=364)\n</code></pre> <p>after having all the weights saved, we will plot them one after the other using the <code>animation.FuncAnimation</code> function. Change the <code>interval</code> if the animation is too slow or too fast.</p> <p><pre><code>xx,yy=np.meshgrid(np.arange(X[:,0].min()-0.1,X[:,0].max()+0.1,(X[:,0].max()-X[:,0].min())/200),\n                      np.arange(X[:,1].min()-0.1,X[:,1].max()+0.1,(X[:,1].max()-X[:,1].min())/200))\n\nZ = [recall(np.c_[xx.ravel(),yy.ravel()],weights).reshape(xx.shape)*1 for weights in weight_array]\n\nnb_frames = 98\n\nfig = go.Figure(frames=[\n    go.Frame(\n        data=[\n            go.Heatmap(\n            x=xx[0],\n            y=yy[:,1],\n            z=Z[k],\n            colorscale=\"Viridis\",\n            showscale=False\n        ),\n           ],\n        name=str(k)\n        )\n    for k in range(nb_frames)])\n\nfig.add_trace(go.Scatter(\n            x=X[:,0],y=X[:,1],mode=\"markers\",\n            marker=dict(\n                size=20,\n                color=T[:,0],\n                colorscale=\"Viridis\",\n                line=dict(color=\"black\",width=2))\n        ))\n\nfig.add_trace(go.Scatter(\n            x=X[:,0],y=X[:,1],mode=\"markers\",\n            marker=dict(\n                size=20,\n                color=T[:,0],\n                colorscale=\"Viridis\",\n                line=dict(color=\"black\",width=2))\n        ))\n\nfig.add_trace(go.Scatter(\n            x=X[:,0],y=X[:,1],mode=\"markers\",\n            marker=dict(\n                size=20,\n                color=T[:,0],\n                colorscale=\"Viridis\",\n                line=dict(color=\"black\",width=2))\n        ))\n\n\n\ndef frame_args(duration):\n    return {\n            \"frame\": {\"duration\": duration},\n            \"mode\": \"immediate\",\n            \"fromcurrent\": True,\n            \"transition\": {\"duration\": duration, \"easing\": \"linear\"},\n        }\n\nsliders = [\n            {\n                \"pad\": {\"b\": 10, \"t\": 60},\n                \"len\": 0.9,\n                \"x\": 0.1,\n                \"y\": 0,\n                \"steps\": [\n                    {\n                        \"args\": [[f.name], frame_args(0)],\n                        \"label\": str(k),\n                        \"method\": \"animate\",\n                    }\n                    for k, f in enumerate(fig.frames)\n                ],\n            }\n        ]\n\n# Layout\nfig.update_layout(\n         title='Change in Decision Boundary with Weight Update',\n         width=600,\n         height=600,\n         scene=dict(\n                    zaxis=dict(range=[-0.1, 6.8], autorange=False),\n                    aspectratio=dict(x=1, y=1, z=1),\n                    ),\n         updatemenus = [\n            {\n                \"buttons\": [\n                    {\n                        \"args\": [None, frame_args(50)],\n                        \"label\": \"&amp;#9654;\", # play symbol\n                        \"method\": \"animate\",\n                    },\n                    {\n                        \"args\": [[None], frame_args(0)],\n                        \"label\": \"&amp;#9724;\", # pause symbol\n                        \"method\": \"animate\",\n                    },\n                ],\n                \"direction\": \"left\",\n                \"pad\": {\"r\": 10, \"t\": 70},\n                \"type\": \"buttons\",\n                \"x\": 0.1,\n                \"y\": 0,\n            }\n         ],\n         sliders=sliders,\n    showlegend=False\n)\n\nfig.show()\n</code></pre> {% include perceptron/plot_4.html %}</p> <p>As you can see, the decision boundary was initially in the right spot but with reversed thresholds, and with changing epochs it flipped.</p>","tags":["neural","networks","perceptron","deep","learning","feedforward","networks"]},{"location":"neural_networks/multiayer-perceptron/perceptron/#4-linear-separability","title":"4. Linear Separability","text":"","tags":["neural","networks","perceptron","deep","learning","feedforward","networks"]},{"location":"neural_networks/multiayer-perceptron/perceptron/#41-introduction","title":"4.1 Introduction","text":"<p>As we saw just now, in the visualization section, a Perceptron tries to draw a line between the two classes of data. That is for 2D (or two input features). For 3D (three input features, it will draw a plane to separate out the two classes.</p> <p>Now for multiclass output, there will be multiple output neuron and each will have a decision boundary that will separate out the different classes(see figure):</p> <p></p> <p>Figure 3: Multiclass Classification by a perceptron.</p> <p>Now it is evident that the data should be linearly separable among each class for a perceptron to work properly.</p>","tags":["neural","networks","perceptron","deep","learning","feedforward","networks"]},{"location":"neural_networks/multiayer-perceptron/perceptron/#42-the-perceptron-convergence-theorem","title":"4.2 The Perceptron Convergence Theorem","text":"<p>The Perceptron Convergence theorem proved by Rosenblatt in 1962, states that:</p> <p>\"given a linearly separable data, the perceptron will converge to a solution within \\(1/ \\gamma^2\\) iterations, where \\(\\gamma\\) is the distance between the separating hyperplane and the closest datapoint to it.\"</p> <p>However there are some assumptions about it: 1. The data should be linearly separable. 2. For every input vector \\(x\\), \\( \\mid  \\mid x \\mid  \\mid \\) is bound by some constant R. In our proof we will assume \\( \\mid  \\mid x \\mid  \\mid  \\leq 1\\). 3. Also the learning rate is chosen to be 1.</p> <p>The point being, if the data is linearly separable, irrespective of the constant \\( \\mid  \\mid x \\mid  \\mid \\) is bound by or the value of learning rate, the perceptron will converge to the solution in finite iterations(i.e, it will find the solution).</p> <p>You can see the proof of this theorem here</p>","tags":["neural","networks","perceptron","deep","learning","feedforward","networks"]},{"location":"neural_networks/multiayer-perceptron/perceptron/#43-linear-inseparability-examplexor-logic","title":"4.3 Linear Inseparability Example(XOR Logic)","text":"<p>Let's try to learn the XOR Logic. Let's prepare the dataset.</p> <pre><code>X = np.array([[0,0],\n             [0,1],\n             [1,0],\n             [1,1]])\n\nT = np.array([[0],[1],[1],[0]])\n</code></pre> <p>Let's first visualize the data points ourselves.</p> <pre><code>fig = go.Figure(layout=dict(width=600,height=600)) \nfig.add_trace(go.Scatter(x=X[:,0],y=X[:,1], mode=\"markers\", marker=dict(size=20,color=T.squeeze())))\nfig.show()\n</code></pre> <p>{% include perceptron/plot_5.html %}</p> <p>There is no line we can draw to separate the two classes. Let's see how the perceptron behaves.</p> <pre><code>W, weight_array = train(input_data=X,target=T,learning_rate=0.001,epochs=50,save_weights=True,random_state=364)\n</code></pre> <pre><code>xx,yy=np.meshgrid(np.arange(X[:,0].min()-0.1,X[:,0].max()+0.1,(X[:,0].max()-X[:,0].min())/200),\n                      np.arange(X[:,1].min()-0.1,X[:,1].max()+0.1,(X[:,1].max()-X[:,1].min())/200))\n\nZ = [recall(np.c_[xx.ravel(),yy.ravel()],weights).reshape(xx.shape)*1 for weights in weight_array]\n\nnb_frames = 60\n\nfig = go.Figure(frames=[\n    go.Frame(\n        data=[\n            go.Heatmap(\n            x=xx[0],\n            y=yy[:,1],\n            z=Z[k],\n            colorscale=\"Viridis\",\n            showscale=False\n        ),\n             ],\n        name=str(k) \n        )\n    for k in range(nb_frames)])\n\nfig.add_trace(go.Scatter(\n            x=X[:,0],y=X[:,1],mode=\"markers\",\n            marker=dict(\n                size=20,\n                color=T[:,0],\n                colorscale=\"Viridis\",\n                line=dict(color=\"black\",width=2))\n        ))\n\nfig.add_trace(go.Scatter(\n            x=X[:,0],y=X[:,1],mode=\"markers\",\n            marker=dict(\n                size=20,\n                color=T[:,0],\n                colorscale=\"Viridis\",\n                line=dict(color=\"black\",width=2))\n        ))\n\nfig.add_trace(go.Scatter(\n            x=X[:,0],y=X[:,1],mode=\"markers\",\n            marker=dict(\n                size=20,\n                color=T[:,0],\n                colorscale=\"Viridis\",\n                line=dict(color=\"black\",width=2))\n        ))\n\n\n\ndef frame_args(duration):\n    return {\n            \"frame\": {\"duration\": duration},\n            \"mode\": \"immediate\",\n            \"fromcurrent\": True,\n            \"transition\": {\"duration\": duration, \"easing\": \"linear\"},\n        }\n\nsliders = [\n            {\n                \"pad\": {\"b\": 10, \"t\": 60},\n                \"len\": 0.9,\n                \"x\": 0.1,\n                \"y\": 0,\n                \"steps\": [\n                    {\n                        \"args\": [[f.name], frame_args(0)],\n                        \"label\": str(k),\n                        \"method\": \"animate\",\n                    }\n                    for k, f in enumerate(fig.frames)\n                ],\n            }\n        ]\n\n# Layout\nfig.update_layout(\n         title='Change in Decision Boundary with Weight Update',\n         width=600,\n         height=600,\n         scene=dict(\n                    zaxis=dict(range=[-0.1, 6.8], autorange=False),\n                    aspectratio=dict(x=1, y=1, z=1),\n                    ),\n         updatemenus = [\n            {\n                \"buttons\": [\n                    {\n                        \"args\": [None, frame_args(50)],\n                        \"label\": \"&amp;#9654;\", # play symbol\n                        \"method\": \"animate\",\n                    },\n                    {\n                        \"args\": [[None], frame_args(0)],\n                        \"label\": \"&amp;#9724;\", # pause symbol\n                        \"method\": \"animate\",\n                    },\n                ],\n                \"direction\": \"left\",\n                \"pad\": {\"r\": 10, \"t\": 70},\n                \"type\": \"buttons\",\n                \"x\": 0.1,\n                \"y\": 0,\n            }\n         ],\n         sliders=sliders,\n    showlegend=False\n)\n\nfig.show()\n</code></pre> <p>{% include perceptron/plot_6.html %}</p> <p>as you can see, the perceptron fails to find a linear boundary and dangles between the two sides. So even a simple function like XOR cannot be learned by the perceptron. This discovery halted the neural network development for at least 20 years. There is an obvious solution though(Multiple layers, but we'll get to that at its own time!).</p>","tags":["neural","networks","perceptron","deep","learning","feedforward","networks"]},{"location":"neural_networks/multiayer-perceptron/perceptron/#44-higher-dimensions-is-the-answer","title":"4.4 Higher dimensions is the answer?","text":"<p>Since we cannot draw a line in the XOR dataset to separate it, how about taking the data to a higher dimension(the 3rd dimension in our case)? We can find a plane to separate the two classes in 3D. We will add a dimension in such a way that it does not change the data when looked in the \\((x,y) \\) plane, but moves point \\((0,0) \\) along the third dimension.</p> <p>The truth table can be like:</p> Input 1 Input 2 New dimension 0 0 1 0 1 0 1 0 0 1 1 0 <pre><code>X = np.array([[0,0,1],\n             [0,1,0],\n             [1,0,0],\n             [1,1,0]])\nT = np.array([[0],[1],[1],[0]])\n</code></pre> <pre><code>fig = go.Figure()\nfig.add_trace(\n    go.Scatter3d(x=X[:,0],y=X[:,1],z=X[:,2],mode=\"markers\",marker=dict(size=10,color=T.squeeze(),colorscale=\"Viridis\"))\n)\n\nfig.update_layout(\n    title=\"Plot Title\",\n\n    xaxis=go.layout.XAxis(\n        title=go.layout.xaxis.Title(\n            text='x Axis',\n            font=dict(\n                family='Courier New, monospace',\n                size=18,\n                color='#7f7f7f'\n            )\n        )\n    ),\n    yaxis_title=\"y Axis Title\",\n    font=dict(\n        family=\"Courier New, monospace\",\n        size=10,\n        color=\"#7f7f7f\"\n    )\n)\n</code></pre> <p>{% include perceptron/plot_7.html %}</p> <p>we can now easily see a plane separating the two kinds, just by lifting one point into the third dimension. Also if you rotate the figure to see the input2 and input1 as the y-axis and x-axis, you can see the exact same plot as before. </p> <p>Now let's train a model and check the accuracy.</p> <pre><code>W = train(X,T,learning_rate=0.5,epochs=20,random_state=364,verbose=True)\n</code></pre> <pre><code>Iteration: 0\nOutput:\n[[ True]\n[False]\n[False]\n[False]]\nAccuracy: 0.25\nIteration: 1\nOutput:\n[[ True]\n[ True]\n[ True]\n[ True]]\nAccuracy: 0.5\nIteration: 2\nOutput:\n[[False]\n[False]\n[False]\n[False]]\nAccuracy: 0.5\nIteration: 3\nOutput:\n[[False]\n[ True]\n[ True]\n[ True]]\nAccuracy: 0.75\nIteration: 4\nOutput:\n[[False]\n[False]\n[False]\n[False]]\nAccuracy: 0.5\nIteration: 5\nOutput:\n[[ True]\n[ True]\n[ True]\n[ True]]\nAccuracy: 0.5\nIteration: 6\nOutput:\n[[False]\n[False]\n[False]\n[False]]\nAccuracy: 0.5\nIteration: 7\nOutput:\n[[False]\n[ True]\n[ True]\n[ True]]\nAccuracy: 0.75\nIteration: 8\nOutput:\n[[False]\n[ True]\n[ True]\n[ True]]\nAccuracy: 0.75\nIteration: 9\nOutput:\n[[False]\n[False]\n[False]\n[False]]\nAccuracy: 0.5\nIteration: 10\nOutput:\n[[False]\n[ True]\n[ True]\n[ True]]\nAccuracy: 0.75\nIteration: 11\nOutput:\n[[False]\n[False]\n[False]\n[False]]\nAccuracy: 0.5\nIteration: 12\nOutput:\n[[ True]\n[ True]\n[ True]\n[ True]]\nAccuracy: 0.5\nIteration: 13\nOutput:\n[[False]\n[False]\n[False]\n[False]]\nAccuracy: 0.5\nIteration: 14\nOutput:\n[[False]\n[ True]\n[ True]\n[ True]]\nAccuracy: 0.75\nIteration: 15\nOutput:\n[[False]\n[ True]\n[ True]\n[False]]\nAccuracy: 1.0\nIteration: 16\nOutput:\n[[False]\n[ True]\n[ True]\n[False]]\nAccuracy: 1.0\nIteration: 17\nOutput:\n[[False]\n[ True]\n[ True]\n[False]]\nAccuracy: 1.0\nIteration: 18\nOutput:\n[[False]\n[ True]\n[ True]\n[False]]\nAccuracy: 1.0\nIteration: 19\nOutput:\n[[False]\n[ True]\n[ True]\n[False]]\nAccuracy: 1.0\n</code></pre> <p>let's plot the decision boundary for this one.</p> <pre><code>weights=W\nxx,yy=np.meshgrid(np.arange(X[:,0].min()-0.1,X[:,0].max()+0.1,(X[:,0].max()-X[:,0].min())/10),\n                  np.arange(X[:,1].min()-0.1,X[:,1].max()+0.1,(X[:,1].max()-X[:,1].min())/10))\nZ = (1*weights[0,0] - xx.ravel()*weights[1,0] - yy.ravel()*weights[2,0])/weights[3,0]\nZ = Z.reshape(xx.shape)\n\nfig = go.Figure()\nfig.add_trace(\n    go.Surface(x=xx[0],y=yy[:,0],z=Z,showscale=False,colorscale=\"Viridis\",opacity=0.9))\n\nfig.add_trace(\n    go.Scatter3d(x=X[:,0],y=X[:,1],z=X[:,2],mode=\"markers\",marker=dict(size=6,color=T.squeeze(),colorscale=\"Viridis\"))\n)\nfig.show()\n</code></pre> <p>{% include perceptron/plot_8.html %}</p> <p>as you can see, by increasing a dimension, we were able to classify what seemed to be impossible before. Infact, it is always possible to classify two classes with a linear function, provided we project the data into correct set of dimensions. It will be explored in kernel classifiers, which are the basis of Support Vector Machines.</p> <p>For now, we have learned how to get around the linear barrier of the perceptrons(by getting into higher dimensions and multi-layer, which will be discussed later). </p> <p>Some Notes for Data Preparation: 1. It is better to normalize the inputs as well as the target values(especiallly for regression). 2. Normalize both train and test inputs with the same mean and variance. 3. Perform a basic form feature selection by trying out the classifier by missing out different feature columns one at a time as see if it can increase the accuracy. If a missing out feature does improve the results, then leave it out completely and try missing out others as well.</p> <p>The above method is actually a simplistic way of testing for the correlation between output and each of its features.</p> <ol> <li>We can also consider dimensionality reduction, more on it later.</li> </ol>","tags":["neural","networks","perceptron","deep","learning","feedforward","networks"]},{"location":"neural_networks/transformer/transformer/","title":"Transformer","text":""},{"location":"neural_networks/transformer/transformer/#attention-is-all-you-need","title":"Attention Is All You Need","text":"<pre><code>import torch\n</code></pre> <p>This paper introduced the first sequence to sequence architecture using only attention mechanism, called \"The Transformer\". </p> <p>To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence aligned RNNs or convolution.</p>"},{"location":"neural_networks/transformer/transformer/#what-is-attention","title":"What is Attention?","text":"<p>Attention was first proposed in Neural Machine Translation by Jointly Learning to Align and Translate. It was introduced as part of recurrent networks especially in an encoder-decoder situation.</p>"},{"location":"neural_networks/transformer/transformer/#before-attention","title":"Before Attention","text":"<p>Let's take the example of Neural Machine Translation, a common way to achieve it was by a recurrent encoder which encodes the text in one language into a context vector and that context vector was passed onto the decoder to autoregresively decode the text in target language.</p> <p>As per Bhadanau et al.2015,</p> <p>A potential issue with this encoder\u2013decoder approach is that a neural network needs to be able to compress all the necessary information of a source sentence into a fixed-length vector. This may make it difficult for the neural network to cope with long sentences, especially those that are longer than the sentences in the training corpus.</p> <p>The problem of compression loses information which may be vital at some timestep in decoder. A new method was introduced which instead of looking at this context vector, a soft-search system would determine which timesteps in the encoder are most relevant, i.e while decoding current timestep which timestep encodings should it attend most to.</p> <p>Let's start formalizing it a bit,</p> <p>An encoder reads the input sentence, a sequence of vectors \\(\\mathbf{x} = \\begin{pmatrix}x_1 &amp; \\cdots &amp; x_{T_x}\\end{pmatrix}\\), into a vector \\(c\\). The most common approach is to use an RNN such that</p> \\[ \\begin{align*} h_t &amp;= f(x_t, h_{t-1})\\\\ c &amp;= q(\\{h_1, h_2, \\dots, h_{T_x}\\}) \\end{align*} \\] <p>where \\(h_t\\) is a hidden state at timestep \\(t\\) and \\(c\\) is the context vector passed onto the decoder. \\(f\\) and \\(q\\) are some non-linear functions. \\(f\\) is usually a LSTM/GRU and \\(q(\\{h_1, h_2, \\dots, h_{T_x}\\}) = h_{T_x}\\), i.e the last hidden state is chosen as the context vector.</p> <p>The decoder is trained to predict the next timestep \\(y_{t'}\\) given the previous outputs \\(\\{y_1, \\dots, y_{t'-1}\\}\\) and the context vector \\(c\\).</p> <p>In other words, the decoder defines a probability over the translation \\(\\mathbf{y}\\) by decomposing the joint probability into the ordered conditionals:</p> \\[p(\\mathbf{y}) = \\prod_{t=1}^T p(y_t \\mid \\{y_1, \\dots, y_{t-1}\\}, c)\\] <p>where \\(\\mathbf{y} = \\begin{pmatrix}y_1 &amp; y_2 &amp; \\cdots &amp; y_{T_y}\\end{pmatrix}\\).</p> <p>With a Recurrent Network, it can be modelled as:</p> \\[p(y_t \\mid \\{y_1, \\dots, y_{t-1}\\}, c) = g(y_{t-1}, s_t, c)\\] <p>where \\(g\\) is a non-linear funtion, \\(s_t\\) is the hidden state of RNN.</p>"},{"location":"neural_networks/transformer/transformer/#introduction","title":"Introduction","text":"<p>As we can see at the decoding of each time step the same context vector is being passed, which is a compressed vector of all hidden states in encoder. At each timestep the decoder's dependency on encoder output can vary. The major idea by Bhadanau et al.2015 was to introduce a dynamic context vector which changes for each timestep depending on what context is deemed necessary by the decoder at that moment. It should be able to pull from the raw hidden states.</p> <p>The probability was redefined to depend on not a context vector but on the input space for the decoder, so</p> \\[p(y_i \\mid \\{y_1, \\dots, y_{i-1}\\}, \\mathbf{x}) = g(y_{i-1}, s_i, c_i)\\] <p>where \\(s_i\\) is the hidden state of decoder. As we can see \\(c\\) has been changed to \\(c_i\\). \\(c\\) originally was a summarization of all hidden states of encoder i.e: \\(c = q(\\{h_1, h_2, \\dots, h_{T_x}\\})\\).</p> <p>Now \\(c_i\\) is the weighted sum of all hidden states of encoder where the weights themselves are dynamic.</p> \\[c_i = \\sum_{j=1}^{T_x} \\alpha_{ij} h_j\\] <p>The weight \\(\\alpha_{ij}\\) is a softmaxed version of underlying scores(because weights should sum to 1).</p> \\[\\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{k=1}^{T_x} \\exp(e_{ik})}\\] <p>where \\(e_{ij}\\), called an alignment model, is how well the inputs around position \\(j\\) and the outcome at position \\(i\\) match. The score is based on the previous decoder hidden state \\(s_{i-1}\\) and \\(j\\) th hidden state of encoder.</p> \\[e_{ij} = a(s_{i-1}, h_j)\\] <p>So while decoding we first compute all scores w.r.t to all encoder hidden states, softmax them and take the weighted average of encoder hidden states to form out context vector at that timestep.</p> <p>Now depending on how \\(a\\) is defined we can have multiple types of attentions. One simple way is to concat(or add) inputs and pass it through a feedforward network which can be trained jointly with the whole system, which is what bhadanau et al.2015 used.</p> <p>By Bhadanau et at.2015:</p> <p>The probability \\(\\alpha_{ij}\\) , or its associated energy \\(e_{ij}\\) , reflects the importance of the annotation \\(h_j\\) with respect to the previous hidden state \\(s_{i\u22121}\\) in deciding the next state \\(s_i\\) and generating \\(y_i\\). Intuitively, this implements a mechanism of attention in the decoder. The decoder decides parts of the source sentence to pay attention to. By letting the decoder have an attention mechanism, we relieve the encoder from the burden of having to encode all information in the source sentence into a fixedlength vector. With this new approach the information can be spread throughout the sequence of annotations, which can be selectively retrieved by the decoder accordingly.</p>"},{"location":"neural_networks/transformer/transformer/#scaled-dot-product-attention","title":"Scaled Dot Product Attention","text":"<p>The attention mechanism introduced by Bhadanau et al.2015 is called additive or concat attention. Another form of attention is dot product attention where instead of using a feedforward network to compute an energy, a dot product is used. Dot product is also an indicator of how close two vectors can be. </p> <p>The dot product attention is defined as:</p> \\[a(s_{i-1}, h_j) = s_{i-1}^\\intercal h_j\\] <p>Vaswani et. al2017, introduced the scaled dot product attention, which is defined as</p> \\[a(s_{i-1}, h_j) = \\frac{s_{i-1}^\\intercal h_j}{\\sqrt{n}}\\] <p>where \\(n\\) is the dimension of \\(h_j\\).</p> <p>The idea behind this scaling was that for very large \\(n\\) dimenional vectors, the dot-product can be huge pushing the softmax into regions with very small gradients. So, the scaling is required.</p> <p><code>Question: Explain how gradients will be small?</code></p> <p>From Vaswani et .al2017,</p> <p>The two most commonly used attention functions are additive attention, and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of \\(\\frac{1}{\\sqrt{d_k}}\\). Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.</p> <p><code>Question: how is it faster?</code></p>"},{"location":"neural_networks/transformer/transformer/#model-architecture","title":"Model Architecture","text":"<p>Now, with the attention mechanism introduced, the major novelty of this paper was the removal of recurrent networks. The transformer network only relied on attention mechanism to learn, arguing attention is all you need.</p> <p>It is still an encoder-decoder system with encoder producing some features at each timestep, which are later used by decoder to autoregressively decode. But how are encodings generated of a sequence, where each timestep is not independent. Afterall, this is what recurrent networks were adding, the sense of \"time\" and capturing the dependency of different timesteps. </p> <p>Answer? Attention. Or more specifically, self-attention. Before we start going further, let's try to rephrase attention to not depend on \"hidden states\" or any recurrent networks.</p>"},{"location":"neural_networks/transformer/transformer/#keys-queries-and-values","title":"Keys, Queries and Values","text":"<p>We can see attention needs not be on hidden states of encoder and decoder. Any sequence of \"annotations\" can be attented by a vector. The annotations happened to be hidden states of encoder and the vector was the hidden state of decoder at that timestep.</p> <p>Vaswani et. al2017 generalized it to a broader context of any sequences. Let's see how.</p> <p>We want to compute the attention score of each decoder timestep. At each time step we want to find the weights relating to current timestep. It can be seen as doing a soft-search our current decoder hidden state in all the encoder hidden states, to remove the concept of \"hidden states\", we call our current decoder hidden state as a \"query\" to look up in a database of \"keys\", our encoder hidden states.</p> <p>A single query \\(q_j\\), of dimension \\((1, d_k)\\) and a matrix of all keys \\(K\\), of dimension, \\((T_k, d_k)\\) we can compute (scaled dot-product)attention scores vector,\\(E_j\\) as:</p> \\[E_j = \\frac{q_jK^\\intercal}{\\sqrt{d_k}}\\] <p>which is a \\((1,T_k)\\) dimensional vector containing scores for each timestep which need to be softmaxed to create attention weights, \\(\\mathbf{\\alpha}_j\\)</p> <p>If we somehow have all the queries, \\(Q\\) in a \\((T_q, d_k)\\) matrix, we can compute all the attention weights for all timesteps as:</p> \\[\\alpha = \\text{softmax}\\left(\\frac{QK^\\intercal}{\\sqrt{d_k}}\\right)\\] <p>where \\(\\alpha\\) is a \\((T_q, T_k)\\) matrix representing all weights for all Queries.</p> <p>Now, once we have the attention weights, we have to do a weighted sum on the encoder hidden states again for each query. We can clearly see that can be as:</p> \\[\\begin{align*} \\text{Attention} &amp;= \\alpha K\\\\                                   &amp;=\\text{softmax}(\\frac{QK^\\intercal}{\\sqrt{d_k}})K \\end{align*}\\] <p>Here, we sum over our keys again, but that may not be the case. To generalize it further the sequence to sum it over, we call them \"values\" represented by \\(V\\) of shape  \\((T_k, d_v)\\). So using this new notation of keys queries and values, we can define (scaled dot-product) Attention as:</p> \\[\\text{Attention}(Q,K,V) = \\text{softmax}(\\frac{QK^\\intercal}{\\sqrt{d_k}})V\\] <p>This is basically a database where we wanna fetch some values which are indexed by some keys and we have some queries which we will apply on keys and then fetch the corresponding values of these keys, but in a \"soft\" way.</p> <p>As we can see, the output shape of Attention is \\((T_q, d_v)\\) that is we generated a new sequence.</p> <p>So given a sequence of embeddings we can create a new sequence of same shape by using them as keys, queries and values. This is how we can replace the concurrent part and replace it by self-attention. Each timestep in output looks at each timestep in input to create a new input and attention weights provide the dependency across timesteps.</p> <p>Note: In attention, optionally, one might want to apply a mask to determine which keys should be available for attention for a certain query. This masking is done before softmax. It is saved in a matrix \\(M\\) same shape as \\((T_q, T_k)\\) but with zeros at places we want to be part of attention and negative infinty at places we don't want to attend to and this matrix is added to scaled dot product output.</p> \\[\\text{Attention}(Q,K,V) = \\text{softmax}(\\frac{QK^\\intercal}{\\sqrt{d_k}}+M)V\\] <p></p> <pre><code>def scaled_dot_product_attn(Q, K, V, M=None):\n    d_k = K.shape[-1]\n    ## all inputs are batched\n    K_T = K.transpose(1, 2)\n    dot_product = torch.bmm(Q, K_T)\n    scaled_dot_product = dot_product/d_k\n    if M is not None:\n        scaled_dot_product+=M\n    attn_wts = torch.nn.functional.softmax(scaled_dot_product, dim=-1)\n    output = torch.bmm(attn_wts, V)\n    return output\n</code></pre> <pre><code>d_k = 2\nd_q = d_k\nd_v = 3\nT_k = 2\nT_q = 4\nT_v = T_k\nbatch = 2\nK = torch.rand(batch, T_k, d_k)\nQ = torch.rand(batch, T_q, d_q)\nV = torch.rand(batch, T_v, d_v)\n\noutput = scaled_dot_product_attn(Q, K, V)\nprint(output.shape)\nmask = (torch.rand(batch, T_q, T_k)&lt;0.2)*(-1e11)\nmasked_output = scaled_dot_product_attn(Q, K, V, mask)\nprint(masked_output.shape)\n</code></pre> <pre><code>torch.Size([2, 4, 3])\ntorch.Size([2, 4, 3])\n</code></pre>"},{"location":"neural_networks/transformer/transformer/#multi-head-attention","title":"Multi Head Attention","text":"<p>This was another addition by the paper. Instead of computing attention only once, we linearly project queries, keys and values \\(h\\) different times and perform \\(h\\) different attentions paralelly which are later concatenated.</p> <p>According to Vaswani et al.2017,</p> <p>Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.</p> <p>It is illustrated in image below:</p> <p></p> <p>So,</p> \\[\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1,\\dots, \\text{head}_h)W^O\\] <p>where</p> \\[\\text{head}_i = \\text{Attention}(QW^Q_i, KW^K_i, VW^V_i)\\] <p>where the projections are parameter matrices \\(W^Q_i \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}\\) ,  \\(W^K_i \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}\\), \\(W^V_i \\in \\mathbb{R}^{d_{\\text{model}} \\times d_v}\\) and \\(W^O \\in \\mathbb{R}^{hd_v \\times d_{\\text{model}}}\\).</p> <p>It is common to have \\(d_k = d_v = \\frac{d_{\\text{model}}}{h}\\). </p> <pre><code>from torch import nn\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, num_heads, d_model):\n        super(MultiHeadAttention, self).__init__()\n        # instead of having h layers from d_model to d_k, \n        # we can have one layer from d_model to h*d_k\n        # h*d_k = d_model\n        assert d_model%num_heads == 0, \"model dim should be divisible by num_heads\"\n        self.num_heads = num_heads\n        self.d_model = d_model\n        self.k_layer = nn.Linear(d_model, d_model)\n        self.q_layer = nn.Linear(d_model, d_model)\n        self.v_layer = nn.Linear(d_model, d_model)\n        self.o_layer = nn.Linear(d_model, d_model)\n\n\n    def forward(self, q, k, v, mask=None):\n\n        #q: batch, T_q, d_model\n        #q_proj: batch, T_q, d_model\n        bsize = q.shape[0]\n\n        q_proj = self.q_layer(q)\n\n        k_proj = self.k_layer(k)\n        v_proj = self.v_layer(v)\n\n        # q_proj_heads: batch,T_q, n_heads, d_q\n        q_proj_heads = q_proj.view(bsize, -1,  self.num_heads, \n                                    int(self.d_model/self.num_heads))\n        k_proj_heads = k_proj.view(bsize, -1,  self.num_heads, \n                                    int(self.d_model/self.num_heads))\n        v_proj_heads = v_proj.view(bsize, -1,  self.num_heads, \n                                    int(self.d_model/self.num_heads))\n\n        # put all heads in batch dim since scaled_dot_product_attn \n        # already handles batch\n\n        # q_proj_batched: batch*n_heads, T_q, dq\n        q_proj_batched = q_proj_heads.transpose(1,2).reshape(bsize*self.num_heads, \n                                -1, int(self.d_model/self.num_heads) )\n        k_proj_batched = k_proj_heads.transpose(1,2).reshape(bsize*self.num_heads, \n                                -1, int(self.d_model/self.num_heads) )\n        v_proj_batched = v_proj_heads.transpose(1,2).reshape(bsize*self.num_heads, \n                                -1, int(self.d_model/self.num_heads) )\n\n        ##attn_out: batch*n_heads, T_q, dq\n        attn_out = scaled_dot_product_attn(q_proj_batched, k_proj_batched, \n                                            v_proj_batched, mask)\n        ## batch, n_heads, Tq, dq\n        attn_out = attn_out.view(bsize, self.num_heads, \n                                -1, int(self.d_model/self.num_heads))\n        ##batch, Tq, n_heads, dq\n        attn_out = attn_out.transpose(1, 2)\n        ##batch, Tq, n_heads, h*dq(d_model)\n        concat_out = attn_out.reshape(bsize, -1, self.d_model)\n#         concat_out = torch.cat(attns_outs, dim=-1)\n        mha_out = self.o_layer(concat_out)\n        return mha_out          \n</code></pre> <pre><code>d_model = 64\nn_heads = 8\nbatch = 2\nT = 10\nq = torch.rand(batch, T, d_model)\nk = torch.rand(batch, T, d_model)\nv = torch.rand(batch, T, d_model)\n\nmha = MultiHeadAttention(n_heads, d_model)\nmha(q,k,v).shape\n</code></pre> <pre><code>torch.Size([2, 10, 64])\n</code></pre>"},{"location":"neural_networks/transformer/transformer/#the-transformer-encoder","title":"The Transformer Encoder","text":"<p>Now with the information we are ready to build the first component of the transformer. It is the replacement of the recurrent network in common sequence to sequence tasks. This encodes the input into a series of annotations or hidden states which are passed onto the decoder.</p> <p>An Encoder comprises of multiple encoder layers (similar to satcking of RNNs etc). Each encoding layer consists of two major components. First multi head self attention layer (i.e key, query, value are all same, the previous layer's output) and mask consists of all zeros. Every timestep in input is allowed to be attended. Second is a simple, positionwise fully connected feed-forward network.</p> <p>Residual Skip Connections are added around the two sublayers, followed by a Layer Normalization. Dropout is added to the output of sublayer before adding.  That is,  \\(\\text{LayerNorm}(x + \\text{Dropout}(\\text{Sublayer}(x)))\\), where sublayer can be an MHA or FeedForward Network.</p> <p></p> <p>These Encoder layers are stacked together to consume previous layers output to form an encoder. The original transformer Encoder has 6 such layers. </p> <p>The feed-forward network is usually a two layered network with a ReLU activation in between which is applied on each time-step independently. The input dimension and output dimension of feed-forward network would be \\(d_{\\text{model}}\\), but the intermediate layer can be different, this is called the feed-forward dim and denoted as \\(d_{ff}\\). </p> <pre><code>class GlobalSelfAttention(MultiHeadAttention):\n\n    def forward(self, x):\n        return super().forward(x, x, x, mask=None)\n\nclass PositionWiseFeedForward(nn.Module):\n    def __init__(self, d_ff, d_model):\n        super(PositionWiseFeedForward, self).__init__()\n        self.fc1 = nn.Linear(d_model, d_ff)\n        self.fc2 = nn.Linear(d_ff, d_model)\n        self.act = nn.ReLU()\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        return self.fc2(x)\n\nclass AddNorm(nn.Module):\n    def __init__(self, d_model, dropout=0.2):\n        super(AddNorm, self).__init__()\n        self.d_model = d_model\n        self.layernorm = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, sublayer):\n        return self.layernorm(x+self.dropout(sublayer))\n</code></pre> <p>With fundamental building blocks ready, let's write an Encoder Layer</p> <pre><code>class EncoderLayer(nn.Module):\n    def __init__(self, num_heads, d_model, d_ff, dropout=0.2):\n        super(EncoderLayer, self).__init__()\n        self.self_attn = GlobalSelfAttention(num_heads, d_model)\n        self.ff = PositionWiseFeedForward(d_ff, d_model)\n        self.addnorm1 = AddNorm(d_model, dropout)\n        self.addnorm2 = AddNorm(d_model, dropout)\n\n    def forward(self, x):\n        ## self globall attn\n        attn_out = self.self_attn(x)\n        x = self.addnorm1(x, attn_out)\n        ff_out = self.ff(x)\n        x = self.addnorm2(x, ff_out)\n        return x\n</code></pre> <pre><code>d_model = 64\nd_ff = 256\nbatch = 2\nT = 20\nn_heads = 8\n\ninput_seq = torch.rand(batch, T, d_model)\nenc = EncoderLayer(n_heads, d_model, d_ff)\nenc(input_seq).shape\n</code></pre> <pre><code>torch.Size([2, 20, 64])\n</code></pre>"},{"location":"neural_networks/transformer/transformer/#positional-encoding","title":"Positional Encoding","text":"<p>The Transformer encoder is a stack of these layers and some other parts. Let's look at the complete figure of encoder.</p> <p></p> <p>As we can see, it is comprised of \\(N\\) encoder layers and has inputs and something called a Positional Encoding System. What is that?</p> <p>Recall how we used self attention to replace the recurrence which helped in making decisions across multiple timesteps. However, the order of a sequence also matters, which recurrent networks also handled. </p> <p>In the current self attention mechanism, if we permute the input tokens randomly, the output would be same (in that permuted order), it does not take into account which token came after which one, it just utilizes the fact that these tokens have come together. </p> <p>To introduce a sense of time Positional Encodings are introduced.</p> <p>By Vaswani et al.2017:</p> <p>Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension \\(d_{\\text{model}}\\) as the embeddings, so that the two can be summed.</p> <p>One simple way is to learn the position encodings by the position of each token through an Embedding Layer and then add them to input embeddings. Other way proposed by this paper is using sine and cosine functions:</p> \\[ \\begin{align*} PE_{(pos, 2i)} &amp;= \\sin{(pos/10000^{2i/d_\\text{model}})}\\\\ PE_{(pos, 2i+1)} &amp;= \\cos{(pos/10000^{2i/d_\\text{model}})}\\\\ \\end{align*} \\] <p>where \\(pos\\) is the position and \\(i\\) is the dimension</p> <pre><code>def positional_encoding(T, d_model):\n    pe = torch.zeros(T, d_model)\n    pos = torch.arange(0, T, dtype=torch.float).unsqueeze(1)\n    div = torch.pow(10000, torch.arange(0, d_model, 2, dtype=torch.float)/d_model)\n    angle = pos/div\n    pe[:, ::2] = torch.sin(angle)\n    pe[:, 1::2] = torch.cos(angle)\n\n    return pe\n</code></pre> <pre><code>T = 60\nd_model = 128\npos_e = positional_encoding(T, d_model)\n\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(12,4))\nplt.pcolormesh(pos_e, cmap='RdBu')\nplt.ylabel('Position')\nplt.xlabel('Depth')\nplt.colorbar()\nplt.show()\n</code></pre> <p></p> <p>One natural question is why this? There are multiple reasons. One it is not learnable. Values are between -1 and 1. Unique for each position. And it can extrapolate to longer sequences than present in training.</p> <p>Also, mentioned in paper:</p> <p>We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset \\(k\\), \\(PE_{pos+k}\\) can be represented as a linear function of \\(PE_{pos}\\).</p> <p>What does it mean \\(PE_{pos+k}\\) to be a linear function of \\(PE_{pos}\\)? Well, it means there exists a matrix \\(M^{(k)}\\) independent of \\(pos\\) such that \\(M^{(k)}PE_{pos} = PE_{pos+k}\\).</p> <p>A general proof can be found here: https://blog.timodenk.com/linear-relationships-in-the-transformers-positional-encoding/</p> <p>But I will provide a small proof here as well.</p> <p>\\(PE_{pos}\\) can be seen as a vector of pairs of \\(\\sin\\) and \\(\\cos\\) functions, e.g for dimensions \\(d\\) and \\(pos=t\\),</p> \\[ PE_t =  \\begin{bmatrix} \\sin (t/10000^{(0/d)}) \\\\ \\cos (t/10000^{(0/d)}) \\\\ \\sin (t/10000^{(1/d)}) \\\\ \\cos (t/10000^{(1/d)}) \\\\ \\vdots \\end{bmatrix} =  \\begin{bmatrix} \\sin (\\omega_0 t) \\\\ \\cos (\\omega_0 t) \\\\ \\sin (\\omega_1 t) \\\\ \\cos (\\omega_1 t) \\\\ \\vdots \\end{bmatrix} \\] <p>where \\(\\omega_p =\\frac{1}{10000^{(p/d)}}\\) is the frequency.</p> <p>For every sine cosine pair of frequency \\(\\omega_p\\), there is a linear transformation \\(M \\in \\mathbb{R}^{2\\times2}\\), independent of \\(t\\), for which the following is true:</p> \\[M \\begin{bmatrix} \\sin (\\omega_p t) \\\\ \\cos (\\omega_p t) \\\\ \\end{bmatrix} = \\begin{bmatrix} \\sin (\\omega_p (t+k)) \\\\ \\cos (\\omega_p (t+k)) \\\\ \\end{bmatrix} \\] <p>Proof: Let \\(M\\) be a \\(2 \\times 2\\) matrix, and \\(v_1, v_2, v_3, v_4\\) are the elements. We want to find them.</p> \\[ \\begin{bmatrix} v_1 &amp; v_2 \\\\ v_3 &amp; v_4 \\end{bmatrix} \\begin{bmatrix} \\sin (\\omega_p t) \\\\ \\cos (\\omega_p t) \\\\ \\end{bmatrix} = \\begin{bmatrix} \\sin (\\omega_p (t+k)) \\\\ \\cos (\\omega_p (t+k)) \\\\ \\end{bmatrix} \\] \\[ \\begin{bmatrix} v_1\\sin (\\omega_p t) + v_2\\cos (\\omega_p t)\\\\ v_3\\sin (\\omega_p t) + v_4\\cos (\\omega_p t)\\\\ \\end{bmatrix} = \\begin{bmatrix} \\sin (\\omega_p t) \\cos(\\omega_p k) + \\cos(\\omega_p t) \\sin(\\omega_p k)\\\\ \\cos (\\omega_p t) \\cos(\\omega_p k) - \\sin(\\omega_p t) \\sin(\\omega_p k)\\\\ \\end{bmatrix} \\] <p>By solving the above equations,</p> \\[v_1 = \\cos(\\omega_pk), v_2 = \\sin(\\omega_pk), v_3 = -\\sin(\\omega_pk), v_4 = \\cos(\\omega_pk)\\] <p>So,</p> \\[M =  \\begin{bmatrix} \\cos(\\omega_pk) &amp; \\sin(\\omega_pk) \\\\ -\\sin(\\omega_pk) &amp; \\cos(\\omega_pk) \\end{bmatrix} \\] <p>Similarly we can find M for other pairs and those can be stacked diagonally to form the final matrix, which is independent of \\(t\\) for a certain \\(k\\).</p> <p>Check out this awesome blog about positional encodings and some more intuition behind it: https://kazemnejad.com/blog/transformer_architecture_positional_encoding/</p> <p>Finally these encodings are added to the input embeddings and then passed onto the first encoder layer, which gives us the complete encoder.</p> <p>According to paper:</p> <p>In the embedding layers, we multiply those weights by \\(\\sqrt{d_\\text{model}}\\).</p> <p>Let's complete the code</p> <pre><code>class PositionalEmbedding(nn.Module):\n    def __init__(self, vocab_size, d_model):\n        super(PositionalEmbedding, self).__init__()\n        self.d_model = d_model\n        self.emb = nn.Embedding(vocab_size, d_model)\n\n    def forward(self, seq):\n        timesteps = seq.shape[1]\n        emb = self.emb(seq)\n        emb = emb*torch.sqrt(torch.tensor(self.d_model))\n        pe = positional_encoding(timesteps, self.d_model)\n        x = emb+pe\n        return x\n</code></pre> <p>In the paper these embedding weights are shared, but we will keep them separate them out here.</p> <pre><code>class Encoder(nn.Module):\n    def __init__(self, n_layers, d_model, d_ff, \n                    num_heads, vocab_size, dropout=0.2):\n        super(Encoder, self).__init__()\n        self.encoder_layers = nn.Sequential(\n                        *[EncoderLayer(num_heads, \n                                        d_model, d_ff, dropout)]\n                                        )\n        self.pos_emb = PositionalEmbedding(vocab_size, d_model)\n\n    def forward(self, seq):\n        emb = self.pos_emb(seq)\n        x = self.encoder_layers(emb)\n        return x\n</code></pre> <pre><code>n_layers = 6\nd_model = 512\nd_ff = 2048\nnum_heads = 8\n\nT = 128\nbatch = 4\nvocab_size = 122\n\ndummy_inp = torch.randint(low=0, high=vocab_size, size=(batch, T))\nenc = Encoder(n_layers, d_model, d_ff, num_heads, vocab_size)\nout = enc(dummy_inp)\nout.shape\n</code></pre> <pre><code>torch.Size([4, 128, 512])\n</code></pre>"},{"location":"neural_networks/transformer/transformer/#the-transformer-decoder","title":"The Transformer Decoder","text":"<p>The transformer decoder is a replacement for recurrent decoder. It works in the same way fundamentally, where it predicts an output which is fed back into at the next timestep and the next token is predicted. This is called autoregressive prediction.</p> <p>Recalling the original recurrent network, attention was used from hidden states of encoder to each timestep in decoder. Here, the idea is same. At each decoder layer, Queries will be fetched from decoder timesteps, and Keys and Values will be the final outputs of the Encoder.</p> <p>The recurrence is replaced by the self attention, same as the encoder. The attention from the encoder is called cross attention. </p> <p>Another benefit of replacing recurrence with attention is that we can use teacher forcing and parallelize decoding. We pass the whole Target Sequence into decoder shifted right, adding <code>&lt;SOS&gt;</code> as the beginning of sequence and predict the unshifted sequence.</p> <p>We have to be careful that a timestep does not attend into future timesteps. That is where the attention mask comes in handy. This is called Masked Self Attention or Causal Self Attention.</p> <p> Figure: Causal Self attention or masked self attention</p> <pre><code>class CausalSelfAttention(MultiHeadAttention):\n\n    def forward(self, x):\n        seq_length = x.shape[1]\n        batch_size = x.shape[0]\n        num_heads = self.num_heads\n\n        mask_shape = (batch_size*num_heads, seq_length, seq_length)\n        mask = (1-torch.triu(torch.ones(*mask_shape), diagonal=1))*-1e9\n        return super().forward(x,x,x, mask=mask)\n</code></pre> <p>Let's look at the complete Decoder Layer Architecture.</p> <p></p> <p>The Masked Attention was just explained. Now the cross attention, where the key and Values are from the encoder outputs and the queries are from decoder. This is the original concept of encoder-decoder attention by Bhadanau.</p> <pre><code>class CrossAttention(MultiHeadAttention):\n\n    def forward(self, x, encoder_output):\n        return super().forward(q=x, k=encoder_output, v=encoder_output, mask=None)\n</code></pre> <p>With that we are now ready with complete Decoder Layer.</p> <pre><code>class DecoderLayer(nn.Module):\n    def __init__(self, num_heads, d_model, d_ff, dropout=0.2):\n        super(DecoderLayer, self).__init__()\n        self.self_attn = CausalSelfAttention(num_heads, d_model)\n        self.cross_attn = CrossAttention(num_heads, d_model)\n        self.ff = PositionWiseFeedForward(d_ff, d_model)\n        self.addnorm1 = AddNorm(d_model, dropout)\n        self.addnorm2 = AddNorm(d_model, dropout)\n        self.addnorm3 = AddNorm(d_model, dropout)\n\n    def forward(self, x, encoder_output):\n        ## self globall attn\n        attn_out = self.self_attn(x)\n        x = self.addnorm1(x, attn_out)\n        attn_out = self.cross_attn(x, encoder_output)\n        x = self.addnorm2(x, attn_out)\n        ff_out = self.ff(x)\n        x = self.addnorm3(x, ff_out)\n        return x\n</code></pre> <pre><code>d_model = 64\nd_ff = 256\nbatch = 2\nT = 20\nn_heads = 8\n\ninput_seq = torch.rand(batch, T, d_model)\nenc_out = torch.rand(batch, T, d_model)\ndec = DecoderLayer(n_heads, d_model, d_ff)\ndec(input_seq, enc_out).shape\n</code></pre> <pre><code>torch.Size([2, 20, 64])\n</code></pre> <p>Now the complete Decoder.</p> <p></p> <p>We know the drill, add positinal encodings and a few more layers at the top. We will transform final outputs to be the same dims as out vocab size.</p> <pre><code>class Decoder(nn.Module):\n    def __init__(self, n_layers, d_model, \n                    d_ff, num_heads, vocab_dim, dropout=0.2):\n        super(Decoder, self).__init__()\n        self.decoder_layers = nn.ModuleList(\n                    [DecoderLayer(num_heads, d_model, \n                                    d_ff, dropout)])\n        self.fc = nn.Linear(d_model, vocab_dim)\n        self.softmax = nn.Softmax(dim=-1)\n        self.pos_emb = PositionalEmbedding(vocab_dim, d_model)\n\n    def forward(self, seq, encoder_output):\n        x = self.pos_emb(seq)\n        for layer in self.decoder_layers:\n            x = layer(x, encoder_output)\n        x = self.fc(x)\n        x = self.softmax(x)\n        return x\n</code></pre> <pre><code>n_layers = 6\nd_model = 512\nd_ff = 2048\nnum_heads = 8\nvocab_size= 64\n\nT = 128\nbatch = 4\nencoder_out = torch.randn(batch, T, d_model)\ndummy_inp = torch.randint(0, vocab_size, (batch, T))\ndec = Decoder(n_layers, d_model, d_ff, num_heads, vocab_size)\nout = dec(dummy_inp, encoder_out)\nout.shape\n</code></pre> <pre><code>torch.Size([4, 128, 64])\n</code></pre> <p>Let's put the transformer together now.</p> <p></p> <p>We will have some tokenized inputs and tokenized targets. Inputs will go into an embedding layer and then passed onto the encoder to produce some encodings. Target embeddings will be passed onto decoder along with encoder output to produce next timestep sequence.</p> <pre><code>class Transformer(nn.Module):\n    def __init__(self, d_model, enc_layers, dec_layers, \n                 enc_d_ff, dec_d_ff, enc_num_heads, dec_num_heads,\n                 inp_vocab_dim, out_vocab_dim, dropout=0.2):\n        super(Transformer, self).__init__()\n\n        self.encoder = Encoder(enc_layers, d_model, \n                                enc_d_ff, enc_num_heads, inp_vocab_dim)\n        self.decoder = Decoder(dec_layers, d_model, \n                                dec_d_ff, dec_num_heads, out_vocab_dim)\n\n    def forward(self, inp_seq, shifted_target):\n        encoder_out = self.encoder(inp_seq)\n        decoder_out = self.decoder(shifted_target, encoder_out)\n        return decoder_out     \n</code></pre> <pre><code>d_model = 512\nenc_layers = dec_layers = 6\nenc_d_ff = dec_d_ff = 2048\nenc_num_heads = dec_num_heads = 8\n\ninput_vocab_dim = 100\noutput_vocab_dim = 52\n\nbatch = 5\nT = 128\n\ninput_seq = torch.randint(0, input_vocab_dim, (batch, T))\nshifted_target_seq = torch.randint(0, output_vocab_dim, (batch, T))\n\ntransformer = Transformer(d_model, enc_layers, dec_layers, enc_d_ff, dec_d_ff, \n                            enc_num_heads, dec_num_heads, input_vocab_dim, output_vocab_dim)\n\ntransformer(input_seq, shifted_target_seq).shape\n</code></pre> <pre><code>torch.Size([5, 128, 52])\n</code></pre>"},{"location":"neural_networks/transformer/transformer/#resources","title":"Resources:","text":"<ol> <li>https://lilianweng.github.io/posts/2018-06-24-attention/ Great post for different types of attentions.</li> <li>https://www.tensorflow.org/text/tutorials/transformer Official TensorFlow post on transformer information.</li> <li>https://nlp.seas.harvard.edu/2018/04/03/attention.html The Annotated Transformer (although i think Add and Norm, is implemented a bit differently)</li> <li>https://mfaizan.github.io/2023/04/02/sines.html Interesting blog developing intuition of positional encoding by using, complex numbers, of all numbers. </li> </ol>"},{"location":"blog/","title":"Blog","text":""}]}