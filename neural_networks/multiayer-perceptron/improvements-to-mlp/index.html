
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="A complete description of mathematics that go in making a multi layered perceptron work.">
      
      
        <meta name="author" content="Murtaza Nazir">
      
      
        <link rel="canonical" href="https://themurtazanazir.github.io/neural_networks/multiayer-perceptron/improvements-to-mlp/">
      
      
        <link rel="prev" href="../multi-layer-perceptron/">
      
      
        <link rel="next" href="../../convolutional_neural_networks/introduction/">
      
      
        <link rel="alternate" type="application/rss+xml" title="RSS feed" href="../../../feed_rss_created.xml">
        <link rel="alternate" type="application/rss+xml" title="RSS feed of updated content" href="../../../feed_rss_updated.xml">
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.23">
    
    
      
        <title>The Multi Layer Perceptron Part  II - murtaza</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.84d31ad4.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="brown" data-md-color-accent="deep-orange">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="murtaza" class="md-header__button md-logo" aria-label="murtaza" data-md-component="logo">
      
  <img src="../../../img/logo.svg" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            murtaza
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              The Multi Layer Perceptron Part  II
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="murtaza" class="md-nav__button md-logo" aria-label="murtaza" data-md-component="logo">
      
  <img src="../../../img/logo.svg" alt="logo">

    </a>
    murtaza
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Linear Algebra
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Linear Algebra
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../linear_algebra/introduction/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../linear_algebra/vectors-linear-combinations/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Vectors & Linear Combinations
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../linear_algebra/vector-spaces-and-subspaces/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Vector Spaces & Subspaces
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Neural Networks
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Neural Networks
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2" checked>
        
          
          <label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Multi-Layer Perceptron
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3_2">
            <span class="md-nav__icon md-icon"></span>
            Multi-Layer Perceptron
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../perceptron/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Perceptron
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../perceptron-convergence-theorem/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Perceptron Convergence
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../multi-layer-perceptron/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Multi-Layer Perceptron
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Improvements to MLP
    
  </span>
  

      </a>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_3" >
        
          
          <label class="md-nav__link" for="__nav_3_3" id="__nav_3_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Convolutional Neural Networks
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3">
            <span class="md-nav__icon md-icon"></span>
            Convolutional Neural Networks
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../convolutional_neural_networks/introduction/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../convolutional_neural_networks/convolutions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Convolutions
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4" >
        
          
          <label class="md-nav__link" for="__nav_3_4" id="__nav_3_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Transformer
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_4">
            <span class="md-nav__icon md-icon"></span>
            Transformer
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../transformer/transformer/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Transformer
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<p><a href="../">← Multi-Layer Perceptron</a></p>
<h1 id="3-improvements-for-the-network">3 Improvements for the network</h1>
<p class="drop-cap">As you can see, the network takes a lot of time to train. Sometimes the network may not converge at all even after a lot of iterations. It is because the error get's stuck at a local minima. Since we update the weights from all the examples at the same time, the error follows the direction of the steepest gradient. But the steepest gradrient may lead to a local minima and so no number of iterations will help it overcome that minima. These are problems with the gradient descent algorithm and hence apply to every model that uses it not just the MLP. Now there are ways around it:</p>

<h5 id="1-changing-learning-rate">1. Changing Learning Rate:</h5>
<p>If the local minima has a small "width", then a higher learning rate may just jump over it. However it makes the network unstable.</p>
<h5 id="2-multiple-models-with-random-initialization">2. Multiple models with random initialization:</h5>
<p>We train the same model multiple times with different starting points and maybe from any other starting point the steepest gradient is towards global minima. This is very effective for small models. But some larger models take weeks to train, which have hundreds of millions of datapoints and we cannot afford to train multiple models.</p>
<h5 id="3-mini-batch-and-stochastic-gradient-descent">3. Mini-Batch and Stochastic Gradient Descent:</h5>
<p>The algorithm we have implemented is called a Batch Gradient Descent. We update the weights for every example at the same time. But sometimes the dataset is too large to perform matrix operations and so we use small random batches of data from the large dataset and update weights for them. We keep on doing this until all the data is utilized and then start another iteration again. It is called <strong>Mini-Batch Gradient Descent</strong>. Now at each weight update i.e at every step of gradient descent isn't towards the steepest gradient of error for all examples but for just this mini-batch that we used. The downside is it may not take the <em>best</em> steps towards the minima but will eventually reach there after taking a "longer road". The upside is it may sometimes not get stuck at a local minima as it may not have the steepest gradient for that training batch. Since it can avoid a local minima, we often use it in a dataset where complete batch gradient descent can be used as well. The number of examples used in a batch is called batch size. If the batch size is one,i.e if we update weight after every example, it is called <strong>Stochastic Gradient Descent</strong>. Remember, bigger the batch size, better steps it will take to minima and smaller the batch size, more deviated the path and so higher chances of avoiding local minima. There is a middle ground we need to find to make the training better.
Remember it may also cause to deviate from the global minimum and reach a local minimum.</p>
<h2 id="32-sgd-and-mini-batch-implementation">3.2 SGD and Mini-Batch Implementation</h2>
<p>Let's do a batch implementation of MLP and let's also include a regression version.</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">MLP_batch</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">init_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">layer_sizes</span><span class="p">,</span><span class="n">random_state</span><span class="p">):</span>
        <span class="c1">#save weights in a list of matrices</span>
        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">layer_sizes</span><span class="p">[</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="n">layer_sizes</span><span class="p">[</span><span class="n">l</span><span class="p">])</span><span class="o">*</span><span class="p">(</span><span class="mi">2</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">layer_sizes</span><span class="p">[</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span><span class="o">-</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">layer_sizes</span><span class="p">[</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">layer_sizes</span><span class="p">))]</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span> <span class="c1"># keep beta = 1</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">A_0</span><span class="p">,</span><span class="n">is_regression</span><span class="p">,</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="o">=</span><span class="p">[]</span>
        <span class="n">A_l</span> <span class="o">=</span> <span class="n">A_0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">A_0</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">weights</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span>
        <span class="k">for</span> <span class="n">weight</span> <span class="ow">in</span> <span class="n">weights</span><span class="p">:</span>
            <span class="n">A_lbias</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(((</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">A_l</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">1</span><span class="p">)),</span><span class="n">A_l</span><span class="p">)),</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># add bias to input data</span>
            <span class="n">H_l</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">A_lbias</span><span class="p">,</span><span class="n">weight</span><span class="p">)</span> <span class="c1"># compute the summation</span>
            <span class="n">A_l</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">H_l</span><span class="p">)</span> <span class="c1"># compute the activation</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">A_l</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">is_regression</span><span class="p">:</span>
            <span class="n">A_l</span> <span class="o">=</span> <span class="n">H_l</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">A_l</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">A_l</span> <span class="c1"># return the final output</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">T</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span><span class="n">is_regression</span><span class="p">,</span><span class="n">batch_size</span><span class="p">):</span>
        <span class="n">A_L</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">is_regression</span><span class="p">:</span>
            <span class="n">delta_L</span> <span class="o">=</span> <span class="n">A_L</span><span class="o">-</span><span class="n">T</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">delta_L</span> <span class="o">=</span> <span class="p">(</span><span class="n">A_L</span><span class="o">-</span><span class="n">T</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">A_L</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">A_L</span><span class="p">))</span> <span class="c1"># beta = 0</span>
        <span class="n">delta_l_next</span> <span class="o">=</span> <span class="n">delta_L</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
<span class="c1">#             print(i)</span>
            <span class="n">A_l</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="c1">#compute error for previous layer</span>
            <span class="n">delta_l</span> <span class="o">=</span> <span class="n">A_l</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">A_l</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">delta_l_next</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">:,:])))</span>
<span class="c1">#             A_0 A_1 A_2</span>
<span class="c1">#             W_1 W_2</span>
<span class="c1">#             0   1    2</span>
            <span class="c1"># add bias output to output matrix</span>
            <span class="n">A_lbias</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(((</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">A_l</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">1</span><span class="p">)),</span><span class="n">A_l</span><span class="p">)),</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1">#update weights using the next errors</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">batch_size</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">*</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">A_lbias</span><span class="p">),</span><span class="n">delta_l_next</span><span class="p">)))</span>
            <span class="c1"># change the next errors for next layer</span>
            <span class="n">delta_l_next</span> <span class="o">=</span> <span class="n">delta_l</span>





    <span class="k">def</span><span class="w"> </span><span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">input_data</span><span class="p">,</span><span class="n">input_target</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span><span class="n">layer_sizes</span><span class="o">=</span><span class="p">(</span><span class="mi">100</span><span class="p">,),</span> 
              <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">is_regression</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">save_weights</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>


        <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>
        <span class="n">Target</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">input_target</span><span class="p">)</span>
        <span class="n">layer_sizes</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">layer_sizes</span><span class="p">)</span>
        <span class="n">layer_sizes</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">n_outputs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">Target</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">Target</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  <span class="o">!=</span> <span class="mi">2</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">is_regression</span> <span class="k">else</span> <span class="mi">1</span>
        <span class="n">layer_sizes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">n_outputs</span><span class="p">)</span>


        <span class="bp">self</span><span class="o">.</span><span class="n">init_weights</span><span class="p">(</span><span class="n">layer_sizes</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">save_weights</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">saved_weights</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">copy</span><span class="p">()]</span>

        <span class="k">if</span> <span class="n">batch_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>




            <span class="c1"># shuffle the input so we don&#39;t train on same sequences</span>
            <span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">Target</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
            <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
            <span class="n">Target</span><span class="o">=</span><span class="n">Target</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>

            <span class="n">b</span><span class="o">=</span><span class="mi">0</span>
            <span class="k">while</span> <span class="n">b</span><span class="o">&lt;</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
                <span class="n">A_0</span><span class="o">=</span><span class="n">X</span><span class="p">[</span><span class="n">b</span><span class="p">:</span><span class="n">b</span><span class="o">+</span><span class="n">batch_size</span><span class="p">,:]</span>
                <span class="n">T</span><span class="o">=</span><span class="n">Target</span><span class="p">[</span><span class="n">b</span><span class="p">:</span><span class="n">b</span><span class="o">+</span><span class="n">batch_size</span><span class="p">,:]</span>
                <span class="n">A_L</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">A_0</span><span class="p">,</span><span class="n">is_regression</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">e</span><span class="o">%</span><span class="p">((</span><span class="n">epochs</span><span class="o">//</span><span class="mi">10</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">verbose</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;epoch:&quot;</span><span class="p">,</span><span class="n">e</span><span class="p">)</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">A_L</span><span class="o">-</span><span class="n">T</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="n">T</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;out: </span><span class="si">{</span><span class="n">A_L</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="c1">#                 print(&quot;weights&quot;,*self.weights,sep=&#39;\n&#39;,end=&#39;\n\n&#39;)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">T</span><span class="p">,</span><span class="n">learning_rate</span><span class="p">,</span><span class="n">is_regression</span><span class="p">,</span><span class="n">batch_size</span><span class="p">)</span>


                <span class="k">if</span> <span class="n">save_weights</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">saved_weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>

                <span class="n">b</span><span class="o">=</span><span class="n">b</span><span class="o">+</span><span class="n">batch_size</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">A_L</span><span class="o">-</span><span class="n">T</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="n">T</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">input_data</span><span class="p">,</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">input_data</span><span class="p">),</span><span class="n">weights</span><span class="p">)</span>
        <span class="c1">#since this output is a realnumber(between 0 &amp; 1)</span>
        <span class="c1"># we will have a threshold to predict its class for now 0.5</span>
        <span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">output</span><span class="o">&gt;</span><span class="mf">0.5</span><span class="p">)</span><span class="o">*</span><span class="mi">1</span>
        <span class="k">return</span> <span class="n">output</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">confmat</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">input_data</span><span class="p">,</span><span class="n">targets</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&#39;&#39;&#39;returns the confusion matrix for binary classification&#39;&#39;&#39;</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">input_data</span><span class="p">))</span>
        <span class="n">T</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">targets</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">tp</span> <span class="o">=</span> <span class="p">((</span><span class="n">T</span><span class="o">==</span><span class="mi">1</span><span class="p">)</span><span class="o">&amp;</span><span class="p">(</span><span class="n">outputs</span><span class="o">==</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="n">tn</span> <span class="o">=</span> <span class="p">((</span><span class="n">T</span><span class="o">==</span><span class="mi">0</span><span class="p">)</span><span class="o">&amp;</span><span class="p">(</span><span class="n">outputs</span><span class="o">==</span><span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="n">fp</span> <span class="o">=</span> <span class="p">((</span><span class="n">T</span><span class="o">==</span><span class="mi">0</span><span class="p">)</span><span class="o">&amp;</span><span class="p">(</span><span class="n">outputs</span><span class="o">==</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="n">fn</span> <span class="o">=</span> <span class="p">((</span><span class="n">T</span><span class="o">==</span><span class="mi">1</span><span class="p">)</span><span class="o">&amp;</span><span class="p">(</span><span class="n">outputs</span><span class="o">==</span><span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">tp</span><span class="p">,</span><span class="n">fp</span><span class="p">],</span>
                        <span class="p">[</span><span class="n">fn</span><span class="p">,</span><span class="n">tn</span><span class="p">]])</span>
</code></pre></div>
<p>Let's make a simple regression dataset that has just one feature and it's target is a simple linear function to clear things.</p>
<div class="highlight"><pre><span></span><code><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">364</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">12000</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">T</span> <span class="o">=</span> <span class="mi">25</span><span class="o">*</span><span class="n">X</span><span class="o">+</span><span class="mf">7.8</span>
</code></pre></div>
<p>Let's now try different forms of gradient descent on it. Also set the random state so we have the same starting points. We will also check the training time for the same number of epochs along with the error.</p>
<h5 id="322-results">3.2.2 Results</h5>
<div class="highlight"><pre><span></span><code><span class="c1">#Batch Gradient Descent(Full Dataset)</span>
<span class="n">batch_model</span><span class="o">=</span><span class="n">MLP_batch</span><span class="p">()</span>
<span class="o">%</span><span class="n">time</span> <span class="n">batch_model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">T</span><span class="p">,</span><span class="n">layer_sizes</span><span class="o">=</span><span class="p">(),</span><span class="n">epochs</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span><span class="n">is_regression</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">learning_rate</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div>
<pre><code>Error: 0.0004875391124500146
CPU times: user 138 ms, sys: 47 µs, total: 138 ms
Wall time: 81.9 ms
</code></pre>
<div class="highlight"><pre><span></span><code><span class="n">mini_batch1024</span> <span class="o">=</span> <span class="n">MLP_batch</span><span class="p">()</span>
<span class="o">%</span><span class="n">time</span> <span class="n">mini_batch1024</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">T</span><span class="p">,</span><span class="n">layer_sizes</span><span class="o">=</span><span class="p">(),</span><span class="n">epochs</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span><span class="n">is_regression</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">learning_rate</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">)</span>
</code></pre></div>
<pre><code>Error: 1.834208786827062e-29
CPU times: user 197 ms, sys: 0 ns, total: 197 ms
Wall time: 100 ms
</code></pre>
<div class="highlight"><pre><span></span><code><span class="n">mini_batch64</span> <span class="o">=</span> <span class="n">MLP_batch</span><span class="p">()</span>
<span class="o">%</span><span class="n">time</span> <span class="n">mini_batch64</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">T</span><span class="p">,</span><span class="n">layer_sizes</span><span class="o">=</span><span class="p">(),</span><span class="n">epochs</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span><span class="n">is_regression</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">learning_rate</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>
</code></pre></div>
<pre><code>Error: 4.0429121392576855e-30
CPU times: user 611 ms, sys: 3.93 ms, total: 615 ms
Wall time: 614 ms
</code></pre>
<div class="highlight"><pre><span></span><code><span class="c1"># for SGD, we will set batch_size=1</span>
<span class="n">SGD</span> <span class="o">=</span> <span class="n">MLP_batch</span><span class="p">()</span>
<span class="o">%</span><span class="n">time</span> <span class="n">SGD</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">T</span><span class="p">,</span><span class="n">layer_sizes</span><span class="o">=</span><span class="p">(),</span><span class="n">epochs</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span><span class="n">is_regression</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">learning_rate</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>
<pre><code>Error: 0.0
CPU times: user 30.8 s, sys: 347 ms, total: 31.2 s
Wall time: 30.7 s
</code></pre>
<p>As you can see, SGD performed the best while taking a lot of time to train. Both of these because it made much more weight updates than any of them. For equal number of weight updates, the batch version should perform best.</p>
<p>Now let's try keeping the weight updates to be same, the best way is to let SGD go over the dataset atleast once. So that is a 12000 weight updates.</p>
<div class="highlight"><pre><span></span><code><span class="c1">#Batch Gradient Descent(Full Dataset)</span>
<span class="n">batch_model</span><span class="o">=</span><span class="n">MLP_batch</span><span class="p">()</span>
<span class="o">%</span><span class="n">time</span> <span class="n">batch_model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">T</span><span class="p">,</span><span class="n">layer_sizes</span><span class="o">=</span><span class="p">(),</span><span class="n">epochs</span><span class="o">=</span><span class="mi">12000</span><span class="p">,</span><span class="n">is_regression</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div>
<pre><code>Error: 3.255002724680526e-06
CPU times: user 13.6 s, sys: 90.9 ms, total: 13.7 s
Wall time: 6.87 s
</code></pre>
<div class="highlight"><pre><span></span><code><span class="n">mini_batch1024</span> <span class="o">=</span> <span class="n">MLP_batch</span><span class="p">()</span>
<span class="o">%</span><span class="n">time</span> <span class="n">mini_batch1024</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">T</span><span class="p">,</span><span class="n">layer_sizes</span><span class="o">=</span><span class="p">(),</span><span class="n">epochs</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span><span class="n">is_regression</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">)</span>
</code></pre></div>
<pre><code>Error: 3.2897801810988825e-06
CPU times: user 2.34 s, sys: 15.8 ms, total: 2.35 s
Wall time: 1.18 s
</code></pre>
<div class="highlight"><pre><span></span><code><span class="n">mini_batch64</span> <span class="o">=</span> <span class="n">MLP_batch</span><span class="p">()</span>
<span class="o">%</span><span class="n">time</span> <span class="n">mini_batch64</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">T</span><span class="p">,</span><span class="n">layer_sizes</span><span class="o">=</span><span class="p">(),</span><span class="n">epochs</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span><span class="n">is_regression</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>
</code></pre></div>
<pre><code>Error: 3.473449823532693e-06
CPU times: user 515 ms, sys: 25 µs, total: 515 ms
Wall time: 512 ms
</code></pre>
<div class="highlight"><pre><span></span><code><span class="c1"># for SGD, we will set batch_size=1</span>
<span class="n">SGD</span> <span class="o">=</span> <span class="n">MLP_batch</span><span class="p">()</span>
<span class="o">%</span><span class="n">time</span> <span class="n">SGD</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">T</span><span class="p">,</span><span class="n">layer_sizes</span><span class="o">=</span><span class="p">(),</span><span class="n">epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">is_regression</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>
<pre><code>Error: 6.853664577305198e-06
CPU times: user 480 ms, sys: 40.5 ms, total: 521 ms
Wall time: 464 ms
</code></pre>
<p>As you can see the opposite has happened, the batch version has done great but in most time and SGD has done worst in least time. But the mini batch of 1024 examples has done acceptable in very small time. We could prefer this model over both the models. It is the middle ground we need to find and depends on what can we spare, time or cost? It also depends on the type of dataset we are using. That stuff is for trial and error, atleast for now.</p>
<h2 id="33-visualization">3.3 Visualization</h2>
<p>Now let's see how the weights are updated. But before that let's project out data error in a chart. Let's see how the error varies as with the weight. We will plot the weights on x and y axis and error on z axis.</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">compute_multiple_error</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">T</span><span class="p">,</span><span class="n">xx</span><span class="p">,</span><span class="n">yy</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;computing error&quot;</span><span class="p">)</span>
    <span class="n">e</span> <span class="o">=</span> <span class="p">(</span><span class="n">T</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="p">(</span><span class="n">xx</span><span class="o">*</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="n">yy</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)):</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">%</span><span class="mi">1000</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="si">}</span><span class="s2"> done&quot;</span><span class="p">)</span>
        <span class="n">e</span> <span class="o">=</span> <span class="n">e</span><span class="o">+</span><span class="p">(</span><span class="n">T</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-</span><span class="p">(</span><span class="n">xx</span><span class="o">*</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-</span><span class="n">yy</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span>
    <span class="k">return</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">T</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">*</span><span class="n">e</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">xx</span><span class="p">,</span><span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">35</span><span class="p">,</span><span class="mi">1000</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">30</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">1000</span><span class="p">))</span>
<span class="n">err</span> <span class="o">=</span> <span class="n">compute_multiple_error</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">T</span><span class="p">,</span><span class="n">xx</span><span class="p">,</span><span class="n">yy</span><span class="p">)</span>
</code></pre></div>
<pre><code>computing error
1000/12000 done
2000/12000 done
3000/12000 done
4000/12000 done
5000/12000 done
6000/12000 done
7000/12000 done
8000/12000 done
9000/12000 done
10000/12000 done
11000/12000 done
12000/12000 done
</code></pre>
<div class="highlight"><pre><span></span><code><span class="n">fig</span> <span class="o">=</span> <span class="n">go</span><span class="o">.</span><span class="n">Figure</span><span class="p">()</span>
<span class="n">fig</span><span class="o">.</span><span class="n">add_trace</span><span class="p">(</span><span class="n">go</span><span class="o">.</span><span class="n">Surface</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">xx</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="n">yy</span><span class="p">,</span><span class="n">z</span><span class="o">=</span><span class="n">err</span><span class="p">))</span>
<span class="n">fig</span><span class="o">.</span><span class="n">update_layout</span><span class="p">(</span>
        <span class="n">scene</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;xaxis&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;title&quot;</span><span class="p">:</span> <span class="s2">&quot;Weight 1&quot;</span><span class="p">},</span>
            <span class="s2">&quot;zaxis&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;title&quot;</span><span class="p">:</span> <span class="s2">&quot;Error&quot;</span><span class="p">},</span>
            <span class="s2">&quot;yaxis&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;title&quot;</span><span class="p">:</span> <span class="s2">&quot;Weight 2&quot;</span><span class="p">}</span>
        <span class="p">})</span>
<span class="n">fig</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>
<p>As you can see, this surface has a minimum. A 3D graph looks clumsy, let's use contours of this 3D plot. However plotly doesn't support uneven labels, we will use the logarithm of error to make the plots have regular contours.</p>
<div class="highlight"><pre><span></span><code><span class="n">fig</span> <span class="o">=</span> <span class="n">go</span><span class="o">.</span><span class="n">Figure</span><span class="p">(</span><span class="n">data</span><span class="o">=</span>
                <span class="p">[</span><span class="n">go</span><span class="o">.</span><span class="n">Contour</span><span class="p">(</span>
                    <span class="n">x</span><span class="o">=</span><span class="n">xx</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                    <span class="n">y</span><span class="o">=</span><span class="n">yy</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span>
                    <span class="n">z</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">log1p</span><span class="p">(</span><span class="n">err</span><span class="p">),</span>
                    <span class="n">contours_coloring</span><span class="o">=</span><span class="s1">&#39;lines&#39;</span><span class="p">,</span>
                    <span class="n">line_width</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                    <span class="n">showscale</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="p">],</span>
                <span class="n">layout</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">xaxis_title</span><span class="o">=</span><span class="s2">&quot;Weight 1&quot;</span><span class="p">,</span>
                           <span class="n">yaxis_title</span><span class="o">=</span><span class="s2">&quot; Weight 2&quot;</span><span class="p">,</span>
                           <span class="n">width</span><span class="o">=</span><span class="mi">915</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">800</span><span class="p">,),</span>

               <span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">add_trace</span><span class="p">(</span><span class="n">go</span><span class="o">.</span><span class="n">Scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="p">[</span><span class="mi">25</span><span class="p">],</span><span class="n">y</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mf">7.8</span><span class="p">],</span><span class="n">marker</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">symbol</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">10</span><span class="p">)))</span>
<span class="n">fig</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>
<p>These are the contour lines and the cross(x) sign is the minimum of the error, we want our models to descend to that point. Let's see how each model does it. Each line has zero gradient(i.e they lie on the same level) and the perpendicular direction at each line is the steepest gradient.</p>
<div class="highlight"><pre><span></span><code><span class="n">batch_model</span><span class="o">=</span><span class="n">MLP_batch</span><span class="p">()</span>
<span class="n">batch_model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">T</span><span class="p">,</span><span class="n">layer_sizes</span><span class="o">=</span><span class="p">(),</span><span class="n">epochs</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span><span class="n">is_regression</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">learning_rate</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">save_weights</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">mini_batch1024</span> <span class="o">=</span> <span class="n">MLP_batch</span><span class="p">()</span>
<span class="n">mini_batch1024</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">T</span><span class="p">,</span><span class="n">layer_sizes</span><span class="o">=</span><span class="p">(),</span><span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span><span class="n">is_regression</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">learning_rate</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                     <span class="n">batch_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span><span class="n">save_weights</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">mini_batch64</span> <span class="o">=</span> <span class="n">MLP_batch</span><span class="p">()</span>
<span class="n">mini_batch64</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">T</span><span class="p">,</span><span class="n">layer_sizes</span><span class="o">=</span><span class="p">(),</span><span class="n">epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">is_regression</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">learning_rate</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                     <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span><span class="n">save_weights</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>


<span class="n">SGD</span> <span class="o">=</span> <span class="n">MLP_batch</span><span class="p">()</span>
<span class="n">SGD</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">T</span><span class="p">,</span><span class="n">layer_sizes</span><span class="o">=</span><span class="p">(),</span><span class="n">epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">is_regression</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">learning_rate</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">save_weights</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>
<pre><code>Error: 3.796053431101478e-08
Error: 1.8660418967252465e-29
Error: 2.1144567049716302e-10
Error: 0.0
</code></pre>
<div class="highlight"><pre><span></span><code><span class="n">weights</span><span class="o">=</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">i</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">saved_weights</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="p">(</span><span class="n">batch_model</span><span class="p">,</span><span class="n">mini_batch1024</span><span class="p">,</span><span class="n">mini_batch64</span><span class="p">,</span><span class="n">SGD</span><span class="p">)]</span>
</code></pre></div>
<p>Let's now draw every weight update</p>
<div class="highlight"><pre><span></span><code><span class="n">fig</span> <span class="o">=</span> <span class="n">go</span><span class="o">.</span><span class="n">Figure</span><span class="p">()</span>

<span class="n">fig</span><span class="o">.</span><span class="n">add_trace</span><span class="p">(</span><span class="n">go</span><span class="o">.</span><span class="n">Contour</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">xx</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">yy</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">z</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">log1p</span><span class="p">(</span><span class="n">err</span><span class="p">),</span> <span class="n">autocontour</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                         <span class="n">line_width</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">showscale</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">contours_coloring</span><span class="o">=</span><span class="s1">&#39;lines&#39;</span><span class="p">,</span> 
                        <span class="p">),</span>
             <span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">add_trace</span><span class="p">(</span><span class="n">go</span><span class="o">.</span><span class="n">Scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="p">[</span><span class="mi">25</span><span class="p">],</span><span class="n">y</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mf">7.8</span><span class="p">],</span><span class="n">mode</span><span class="o">=</span><span class="s2">&quot;markers&quot;</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">symbol</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">),</span>
                         <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Minimum&quot;</span><span class="p">),)</span>

<span class="k">for</span> <span class="n">p</span><span class="p">,</span><span class="n">name</span><span class="p">,</span><span class="n">color</span><span class="p">,</span><span class="n">opacity</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span>
                        <span class="p">[</span><span class="s1">&#39;Full Batch&#39;</span><span class="p">,</span><span class="s1">&#39;1024&#39;</span><span class="p">,</span><span class="s1">&#39;64&#39;</span><span class="p">,</span><span class="s1">&#39;1(SGD)&#39;</span><span class="p">],</span>
                        <span class="p">[</span><span class="s1">&#39;Blue&#39;</span><span class="p">,</span><span class="s2">&quot;Cyan&quot;</span><span class="p">,</span><span class="s2">&quot;Green&quot;</span><span class="p">,</span><span class="s2">&quot;Magenta&quot;</span><span class="p">],</span>
                        <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mf">0.8</span><span class="p">,</span><span class="mf">0.8</span><span class="p">,</span><span class="mf">0.8</span><span class="p">]):</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">add_trace</span><span class="p">(</span><span class="n">go</span><span class="o">.</span><span class="n">Scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">weights</span><span class="p">[</span><span class="n">p</span><span class="p">][</span><span class="mi">1</span><span class="p">,:],</span><span class="n">y</span><span class="o">=</span><span class="n">weights</span><span class="p">[</span><span class="n">p</span><span class="p">][</span><span class="mi">0</span><span class="p">,:],</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span><span class="n">opacity</span><span class="o">=</span><span class="n">opacity</span><span class="p">,</span>
                             <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;lines+markers&quot;</span><span class="p">,</span><span class="n">line</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">2</span><span class="p">)),)</span>


<span class="n">fig</span><span class="o">.</span><span class="n">update_layout</span><span class="p">(</span><span class="n">legend</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span><span class="n">height</span><span class="o">=</span><span class="mi">800</span><span class="p">,</span><span class="n">width</span><span class="o">=</span><span class="mi">915</span><span class="p">)</span>

<span class="n">fig</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>
<p>As you can see, the bigger the batch size, the better the weight updates are. Also look the full batch version took steps which were perpendicular to the contour line at every point(i.e the steepest gradient). But we cannot always afford to use full dataset at one time. And besides, this random-looking update might help us to deviate from local minima. Still we can use a few more tricks to make it faster and better.</p>
<h2 id="34-the-momentum">3.4 The Momentum</h2>
<p>While using the Mini-Batch or Stochastic Gradient Descent, we use certain ways to keep the weights on track and not deviate too much. One of them is called picking up the momentum. Since in SGD or MBD, each weight update is based upon just the examples we are considering, the weights are change according to just that batch and does not consider the whole dataset and make training longer, especially when there is no local minima and traing set is very large. </p>
<p>The idea is to not just update the weights from the current batch's \(\Delta w\), but from the previous batch as well.
So, we update the weight of any layer \(l\), iteration \(t\) as: </p>
<div class="arithmatex">\[ w^{(l,t)}_{pq} \leftarrow w^{(l,t-1)}_{pq} - \Delta w^{(l,t)}_{pq}\tag{90} \]</div>
<p>where,</p>
<div class="arithmatex">\[ \Delta w^{(l,t)}_{pq} = \eta \delta^{(l)}_qa^{(l-1)}_p + \alpha \Delta w^{(l,t-1)}_{pq}\tag{91} \]</div>
<p>and </p>
<ol>
<li>\(w^{(l,t)}_{pq}\) is the weight of layer \(l\) of iteration \(t\).</li>
<li>\(\alpha\) is the weightage for the previous weight update. \(0.9\) is a good number.</li>
</ol>
<p>Note: <em>iteration</em> means a weight update and not an epoch.</p>
<p>However that method is effective as it can be, but we will need to tune learning rate again and it has different impact. The current weight has a weight of \(\eta\) while the previous weight updates have \(\alpha\) weight. Earlier all of it had weight of \(\eta\). So the proposition here is to assign a weight of \(1-\alpha\) to the current update and \(\alpha\) to previous ones so to make a total weight of unity and then we use the learning rate all over it.</p>
<p>Like:</p>
<div class="arithmatex">\[ \Delta w^{(l,t)}_{pq} = \eta \big((1-\alpha)\delta ^{(l)}_qa^{(l-1)}_p + \alpha \Delta w^{(l,t-1)}_{pq}\big) \tag{92} \]</div>
<p>It can be easily done by:</p>
<div class="highlight"><pre><span></span><code><span class="n">delta_w</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">momentum</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">batch_size</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">A_lbias</span><span class="p">),</span><span class="n">delta_l_next</span><span class="p">))</span> <span class="o">+</span> <span class="n">momentum</span><span class="o">*</span><span class="n">delta_w</span>
<span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span> <span class="o">-</span> <span class="n">learning_rate</span><span class="o">*</span><span class="n">delta_w</span>
</code></pre></div>
<p>Let's try to write it up. We need just a few changes in the code.</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">MLP_momentum</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">init_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">layer_sizes</span><span class="p">,</span><span class="n">random_state</span><span class="p">):</span>
        <span class="c1">#save weights in a list of matrices</span>
        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">layer_sizes</span><span class="p">[</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="n">layer_sizes</span><span class="p">[</span><span class="n">l</span><span class="p">])</span><span class="o">*</span><span class="p">(</span><span class="mi">2</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">layer_sizes</span><span class="p">[</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span><span class="o">-</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">layer_sizes</span><span class="p">[</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">layer_sizes</span><span class="p">))]</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span> <span class="c1"># keep beta = 1</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">A_0</span><span class="p">,</span><span class="n">is_regression</span><span class="p">,</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="o">=</span><span class="p">[]</span>
        <span class="n">A_l</span> <span class="o">=</span> <span class="n">A_0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">A_0</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">weights</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span>
        <span class="k">for</span> <span class="n">weight</span> <span class="ow">in</span> <span class="n">weights</span><span class="p">:</span>
            <span class="n">A_lbias</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(((</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">A_l</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">1</span><span class="p">)),</span><span class="n">A_l</span><span class="p">)),</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># add bias to input data</span>
            <span class="n">H_l</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">A_lbias</span><span class="p">,</span><span class="n">weight</span><span class="p">)</span> <span class="c1"># compute the summation</span>
            <span class="n">A_l</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">H_l</span><span class="p">)</span> <span class="c1"># compute the activation</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">A_l</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">is_regression</span><span class="p">:</span>
            <span class="n">A_l</span> <span class="o">=</span> <span class="n">H_l</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">A_l</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">A_l</span> <span class="c1"># return the final output</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">T</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span><span class="n">is_regression</span><span class="p">,</span><span class="n">batch_size</span><span class="p">,</span><span class="n">momentum</span><span class="p">,</span><span class="n">delta_w</span><span class="p">):</span>
        <span class="n">A_L</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">is_regression</span><span class="p">:</span>
            <span class="n">delta_L</span> <span class="o">=</span> <span class="n">A_L</span><span class="o">-</span><span class="n">T</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">delta_L</span> <span class="o">=</span> <span class="p">(</span><span class="n">A_L</span><span class="o">-</span><span class="n">T</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">A_L</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">A_L</span><span class="p">))</span> <span class="c1"># beta = 0</span>
        <span class="n">delta_l_next</span> <span class="o">=</span> <span class="n">delta_L</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
<span class="c1">#             print(i)</span>
            <span class="n">A_l</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="c1">#compute error for previous layer</span>
            <span class="n">delta_l</span> <span class="o">=</span> <span class="n">A_l</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">A_l</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">delta_l_next</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">:,:])))</span>
<span class="c1">#             A_0 A_1 A_2</span>
<span class="c1">#             W_1 W_2</span>
<span class="c1">#             0   1    2</span>
            <span class="c1"># add bias output to output matrix</span>
            <span class="n">A_lbias</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(((</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">A_l</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">1</span><span class="p">)),</span><span class="n">A_l</span><span class="p">)),</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

            <span class="c1"># compute delta_w</span>
            <span class="n">delta_w</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">momentum</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">batch_size</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">A_lbias</span><span class="p">),</span><span class="n">delta_l_next</span><span class="p">))</span> <span class="o">+</span> <span class="n">momentum</span><span class="o">*</span><span class="n">delta_w</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>


            <span class="c1">#update weights using the next errors</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">learning_rate</span><span class="o">*</span><span class="n">delta_w</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="c1"># change the next errors for next layer</span>
            <span class="n">delta_l_next</span> <span class="o">=</span> <span class="n">delta_l</span>
        <span class="k">return</span> <span class="n">delta_w</span>





    <span class="k">def</span><span class="w"> </span><span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">,</span> <span class="n">input_target</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">layer_sizes</span><span class="o">=</span><span class="p">(</span><span class="mi">100</span><span class="p">,),</span> 
              <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">is_regression</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
              <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">save_weights</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">warm_start</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>


        <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>
        <span class="n">Target</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">input_target</span><span class="p">)</span>
        <span class="n">layer_sizes</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">layer_sizes</span><span class="p">)</span>
        <span class="n">layer_sizes</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">n_outputs</span> <span class="o">=</span> <span class="n">Target</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">layer_sizes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">n_outputs</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">warm_start</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">init_weights</span><span class="p">(</span><span class="n">layer_sizes</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">save_weights</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">saved_weights</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">copy</span><span class="p">()]</span>

        <span class="k">if</span> <span class="n">batch_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># initialize delta_w to be zero for every layer</span>
        <span class="n">delta_w</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="o">*</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>




            <span class="c1"># shuffle the input so we don&#39;t train on same sequences</span>
            <span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">Target</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
            <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
            <span class="n">Target</span><span class="o">=</span><span class="n">Target</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>

            <span class="n">b</span><span class="o">=</span><span class="mi">0</span>
            <span class="k">while</span> <span class="n">b</span><span class="o">&lt;</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
                <span class="n">A_0</span><span class="o">=</span><span class="n">X</span><span class="p">[</span><span class="n">b</span><span class="p">:</span><span class="n">b</span><span class="o">+</span><span class="n">batch_size</span><span class="p">,:]</span>
                <span class="n">T</span><span class="o">=</span><span class="n">Target</span><span class="p">[</span><span class="n">b</span><span class="p">:</span><span class="n">b</span><span class="o">+</span><span class="n">batch_size</span><span class="p">,:]</span>
                <span class="n">A_L</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">A_0</span><span class="p">,</span><span class="n">is_regression</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">e</span><span class="o">%</span><span class="p">((</span><span class="n">epochs</span><span class="o">//</span><span class="mi">10</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">verbose</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;epoch:&quot;</span><span class="p">,</span><span class="n">e</span><span class="p">)</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">A_L</span><span class="o">-</span><span class="n">T</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="n">T</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="c1">#                     print(f&quot;out: {A_L}&quot;)</span>
    <span class="c1">#                 print(&quot;weights&quot;,*self.weights,sep=&#39;\n&#39;,end=&#39;\n\n&#39;)</span>
                <span class="n">delta_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">T</span><span class="p">,</span><span class="n">learning_rate</span><span class="p">,</span><span class="n">is_regression</span><span class="p">,</span><span class="n">batch_size</span><span class="p">,</span><span class="n">momentum</span><span class="p">,</span><span class="n">delta_w</span><span class="p">)</span>


                <span class="k">if</span> <span class="n">save_weights</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">saved_weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>
                <span class="n">b</span><span class="o">=</span><span class="n">b</span><span class="o">+</span><span class="n">batch_size</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">A_L</span><span class="o">-</span><span class="n">T</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="n">T</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">input_data</span><span class="p">,</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">input_data</span><span class="p">),</span><span class="n">weights</span><span class="p">)</span>
        <span class="c1">#since this output is a realnumber(between 0 &amp; 1)</span>
        <span class="c1"># we will have a threshold to predict its class for now 0.5</span>
        <span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">output</span><span class="o">&gt;</span><span class="mf">0.5</span><span class="p">)</span><span class="o">*</span><span class="mi">1</span>
        <span class="k">return</span> <span class="n">output</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">confmat</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">input_data</span><span class="p">,</span><span class="n">targets</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&#39;&#39;&#39;returns the confusion matrix for binary classification&#39;&#39;&#39;</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">input_data</span><span class="p">))</span>
        <span class="n">T</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">targets</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">tp</span> <span class="o">=</span> <span class="p">((</span><span class="n">T</span><span class="o">==</span><span class="mi">1</span><span class="p">)</span><span class="o">&amp;</span><span class="p">(</span><span class="n">outputs</span><span class="o">==</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="n">tn</span> <span class="o">=</span> <span class="p">((</span><span class="n">T</span><span class="o">==</span><span class="mi">0</span><span class="p">)</span><span class="o">&amp;</span><span class="p">(</span><span class="n">outputs</span><span class="o">==</span><span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="n">fp</span> <span class="o">=</span> <span class="p">((</span><span class="n">T</span><span class="o">==</span><span class="mi">0</span><span class="p">)</span><span class="o">&amp;</span><span class="p">(</span><span class="n">outputs</span><span class="o">==</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="n">fn</span> <span class="o">=</span> <span class="p">((</span><span class="n">T</span><span class="o">==</span><span class="mi">1</span><span class="p">)</span><span class="o">&amp;</span><span class="p">(</span><span class="n">outputs</span><span class="o">==</span><span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">tp</span><span class="p">,</span><span class="n">fp</span><span class="p">],</span>
                        <span class="p">[</span><span class="n">fn</span><span class="p">,</span><span class="n">tn</span><span class="p">]])</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">SGD</span> <span class="o">=</span> <span class="n">MLP_momentum</span><span class="p">()</span>
<span class="n">SGD</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">T</span><span class="p">,</span><span class="n">layer_sizes</span><span class="o">=</span><span class="p">(),</span><span class="n">epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">is_regression</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">learning_rate</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
          <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">save_weights</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">momentum</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">SGD_mom</span> <span class="o">=</span> <span class="n">MLP_momentum</span><span class="p">()</span>
<span class="n">SGD_mom</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">T</span><span class="p">,</span><span class="n">layer_sizes</span><span class="o">=</span><span class="p">(),</span><span class="n">epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">is_regression</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">learning_rate</span><span class="o">=</span>
              <span class="mi">1</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
          <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">save_weights</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">momentum</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
</code></pre></div>
<pre><code>0.0
</code></pre>
<div class="highlight"><pre><span></span><code><span class="n">weights</span><span class="o">=</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">i</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">saved_weights</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="p">(</span><span class="n">SGD</span><span class="p">,</span><span class="n">SGD_mom</span><span class="p">)]</span>
</code></pre></div>
<p>Let's now draw every weight update</p>
<div class="highlight"><pre><span></span><code><span class="n">fig</span> <span class="o">=</span> <span class="n">go</span><span class="o">.</span><span class="n">Figure</span><span class="p">()</span>

<span class="n">fig</span><span class="o">.</span><span class="n">add_trace</span><span class="p">(</span><span class="n">go</span><span class="o">.</span><span class="n">Contour</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">xx</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">yy</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">z</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">log1p</span><span class="p">(</span><span class="n">err</span><span class="p">),</span> <span class="n">autocontour</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                         <span class="n">line_width</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">showscale</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">contours_coloring</span><span class="o">=</span><span class="s1">&#39;lines&#39;</span><span class="p">,</span> 
                        <span class="p">),</span>
             <span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">add_trace</span><span class="p">(</span><span class="n">go</span><span class="o">.</span><span class="n">Scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="p">[</span><span class="mi">25</span><span class="p">],</span><span class="n">y</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mf">7.8</span><span class="p">],</span><span class="n">mode</span><span class="o">=</span><span class="s2">&quot;markers&quot;</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">symbol</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">),</span>
                         <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Minimum&quot;</span><span class="p">),)</span>

<span class="k">for</span> <span class="n">p</span><span class="p">,</span><span class="n">name</span><span class="p">,</span><span class="n">color</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span>
                        <span class="p">[</span><span class="s1">&#39;Without Momentum(SGD)&#39;</span><span class="p">,</span><span class="s1">&#39;With Momentum(SGD)&#39;</span><span class="p">],</span>
                        <span class="p">[</span><span class="s1">&#39;Blue&#39;</span><span class="p">,</span><span class="s2">&quot;Red&quot;</span><span class="p">,],</span>
                               <span class="p">):</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">add_trace</span><span class="p">(</span><span class="n">go</span><span class="o">.</span><span class="n">Scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">weights</span><span class="p">[</span><span class="n">p</span><span class="p">][</span><span class="mi">1</span><span class="p">,:],</span><span class="n">y</span><span class="o">=</span><span class="n">weights</span><span class="p">[</span><span class="n">p</span><span class="p">][</span><span class="mi">0</span><span class="p">,:],</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span><span class="n">opacity</span><span class="o">=</span><span class="n">opacity</span><span class="p">,</span>
                             <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;lines+markers&quot;</span><span class="p">,</span>
                             <span class="n">line</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">2</span><span class="p">)),</span>
                 <span class="p">)</span>



<span class="n">fig</span><span class="o">.</span><span class="n">update_layout</span><span class="p">(</span><span class="n">legend</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span><span class="n">height</span><span class="o">=</span><span class="mi">800</span><span class="p">,</span><span class="n">width</span><span class="o">=</span><span class="mi">915</span><span class="p">,</span><span class="n">xaxis_title</span><span class="o">=</span><span class="s2">&quot;Weight 1&quot;</span><span class="p">,</span> <span class="n">yaxis_title</span><span class="o">=</span><span class="s2">&quot;Weight 2&quot;</span><span class="p">,</span>
                 <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Contour Plot of Error&quot;</span><span class="p">)</span>

<span class="n">fig</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>
<p>As you can see, the updates are much more smooth for SGD. This also makes it possible to use smaller learning rate and thus making it more stable. Let's now see for a complete batch and see something different.</p>
<div class="highlight"><pre><span></span><code><span class="n">batch_model</span><span class="o">=</span><span class="n">MLP_momentum</span><span class="p">()</span>
<span class="n">batch_model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">T</span><span class="p">,</span><span class="n">layer_sizes</span><span class="o">=</span><span class="p">(),</span><span class="n">epochs</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span><span class="n">is_regression</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">learning_rate</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                  <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">save_weights</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">momentum</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">batch_model_mom</span><span class="o">=</span><span class="n">MLP_momentum</span><span class="p">()</span>
<span class="n">batch_model_mom</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">T</span><span class="p">,</span><span class="n">layer_sizes</span><span class="o">=</span><span class="p">(),</span><span class="n">epochs</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span><span class="n">is_regression</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">learning_rate</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                      <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">save_weights</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
</code></pre></div>
<pre><code>3.8757600362031304e-05
</code></pre>
<div class="highlight"><pre><span></span><code><span class="n">weights</span><span class="o">=</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">i</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">saved_weights</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="p">(</span><span class="n">batch_model</span><span class="p">,</span><span class="n">batch_model_mom</span><span class="p">)]</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">fig</span> <span class="o">=</span> <span class="n">go</span><span class="o">.</span><span class="n">Figure</span><span class="p">()</span>

<span class="n">fig</span><span class="o">.</span><span class="n">add_trace</span><span class="p">(</span><span class="n">go</span><span class="o">.</span><span class="n">Contour</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">xx</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">yy</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">z</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">log1p</span><span class="p">(</span><span class="n">err</span><span class="p">),</span> <span class="n">autocontour</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                         <span class="n">line_width</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">showscale</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">contours_coloring</span><span class="o">=</span><span class="s1">&#39;lines&#39;</span><span class="p">,</span> 
                        <span class="p">),</span>
             <span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">add_trace</span><span class="p">(</span><span class="n">go</span><span class="o">.</span><span class="n">Scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="p">[</span><span class="mi">25</span><span class="p">],</span><span class="n">y</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mf">7.8</span><span class="p">],</span><span class="n">mode</span><span class="o">=</span><span class="s2">&quot;markers&quot;</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">symbol</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">),</span>
                         <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Minimum&quot;</span><span class="p">),)</span>

<span class="k">for</span> <span class="n">p</span><span class="p">,</span><span class="n">name</span><span class="p">,</span><span class="n">color</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span>
                        <span class="p">[</span><span class="s1">&#39;Without Momentum(Batch)&#39;</span><span class="p">,</span><span class="s1">&#39;With Momentum(Batch)&#39;</span><span class="p">],</span>
                        <span class="p">[</span><span class="s1">&#39;Blue&#39;</span><span class="p">,</span><span class="s2">&quot;Red&quot;</span><span class="p">,],</span>
                               <span class="p">):</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">add_trace</span><span class="p">(</span><span class="n">go</span><span class="o">.</span><span class="n">Scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">weights</span><span class="p">[</span><span class="n">p</span><span class="p">][</span><span class="mi">1</span><span class="p">,:],</span><span class="n">y</span><span class="o">=</span><span class="n">weights</span><span class="p">[</span><span class="n">p</span><span class="p">][</span><span class="mi">0</span><span class="p">,:],</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span><span class="n">opacity</span><span class="o">=</span><span class="n">opacity</span><span class="p">,</span>
                             <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;lines+markers&quot;</span><span class="p">,</span>
                             <span class="n">line</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">2</span><span class="p">)),</span>
                 <span class="p">)</span>



<span class="n">fig</span><span class="o">.</span><span class="n">update_layout</span><span class="p">(</span><span class="n">legend</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span><span class="n">height</span><span class="o">=</span><span class="mi">800</span><span class="p">,</span><span class="n">width</span><span class="o">=</span><span class="mi">915</span><span class="p">,</span><span class="n">xaxis_title</span><span class="o">=</span><span class="s2">&quot;Weight 1&quot;</span><span class="p">,</span> <span class="n">yaxis_title</span><span class="o">=</span><span class="s2">&quot;Weight 2&quot;</span><span class="p">,</span>
                 <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Contour Plot of Error&quot;</span><span class="p">)</span>

<span class="n">fig</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>
<p>Here are a few important points to note:
1. The step size increases as we go along the right gradient but decreases and eventually stops and changes once we are not moving along the gradient as if the weights cannot stop and have a certain <em>momentum</em> like a block of mass.</p>
<ol>
<li>
<p>If we use the complete dataset as the batch, then, without momentum, it will take the best steps along the steepest gradient to the minima(local or global). But with momentum, it wanders off a little. It may even reach the minimum(local ot global) and yet go past it a bit(like a ball with momentum would). If it was a local minima, it may just cross the barrier and move to global minima. If it was a global minima, then it would oscillate a little and eventually stop.</p>
</li>
<li>
<p>For mini batch, it will depend on the batch size. If the batch size is too small, then it will act as it does for SGD,(i.e smoothen the weight updates) and if the batch size is too big, then it will act as it does for complete batch(i.e, deviate and move past minima). The batch size will decide the degree of it.</p>
</li>
</ol>
<p>Our XOR data has probably a local minima(or a saddle point) which does not allow all the models to converge at the global minimum. Some initializations get stuck at a local minima. Here is one. try changing the epochs and learning rates, it is still very hard to get a very low error score. So we can improve it by the momentum. With the right momentum and the correct learning rate, we can get past this minima and get a lower score in very few epochs.</p>
<div class="highlight"><pre><span></span><code><span class="n">simple_model</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">()</span>
<span class="n">simple_model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">XOR_inp</span><span class="p">,</span><span class="n">XOR_target</span><span class="p">,</span><span class="mi">30001</span><span class="p">,</span><span class="n">layer_sizes</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,),</span><span class="n">learning_rate</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">365</span><span class="p">)</span>
</code></pre></div>
<pre><code>Error: 0.1250841346167755
</code></pre>
<div class="highlight"><pre><span></span><code><span class="n">mom_model</span> <span class="o">=</span> <span class="n">MLP_momentum</span><span class="p">()</span>
<span class="n">mom_model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">XOR_inp</span><span class="p">,</span><span class="n">XOR_target</span><span class="p">,</span><span class="mi">30001</span><span class="p">,</span><span class="n">layer_sizes</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,),</span><span class="n">learning_rate</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">365</span><span class="p">,</span><span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
</code></pre></div>
<pre><code>3.434487809294881e-05
</code></pre>
<p>Another thing we try is called <strong>Weight Decay</strong>. The argument goes as small weights try to keep the outputs in the linear middle part of the sigmoid which change faster than at the end points(because of the gradient). It is done by multiplying all the weights by a small constant \(\epsilon\) after each iteraton. It makes the network simpler and  may improve the results, but it can also sometimes make the learning significantly worse. Choosing \(\epsilon\) is done experimentally.</p>
<p>We will discuss more optimizations in detail later on.</p>
<h2 id="35-misc">3.5 Misc</h2>
<h5 id="351-amount-of-training-data">3.5.1 Amount of Training Data</h5>
<p>Since there are many parameters(weights) to look for, there actually should be sufficient data to learn that. The amont of data actually depends on the problem, but a rule of thumb is to use <strong>data 10 times the number of weights</strong>. </p>
<h5 id="352-number-of-hidden-layers-and-nodes">3.5.2 Number of Hidden layers and Nodes</h5>
<p>These are two important decisions needed to be made to get a good result. How many hidden layers to use and how many nodes in each layer? Although there is no theory for number of nodes but according to <strong>Universal Approximation Theorem</strong>, just one hidden layer will do just fine. A combination of sigmoid layers can approxiamte to any function with the sufficient number of nodes. However that one layer might need a lot of hidden nodes, so two hidden layers are usually used to be safe. The two hidden layer systemwill work for most of the problems but there are always exceptions.</p>
<p>We will talk more about the <strong>Universal Approximation Theorem</strong> later on.</p>
<h5 id="352-when-to-stop-training">3.5.2 When to Stop Training?</h5>
<p>This is also an important factor deciding the results. We do not want to overfit the data nor to we want to underfit it. So it is better to use a validation data to check. We will continue to train untill both the training and validation error are reducing. At the point the validation error starts increasing, we stop the training.</p>
<p><img alt="" src="../Files/Pictures/stop_train.png" /></p>
<p>This technique is called <strong>early stopping</strong>.</p>
<h1 id="4-mlp-in-practice">4 MLP in Practice</h1>
<p>Let's see some examples how MLP can be used. </p>
<h3 id="41-a-regression-problem">4.1 A Regression Problem</h3>
<p>Let's try fitting a sine wave with a small noise as well.</p>
<div class="highlight"><pre><span></span><code><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">479</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">T</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">*</span><span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="mi">4</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">*</span><span class="n">X</span><span class="p">)</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="mf">0.4</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="c1"># plot the data</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">go</span><span class="o">.</span><span class="n">Figure</span><span class="p">(</span><span class="n">go</span><span class="o">.</span><span class="n">Scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span><span class="n">y</span><span class="o">=</span><span class="n">T</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;markers&quot;</span><span class="p">),</span>
                <span class="n">layout</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">xaxis_title</span><span class="o">=</span><span class="s2">&quot;Input&quot;</span><span class="p">,</span> <span class="n">yaxis_title</span><span class="o">=</span><span class="s2">&quot;Target Values&quot;</span><span class="p">)</span>
               <span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>
<p>Let's now train an MLP on this data. But before that we will normalize the data and split it into training validation and test data in the ratio 4:1:1.</p>
<div class="highlight"><pre><span></span><code><span class="n">X</span> <span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="o">-</span><span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span><span class="o">/</span><span class="n">X</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">T</span> <span class="o">=</span><span class="p">(</span><span class="n">T</span><span class="o">-</span><span class="n">T</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span><span class="o">/</span><span class="n">T</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
<span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">[</span><span class="n">idx</span><span class="p">,:]</span>
<span class="n">T</span><span class="o">=</span><span class="n">T</span><span class="p">[</span><span class="n">idx</span><span class="p">,:]</span>
<span class="n">train</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">//</span><span class="mi">3</span><span class="p">),:]</span>
<span class="n">test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">//</span><span class="mi">3</span><span class="p">):</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">//</span><span class="mi">3</span><span class="p">)</span><span class="o">+</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">//</span><span class="mi">6</span><span class="p">):,:]</span>
<span class="n">val</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">//</span><span class="mi">3</span><span class="p">)</span><span class="o">+</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">//</span><span class="mi">6</span><span class="p">):,:]</span>
<span class="n">traintarget</span> <span class="o">=</span> <span class="n">T</span><span class="p">[:</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">//</span><span class="mi">3</span><span class="p">),:]</span>
<span class="n">testtarget</span> <span class="o">=</span> <span class="n">T</span><span class="p">[</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">//</span><span class="mi">3</span><span class="p">):</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">//</span><span class="mi">3</span><span class="p">)</span><span class="o">+</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">//</span><span class="mi">6</span><span class="p">):,:]</span>
<span class="n">valtarget</span> <span class="o">=</span> <span class="n">T</span><span class="p">[</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">//</span><span class="mi">3</span><span class="p">)</span><span class="o">+</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">//</span><span class="mi">6</span><span class="p">):,:]</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="c1"># plot the data</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">go</span><span class="o">.</span><span class="n">Figure</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="p">[</span><span class="n">go</span><span class="o">.</span><span class="n">Scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">train</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span><span class="n">y</span><span class="o">=</span><span class="n">traintarget</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span><span class="n">mode</span><span class="o">=</span><span class="s2">&quot;markers&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;training set&quot;</span><span class="p">),</span>
                     <span class="n">go</span><span class="o">.</span><span class="n">Scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">val</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span><span class="n">y</span><span class="o">=</span><span class="n">valtarget</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span><span class="n">mode</span><span class="o">=</span><span class="s2">&quot;markers&quot;</span><span class="p">,</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;validation set&quot;</span><span class="p">)],</span>
                <span class="n">layout</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">xaxis_title</span><span class="o">=</span><span class="s2">&quot;Input&quot;</span><span class="p">,</span> <span class="n">yaxis_title</span><span class="o">=</span><span class="s2">&quot;Target Values&quot;</span><span class="p">)</span>
               <span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>
<p>To start with, we will use one hidden layer, with 3 nodes and learning rate of 0.25 for 101 epochs and see what the output is.</p>
<div class="highlight"><pre><span></span><code><span class="n">m</span> <span class="o">=</span> <span class="n">MLP_momentum</span><span class="p">()</span>
<span class="n">m</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">train</span><span class="p">,</span><span class="n">traintarget</span><span class="p">,</span><span class="mi">101</span><span class="p">,</span><span class="n">layer_sizes</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,),</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span><span class="n">is_regression</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span><span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
</code></pre></div>
<pre><code>epoch: 0
Error: 0.9954347804927682
epoch: 11
Error: 0.695684508117699
epoch: 22
Error: 0.6610644383959196
epoch: 33
Error: 0.6622483713072506
epoch: 44
Error: 0.652190286530414
epoch: 55
Error: 0.646344230816324
epoch: 66
Error: 0.6426767619901053
epoch: 77
Error: 0.63862336612565
epoch: 88
Error: 0.6343649929165276
epoch: 99
Error: 0.6300130633180782





0.6296088646689288
</code></pre>
<p>As we can see, the error is decreasing, now we have to figure out a couple of things. First figure out the early stopping. We will keep track of last two validation errors after every 10 epochs to make sure the validation error actually increases(when it does!) and do not stop the program prematurely.</p>
<div class="highlight"><pre><span></span><code><span class="n">train_error</span><span class="o">=</span><span class="p">[]</span>
<span class="n">val_error</span><span class="o">=</span><span class="p">[]</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">MLP_momentum</span><span class="p">()</span>
<span class="n">m</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">train</span><span class="p">,</span><span class="n">traintarget</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="n">layer_sizes</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,),</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span><span class="n">is_regression</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">old_val_error1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">infty</span>
<span class="n">old_val_error2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">infty</span>
<span class="n">new_error</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">val</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">val</span><span class="p">,</span><span class="n">is_regression</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">-</span><span class="n">valtarget</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="n">val_error</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_error</span><span class="p">)</span>
<span class="n">train_error</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="mi">1</span><span class="o">/</span><span class="n">train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">train</span><span class="p">,</span><span class="n">is_regression</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">-</span><span class="n">traintarget</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
<span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">while</span> <span class="p">(((</span><span class="n">old_val_error1</span><span class="o">-</span><span class="n">new_error</span><span class="p">)</span><span class="o">&gt;</span><span class="mf">0.0005</span><span class="p">)</span> <span class="ow">or</span> <span class="p">((</span><span class="n">old_val_error2</span><span class="o">-</span><span class="n">old_val_error1</span><span class="p">)</span><span class="o">&gt;</span><span class="mf">0.0005</span><span class="p">)):</span>
    <span class="n">count</span><span class="o">+=</span><span class="mi">1</span>
    <span class="n">m</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">train</span><span class="p">,</span><span class="n">traintarget</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="n">layer_sizes</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,),</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span><span class="n">is_regression</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">momentum</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">warm_start</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">old_val_error2</span><span class="o">=</span><span class="n">old_val_error1</span>
    <span class="n">old_val_error1</span><span class="o">=</span><span class="n">new_error</span>
    <span class="n">new_error</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">val</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">val</span><span class="p">,</span><span class="n">is_regression</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">-</span><span class="n">valtarget</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">val_error</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_error</span><span class="p">)</span>
    <span class="n">train_error</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="mi">1</span><span class="o">/</span><span class="n">train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">train</span><span class="p">,</span><span class="n">is_regression</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">-</span><span class="n">traintarget</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;learning_stopped at epoch:&quot;</span><span class="p">,</span> <span class="n">count</span><span class="o">*</span><span class="mi">100</span><span class="p">)</span>
<span class="n">extra_train_error</span><span class="o">=</span><span class="p">[]</span>
<span class="n">extra_val_error</span><span class="o">=</span><span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">m</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">train</span><span class="p">,</span><span class="n">traintarget</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="n">layer_sizes</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,),</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span><span class="n">is_regression</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span><span class="n">warm_start</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">old_val_error2</span><span class="o">=</span><span class="n">old_val_error1</span>
    <span class="n">old_val_error1</span><span class="o">=</span><span class="n">new_error</span>
    <span class="n">new_error</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">val</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">val</span><span class="p">,</span><span class="n">is_regression</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">-</span><span class="n">valtarget</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">extra_val_error</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_error</span><span class="p">)</span>
    <span class="n">extra_train_error</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="mi">1</span><span class="o">/</span><span class="n">train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">train</span><span class="p">,</span><span class="n">is_regression</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">-</span><span class="n">traintarget</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
</code></pre></div>
<pre><code>learning_stopped at epoch: 1600
</code></pre>
<div class="highlight"><pre><span></span><code><span class="n">fig</span> <span class="o">=</span> <span class="n">go</span><span class="o">.</span><span class="n">Figure</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="p">[</span>
    <span class="n">go</span><span class="o">.</span><span class="n">Scatter</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">train_error</span><span class="p">,</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;train error before early stopping&quot;</span><span class="p">),</span>
    <span class="n">go</span><span class="o">.</span><span class="n">Scatter</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">val_error</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;validation error before early stoppping&quot;</span><span class="p">),</span>
    <span class="n">go</span><span class="o">.</span><span class="n">Scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">train_error</span><span class="p">),</span><span class="nb">len</span><span class="p">(</span><span class="n">train_error</span><span class="p">)</span><span class="o">+</span><span class="nb">len</span><span class="p">(</span><span class="n">extra_train_error</span><span class="p">))),</span><span class="n">y</span><span class="o">=</span><span class="n">extra_train_error</span><span class="p">,</span>
               <span class="n">name</span><span class="o">=</span> <span class="s2">&quot;train error after early stopping&quot;</span><span class="p">),</span>
    <span class="n">go</span><span class="o">.</span><span class="n">Scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">val_error</span><span class="p">),</span><span class="nb">len</span><span class="p">(</span><span class="n">val_error</span><span class="p">)</span><span class="o">+</span><span class="nb">len</span><span class="p">(</span><span class="n">extra_val_error</span><span class="p">))),</span><span class="n">y</span><span class="o">=</span><span class="n">extra_val_error</span><span class="p">,</span>
               <span class="n">name</span><span class="o">=</span> <span class="s2">&quot;validation error after early stopping&quot;</span><span class="p">),</span>    
<span class="p">],</span>
               <span class="n">layout</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">xaxis_title</span><span class="o">=</span><span class="s2">&quot;Epochs x100&quot;</span><span class="p">,</span> <span class="n">yaxis_title</span><span class="o">=</span><span class="s2">&quot;Sum of Squares Error&quot;</span><span class="p">))</span>
<span class="n">fig</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>
<p>As you can see there is not much change in the error after the early stopping point. We have found the for how long the network to be run but now let's figure how many hidden nodes to be used.</p>
<p>We already have a early stopping function and we will run this for different sizes of hidden nodes and record the results. However we will run each model 10 times with different initialization and work on averages.</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">early_stopping</span><span class="p">(</span><span class="n">train</span><span class="p">,</span><span class="n">traintarget</span><span class="p">,</span><span class="n">val</span><span class="p">,</span><span class="n">valtarget</span><span class="p">,</span><span class="n">epochs</span><span class="p">,</span><span class="n">layer_sizes</span><span class="p">,</span><span class="n">learning_rate</span><span class="p">,</span>
                   <span class="n">is_regression</span><span class="p">,</span><span class="n">momentum</span><span class="p">,</span><span class="n">random_state</span><span class="p">,</span><span class="n">diff</span><span class="o">=</span><span class="mf">0.001</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">MLP_momentum</span><span class="p">()</span>
    <span class="n">error</span><span class="o">=</span><span class="n">m</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">train</span><span class="p">,</span><span class="n">traintarget</span><span class="p">,</span><span class="n">epochs</span><span class="p">,</span><span class="n">layer_sizes</span><span class="o">=</span><span class="n">layer_sizes</span><span class="p">,</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
            <span class="n">is_regression</span><span class="o">=</span><span class="n">is_regression</span><span class="p">,</span><span class="n">momentum</span><span class="o">=</span><span class="n">momentum</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)</span>

    <span class="n">old_val_error1</span> <span class="o">=</span> <span class="mi">2000000000</span>
    <span class="n">old_val_error2</span> <span class="o">=</span> <span class="mi">1999999999</span>
    <span class="n">new_error</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">val</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">val</span><span class="p">,</span><span class="n">is_regression</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">-</span><span class="n">valtarget</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

    <span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="p">(((</span><span class="n">old_val_error1</span><span class="o">-</span><span class="n">new_error</span><span class="p">)</span><span class="o">&gt;</span><span class="n">diff</span><span class="p">)</span> <span class="ow">or</span> <span class="p">((</span><span class="n">old_val_error2</span><span class="o">-</span><span class="n">old_val_error1</span><span class="p">)</span><span class="o">&gt;</span><span class="n">diff</span><span class="p">)):</span>
        <span class="n">count</span><span class="o">+=</span><span class="n">epochs</span>
        <span class="n">error</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">train</span><span class="p">,</span><span class="n">traintarget</span><span class="p">,</span><span class="n">epochs</span><span class="p">,</span><span class="n">layer_sizes</span><span class="o">=</span><span class="n">layer_sizes</span><span class="p">,</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
            <span class="n">is_regression</span><span class="o">=</span><span class="n">is_regression</span><span class="p">,</span><span class="n">momentum</span><span class="o">=</span><span class="n">momentum</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span><span class="n">warm_start</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="n">old_val_error2</span><span class="o">=</span><span class="n">old_val_error1</span>
        <span class="n">old_val_error1</span><span class="o">=</span><span class="n">new_error</span>
        <span class="n">new_error</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">val</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">val</span><span class="p">,</span><span class="n">is_regression</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">-</span><span class="n">valtarget</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">m</span><span class="p">,</span> <span class="n">new_error</span>
</code></pre></div>
<p>We will save error for every differnt initialization and work on averages, standard deviations, minimums and maximums. We will use a pandas dataframe to form a table.</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">hidden_units</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">25</span><span class="p">]</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Mean Error&quot;</span><span class="p">,</span><span class="s2">&quot;Standard Deviation&quot;</span><span class="p">,</span> <span class="s2">&quot;Max Error&quot;</span><span class="p">,</span> <span class="s2">&quot;Min Error&quot;</span><span class="p">])</span>
<span class="k">for</span> <span class="n">unit</span> <span class="ow">in</span> <span class="n">hidden_units</span><span class="p">:</span>
    <span class="n">errors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">early_stopping</span><span class="p">(</span><span class="n">train</span><span class="p">,</span><span class="n">traintarget</span><span class="p">,</span><span class="n">val</span><span class="p">,</span><span class="n">valtarget</span><span class="p">,</span><span class="mi">10</span><span class="p">,(</span><span class="n">unit</span><span class="p">,),</span><span class="mf">0.25</span><span class="p">,</span><span class="kc">True</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span>
                                      <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">12000</span><span class="p">))[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)])</span>
    <span class="n">df</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">unit</span><span class="p">)]</span><span class="o">=</span><span class="p">[</span><span class="n">errors</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span><span class="n">errors</span><span class="o">.</span><span class="n">std</span><span class="p">(),</span><span class="n">errors</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span><span class="n">errors</span><span class="o">.</span><span class="n">min</span><span class="p">()]</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Units done: </span><span class="si">{</span><span class="n">unit</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>
<pre><code>Units done: 1
Units done: 2
Units done: 3
Units done: 5
Units done: 10
Units done: 25
</code></pre>
<div class="highlight"><pre><span></span><code><span class="n">df</span>
</code></pre></div>
<pre><code>|   | 1 | 2 | 3 | 5 | 10 | 25 |
|---|---|---|---|---|---|---|
|Mean Error |0.423110  |0.337027  |0.221558  |0.203000  |0.168734  |1.118023  |
|Standard Deviation  |0.000311  |0.126717  |0.117290  |0.105529  |0.085867  |1.072914  |
|Max Error  |0.423543  |0.429690  |0.405281  |0.421871  |0.422707  |3.377288  |
|Min Error  |0.422477  |0.141333  |0.139736  |0.145992  |0.122757  |0.244395  |
</code></pre>
<p>Take a look at both the mean(or max) error and standard deviation of each model with their hidden nodes. We prefer the model with low error (mean and max) and low standard deviation. We can also change learning rate and test out the results. Also we can try more layers and optimize the nodes in each layer.</p>
<h2 id="final-code">Final Code</h2>
<p>Before starting the next part we have to add a bunch of stuff to our code, implementing the cross-entropy error. We will also include the early stopping method with it. We will also fix the confusion matrix for multiple classes.</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">MLP</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">init_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">layer_sizes</span><span class="p">,</span><span class="n">random_state</span><span class="p">):</span>
        <span class="c1">#save weights in a list of matrices</span>
        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">layer_sizes</span><span class="p">[</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="n">layer_sizes</span><span class="p">[</span><span class="n">l</span><span class="p">])</span><span class="o">*</span><span class="p">(</span><span class="mi">2</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">layer_sizes</span><span class="p">[</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span><span class="o">-</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">layer_sizes</span><span class="p">[</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">layer_sizes</span><span class="p">))]</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span> <span class="c1"># keep beta = 1</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">A_0</span><span class="p">,</span><span class="n">is_regression</span><span class="p">,</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="o">=</span><span class="p">[]</span>
        <span class="n">A_l</span> <span class="o">=</span> <span class="n">A_0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">A_0</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">weights</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span>
        <span class="k">for</span> <span class="n">weight</span> <span class="ow">in</span> <span class="n">weights</span><span class="p">:</span>
            <span class="n">A_lbias</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(((</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">A_l</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">1</span><span class="p">)),</span><span class="n">A_l</span><span class="p">)),</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># add bias to input data</span>
            <span class="n">H_l</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">A_lbias</span><span class="p">,</span><span class="n">weight</span><span class="p">)</span> <span class="c1"># compute the summation</span>
            <span class="n">A_l</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">H_l</span><span class="p">)</span> <span class="c1"># compute the activation</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">A_l</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">is_regression</span><span class="p">:</span>
            <span class="n">A_l</span> <span class="o">=</span> <span class="n">H_l</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">A_l</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">A_l</span> <span class="c1"># return the final output</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">T</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span><span class="n">is_regression</span><span class="p">,</span><span class="n">batch_size</span><span class="p">,</span><span class="n">momentum</span><span class="p">,</span><span class="n">delta_w</span><span class="p">,</span><span class="n">loss</span><span class="p">):</span>
        <span class="n">A_L</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">is_regression</span><span class="p">:</span>
            <span class="n">delta_L</span> <span class="o">=</span> <span class="n">A_L</span><span class="o">-</span><span class="n">T</span>
        <span class="k">elif</span> <span class="n">loss</span> <span class="o">==</span> <span class="s1">&#39;sumofsquares&#39;</span><span class="p">:</span>
            <span class="n">delta_L</span> <span class="o">=</span> <span class="p">(</span><span class="n">A_L</span><span class="o">-</span><span class="n">T</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">A_L</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">A_L</span><span class="p">))</span> <span class="c1"># beta = 0</span>
        <span class="k">elif</span> <span class="n">loss</span> <span class="o">==</span> <span class="s1">&#39;crossentropy&#39;</span><span class="p">:</span>
<span class="c1">#             delta_L = (T*(A_L-1)*A_L)</span>
            <span class="n">delta_L</span><span class="o">=</span> <span class="n">A_L</span><span class="o">-</span><span class="n">T</span>
        <span class="n">delta_l_next</span> <span class="o">=</span> <span class="n">delta_L</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">A_l</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="c1">#compute error for previous layer</span>
            <span class="n">delta_l</span> <span class="o">=</span> <span class="n">A_l</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">A_l</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">delta_l_next</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">:,:])))</span>

            <span class="c1"># add bias output to output matrix</span>
            <span class="n">A_lbias</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(((</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">A_l</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">1</span><span class="p">)),</span><span class="n">A_l</span><span class="p">)),</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

            <span class="c1"># compute delta_w</span>
            <span class="n">delta_w</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">momentum</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">batch_size</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">A_lbias</span><span class="p">),</span><span class="n">delta_l_next</span><span class="p">))</span> <span class="o">+</span> <span class="n">momentum</span><span class="o">*</span><span class="n">delta_w</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>


            <span class="c1">#update weights using the next errors</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">learning_rate</span><span class="o">*</span><span class="n">delta_w</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

            <span class="c1"># change the next errors for next layer</span>
            <span class="n">delta_l_next</span> <span class="o">=</span> <span class="n">delta_l</span>
        <span class="k">return</span> <span class="n">delta_w</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">compute_error</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">A_L</span><span class="p">,</span><span class="n">T</span><span class="p">,</span><span class="n">loss</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">loss</span> <span class="o">==</span> <span class="s1">&#39;sumofsquares&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">A_L</span><span class="o">-</span><span class="n">T</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="n">T</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">loss</span> <span class="o">==</span> <span class="s1">&#39;crossentropy&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">T</span><span class="o">*</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">A_L</span><span class="p">))</span><span class="o">+</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">T</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">A_L</span><span class="p">)))</span><span class="o">/</span><span class="n">T</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>




    <span class="k">def</span><span class="w"> </span><span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">,</span> <span class="n">input_target</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">layer_sizes</span><span class="o">=</span><span class="p">(</span><span class="mi">100</span><span class="p">,),</span> 
              <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;sumofsquares&#39;</span><span class="p">,</span> <span class="n">is_regression</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
              <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">save_weights</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">warm_start</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>


        <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>
        <span class="n">Target</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">input_target</span><span class="p">)</span>
        <span class="n">layer_sizes</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">layer_sizes</span><span class="p">)</span>
        <span class="n">layer_sizes</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">n_outputs</span> <span class="o">=</span> <span class="n">Target</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">layer_sizes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">n_outputs</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">warm_start</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">init_weights</span><span class="p">(</span><span class="n">layer_sizes</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">save_weights</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">saved_weights</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">copy</span><span class="p">()]</span>

        <span class="k">if</span> <span class="n">batch_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># initialize delta_w to be zero for every layer</span>
        <span class="n">delta_w</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="o">*</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>




            <span class="c1"># shuffle the input so we don&#39;t train on same sequences</span>
            <span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">Target</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
            <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
            <span class="n">Target</span><span class="o">=</span><span class="n">Target</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>

            <span class="n">b</span><span class="o">=</span><span class="mi">0</span>
            <span class="k">while</span> <span class="n">b</span><span class="o">&lt;</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
                <span class="n">A_0</span><span class="o">=</span><span class="n">X</span><span class="p">[</span><span class="n">b</span><span class="p">:</span><span class="n">b</span><span class="o">+</span><span class="n">batch_size</span><span class="p">,:]</span>
                <span class="n">T</span><span class="o">=</span><span class="n">Target</span><span class="p">[</span><span class="n">b</span><span class="p">:</span><span class="n">b</span><span class="o">+</span><span class="n">batch_size</span><span class="p">,:]</span>
                <span class="n">A_L</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">A_0</span><span class="p">,</span><span class="n">is_regression</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">e</span><span class="o">%</span><span class="p">((</span><span class="n">epochs</span><span class="o">//</span><span class="mi">10</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">verbose</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;epoch:&quot;</span><span class="p">,</span><span class="n">e</span><span class="p">)</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">compute_error</span><span class="p">(</span><span class="n">A_L</span><span class="p">,</span><span class="n">T</span><span class="p">,</span><span class="n">loss</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="n">delta_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">T</span><span class="p">,</span><span class="n">learning_rate</span><span class="p">,</span><span class="n">is_regression</span><span class="p">,</span><span class="n">batch_size</span><span class="p">,</span><span class="n">momentum</span><span class="p">,</span><span class="n">delta_w</span><span class="p">,</span><span class="n">loss</span><span class="p">)</span>


                <span class="k">if</span> <span class="n">save_weights</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">saved_weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>
                <span class="n">b</span><span class="o">=</span><span class="n">b</span><span class="o">+</span><span class="n">batch_size</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_error</span><span class="p">(</span><span class="n">A_L</span><span class="p">,</span><span class="n">T</span><span class="p">,</span><span class="n">loss</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">early_stopping</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">train</span><span class="p">,</span><span class="n">traintarget</span><span class="p">,</span><span class="n">val</span><span class="p">,</span><span class="n">valtarget</span><span class="p">,</span><span class="n">epochs</span><span class="p">,</span><span class="n">layer_sizes</span><span class="p">,</span><span class="n">learning_rate</span><span class="p">,</span>
                       <span class="n">is_regression</span><span class="p">,</span><span class="n">momentum</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">diff</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;sumofsquares&#39;</span><span class="p">):</span>
        <span class="n">error</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">train</span><span class="p">,</span><span class="n">traintarget</span><span class="p">,</span><span class="n">epochs</span><span class="p">,</span><span class="n">layer_sizes</span><span class="o">=</span><span class="n">layer_sizes</span><span class="p">,</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
                           <span class="n">is_regression</span><span class="o">=</span><span class="n">is_regression</span><span class="p">,</span><span class="n">momentum</span><span class="o">=</span><span class="n">momentum</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                           <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)</span>
        <span class="n">old_val_error1</span> <span class="o">=</span> <span class="mi">2000000000</span>
        <span class="n">old_val_error2</span> <span class="o">=</span> <span class="mi">1999999999</span>
        <span class="n">new_error</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_error</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">val</span><span class="p">,</span><span class="n">is_regression</span><span class="o">=</span><span class="n">is_regression</span><span class="p">),</span><span class="n">valtarget</span><span class="p">,</span><span class="n">loss</span><span class="p">)</span>

        <span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">while</span> <span class="p">(((</span><span class="n">old_val_error1</span><span class="o">-</span><span class="n">new_error</span><span class="p">)</span><span class="o">&gt;</span><span class="n">diff</span><span class="p">)</span> <span class="ow">or</span> <span class="p">((</span><span class="n">old_val_error2</span><span class="o">-</span><span class="n">old_val_error1</span><span class="p">)</span><span class="o">&gt;</span><span class="n">diff</span><span class="p">)):</span>
            <span class="n">count</span><span class="o">+=</span><span class="n">epochs</span>
            <span class="n">error</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">train</span><span class="p">,</span><span class="n">traintarget</span><span class="p">,</span><span class="n">epochs</span><span class="p">,</span><span class="n">layer_sizes</span><span class="o">=</span><span class="n">layer_sizes</span><span class="p">,</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
                <span class="n">is_regression</span><span class="o">=</span><span class="n">is_regression</span><span class="p">,</span><span class="n">momentum</span><span class="o">=</span><span class="n">momentum</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span><span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span><span class="n">warm_start</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

            <span class="n">old_val_error2</span><span class="o">=</span><span class="n">old_val_error1</span>
            <span class="n">old_val_error1</span><span class="o">=</span><span class="n">new_error</span>
            <span class="n">new_error</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_error</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">val</span><span class="p">,</span><span class="n">is_regression</span><span class="o">=</span><span class="n">is_regression</span><span class="p">),</span><span class="n">valtarget</span><span class="p">,</span><span class="n">loss</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">new_error</span>


    <span class="k">def</span><span class="w"> </span><span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">input_data</span><span class="p">,</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">input_data</span><span class="p">),</span><span class="n">weights</span><span class="p">)</span>
        <span class="c1">#since this output is a realnumber(between 0 &amp; 1)</span>
        <span class="c1"># we will have a threshold to predict its class for now 0.5</span>
        <span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">output</span><span class="o">&gt;</span><span class="mf">0.5</span><span class="p">)</span><span class="o">*</span><span class="mi">1</span>
        <span class="k">return</span> <span class="n">output</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">confmat</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">input_data</span><span class="p">,</span><span class="n">targets</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&#39;&#39;&#39;returns the confusion matrix for binary classification&#39;&#39;&#39;</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span><span class="n">is_regression</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">targets</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">t</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span><span class="o">*</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
                <span class="n">mat</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">out</span><span class="o">==</span><span class="n">i</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">t</span><span class="o">==</span><span class="n">j</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">mat</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">mat</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">mat</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">mat</span><span class="p">)</span>
</code></pre></div>
<p>Let's save this model in our <a href="/downloads/utils.py">utils.py</a> file.</p>
<h3 id="42-a-classification-problem">4.2 A Classification Problem</h3>
<p>There are multiple ways to approach a classification problem. The inputs are the features normalized and there are a couple of choices for outputs.</p>
<ol>
<li>The first is to use a single linear node and then put some threshold for different outputs. For example, for a 4 class problem we can use:</li>
</ol>
<p>$$
C = \begin{cases}
C_1 &amp; \text{if }  y \leq -0.5 \
C_2 &amp; \text{if } -0.5 &lt; y \leq 0 \
C_3 &amp; \text{if } 0 &lt; y \leq 0.5 \
C_4 &amp; \text{if } y &gt; 0.5
\end{cases}
$$
But this gets impractical when the number of classes gets large and the boundaries are artificial; say what about the output 0.5? We classify it as \(C_3\), but the network gives us no information that how hard this example was to classify.</p>
<ol>
<li>A more suitable way is to use output is called <strong>\(1\text{-of-}N\) encoding</strong>. A separate node is used to represent each posiible class and the target vector consists of zeros everywhere except for the class we are representing, e.g (0,0,1,0,0) means the third class out of 5. We are using binary output for each output node.</li>
</ol>
<p>We can use one of two ways to select a class. After all the output neurons have given their sigmoidal outputs, we can now choose the neuron to be 1 which has highest value and all others are Zero. This makes the result unambigious, since it is highly unlikely that two neurons have same values and they both happen to be the maximum values. This is called <strong>Hard Max</strong> method. We can also use the <strong>Soft Max</strong> version, where we first scale the the outputs to be comparable to each other and the sum also adds up to 1. It gives us the probability of that class(more or less!). So if there is a clear winner that neuron will be close to 1, otherwise if there are other similar values they will each have a value of \(\frac{1}{p}\), where \(p\) is the number of output neurons that have similar values.</p>
<p>There is another problem with the classification process. It is called class imablance. If we have, say, two class and we have 90% of training data for class 1, then the classifier will learn to always predict class 1, irrespective of the data. It will surely give it a 90% accuracy, but it is  a bad classifier. All the training examples from each classes should almost same. We may randomly throw off some data from the class which has larger data. There is another method called as <strong>novelty detection</strong>, where we train the whole model on just the over-represented data and then just predict if the data is different or similar to the training data. We will cover it in detail later.  </p>
<p><strong>The Iris Dataset</strong>:
This data is concerned with classification of <em>iris</em> flower into different species. This data is already available in sklearn library. This data contains features of <em>'sepal length'</em>, <em>'sepal width'</em>, <em>'petal length'</em>, <em>'petal width'</em>. There are three targets, the species, viz, <em>'setosa'</em>, <em>'versicolor'</em>, <em>'virginica'</em>. These are encoded as 0,1,2 respectively.</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_iris</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">data</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()[</span><span class="s1">&#39;data&#39;</span><span class="p">]</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()[</span><span class="s1">&#39;target&#39;</span><span class="p">]</span>
</code></pre></div>
<p>Upon inspection, you will find that this data has equal number of datapoints from each class, so we don't have to discard any datapoints. Let's first split the data into train, test and validation. Let's keep half of the data for training, a quarter for validation and a quarter for testing, but first we will randomise the dataset. Also we need to convert the target into 1-of-N encoding.</p>
<div class="highlight"><pre><span></span><code><span class="n">encoded_target</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">3</span><span class="p">))</span>
<span class="n">encoded_target</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span><span class="n">target</span><span class="p">]</span><span class="o">=</span><span class="mi">1</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">idx</span><span class="p">,:]</span>
<span class="n">encoded_target</span> <span class="o">=</span> <span class="n">encoded_target</span><span class="p">[</span><span class="n">idx</span><span class="p">,:]</span>

<span class="n">train</span> <span class="o">=</span> <span class="n">data</span><span class="p">[::</span><span class="mi">2</span><span class="p">,:]</span>
<span class="n">traint</span> <span class="o">=</span> <span class="n">encoded_target</span><span class="p">[::</span><span class="mi">2</span><span class="p">,:]</span>

<span class="n">val</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="mi">1</span><span class="p">::</span><span class="mi">4</span><span class="p">,:]</span>
<span class="n">valt</span> <span class="o">=</span> <span class="n">encoded_target</span><span class="p">[</span><span class="mi">1</span><span class="p">::</span><span class="mi">4</span><span class="p">,:]</span>

<span class="n">test</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="mi">3</span><span class="p">::</span><span class="mi">4</span><span class="p">]</span>
<span class="n">testt</span> <span class="o">=</span> <span class="n">encoded_target</span><span class="p">[</span><span class="mi">3</span><span class="p">::</span><span class="mi">4</span><span class="p">,:]</span>
</code></pre></div>
<p>Let's normalize the data. Let's divide by the maximum instead of standard deviation. We will normalize every set with mean and max(or std) from the training data. </p>
<div class="highlight"><pre><span></span><code><span class="n">mean</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">maximum</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">std</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">train</span> <span class="o">=</span> <span class="p">(</span><span class="n">train</span><span class="o">-</span><span class="n">mean</span><span class="p">)</span><span class="o">/</span><span class="n">maximum</span>
<span class="n">val</span> <span class="o">=</span> <span class="p">(</span><span class="n">val</span><span class="o">-</span><span class="n">mean</span><span class="p">)</span><span class="o">/</span><span class="n">maximum</span>
<span class="n">test</span> <span class="o">=</span> <span class="p">(</span><span class="n">test</span><span class="o">-</span><span class="n">mean</span><span class="p">)</span><span class="o">/</span><span class="n">maximum</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">m</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">()</span>
<span class="n">m</span><span class="o">.</span><span class="n">early_stopping</span><span class="p">(</span><span class="n">train</span><span class="p">,</span><span class="n">traint</span><span class="p">,</span><span class="n">val</span><span class="p">,</span><span class="n">valt</span><span class="p">,</span><span class="mi">10</span><span class="p">,(</span><span class="mi">20</span><span class="p">,),</span><span class="mi">15</span><span class="p">,</span><span class="kc">False</span><span class="p">,</span><span class="mf">0.9</span><span class="p">,</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;crossentropy&#39;</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div>
<pre><code>0.0798786726307614
</code></pre>
<div class="highlight"><pre><span></span><code><span class="n">m</span><span class="o">.</span><span class="n">compute_error</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">train</span><span class="p">,</span><span class="kc">False</span><span class="p">),</span><span class="n">traint</span><span class="p">,</span><span class="s1">&#39;crossentropy&#39;</span><span class="p">)</span>
</code></pre></div>
<pre><code>0.18322617355694956
</code></pre>
<div class="highlight"><pre><span></span><code><span class="n">m</span><span class="o">.</span><span class="n">confmat</span><span class="p">(</span><span class="n">train</span><span class="p">,</span><span class="n">traint</span><span class="p">)</span>
</code></pre></div>
<pre><code>(array([[28.,  0.,  0.],
        [ 0., 23.,  1.],
        [ 0.,  2., 21.]]), 0.96)
</code></pre>
<div class="highlight"><pre><span></span><code><span class="n">m</span><span class="o">.</span><span class="n">confmat</span><span class="p">(</span><span class="n">val</span><span class="p">,</span><span class="n">valt</span><span class="p">)</span>
</code></pre></div>
<pre><code>(array([[14.,  0.,  0.],
        [ 0., 11.,  0.],
        [ 0.,  0., 13.]]), 1.0)
</code></pre>
<div class="highlight"><pre><span></span><code><span class="n">m</span><span class="o">.</span><span class="n">confmat</span><span class="p">(</span><span class="n">test</span><span class="p">,</span><span class="n">testt</span><span class="p">)</span>
</code></pre></div>
<pre><code>(array([[ 8.,  0.,  0.],
        [ 0., 13.,  0.],
        [ 0.,  1., 15.]]), 0.972972972972973)
</code></pre>
<p>The confusion matrices reveal great results, on test set as well. You may even choose to change the activation of output nodes. (try softmax!)</p>
<h3 id="43-time-series-problem">4.3 Time Series Problem</h3>
<p>Time Series data has a few problems:</p>
<ol>
<li>
<p>Even if there is some regularities in the data, it can appear many different scales. There are local variations, which hide the general trend.</p>
</li>
<li>
<p>How many datapoints of the past should we use to make a prediction? and at what intervals?</p>
</li>
</ol>
<p>This comes to a choice of \(\tau\) and \(k\) deciding the interval and the number respectively.</p>
<p>For example, if \(\tau=2\) and \(k=3\), then input data are elements 1,2,and 5 and 7 is the target. The next element is 2,4,6,and target is 8. Just make sure you don't use parameters such that data is picked systematically. e.g: If some feature is only visible at odd datapoints, then it will be completely missed in even ones.</p>
<p>After done it is a simple regression problem.</p>
<h3 id="364-the-auto-associative-network">3.6.4 The Auto-Associative Network</h3>
<p>This is an interesting use of neural networks. In this network the input and the targets are the same. It is also called <strong>auto-encoder</strong> at times. It looks useless at first, but if we have a small hidden layer(lesser nodes than input nodes), it forms a bottleneck condition, where the outer nodes (input and output) are equal to each other and more than the hidden ones. Once the network is trained to reproduce the the own result, the hidden outputs(activations) will represent a compressed version of the data. It reduces the dimensions. We can use it to compress the data and then use the second weights to regenerate the input again.</p>
<p>It can also be used to denoise images. Once a model is trained, it can be used to pass noisy images to return clearer images.</p>
<p>The hidden acivations, if they all have linear activations, will be the <strong>Principal Components</strong> of input data. <strong>Principal Component Analysis</strong> is a useful dimensionality reduction technique and will be discussed in detail later.</p>







  
  






                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2025 Murtaza Nazir
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../../..", "features": ["content.code.copy"], "search": "../../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
        <script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
      
    
  </body>
</html>