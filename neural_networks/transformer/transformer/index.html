
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Building AI one tiny piece at a time">
      
      
        <meta name="author" content="Murtaza Nazir">
      
      
        <link rel="canonical" href="https://themurtazanazir.github.io/neural_networks/transformer/transformer/">
      
      
        <link rel="prev" href="../../convolutional_neural_networks/convolutions/">
      
      
      
        <link rel="alternate" type="application/rss+xml" title="RSS feed" href="../../../feed_rss_created.xml">
        <link rel="alternate" type="application/rss+xml" title="RSS feed of updated content" href="../../../feed_rss_updated.xml">
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.23">
    
    
      
        <title>Transformer - murtaza</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.84d31ad4.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="brown" data-md-color-accent="deep-orange">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="murtaza" class="md-header__button md-logo" aria-label="murtaza" data-md-component="logo">
      
  <img src="../../../img/logo.svg" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            murtaza
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Transformer
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="murtaza" class="md-nav__button md-logo" aria-label="murtaza" data-md-component="logo">
      
  <img src="../../../img/logo.svg" alt="logo">

    </a>
    murtaza
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Linear Algebra
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Linear Algebra
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../linear_algebra/introduction/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../linear_algebra/vectors-linear-combinations/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Vectors & Linear Combinations
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../linear_algebra/vector-spaces-and-subspaces/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Vector Spaces & Subspaces
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Neural Networks
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Neural Networks
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2" >
        
          
          <label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Multi-Layer Perceptron
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2">
            <span class="md-nav__icon md-icon"></span>
            Multi-Layer Perceptron
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../multiayer-perceptron/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../multiayer-perceptron/perceptron/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Perceptron
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../multiayer-perceptron/perceptron-convergence-theorem/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Perceptron Convergence
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../multiayer-perceptron/multi-layer-perceptron/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Multi-Layer Perceptron
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../multiayer-perceptron/improvements-to-mlp/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Improvements to MLP
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_3" >
        
          
          <label class="md-nav__link" for="__nav_3_3" id="__nav_3_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Convolutional Neural Networks
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3">
            <span class="md-nav__icon md-icon"></span>
            Convolutional Neural Networks
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../convolutional_neural_networks/introduction/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../convolutional_neural_networks/convolutions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Convolutions
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4" checked>
        
          
          <label class="md-nav__link" for="__nav_3_4" id="__nav_3_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Transformer
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_4_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3_4">
            <span class="md-nav__icon md-icon"></span>
            Transformer
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Transformer
    
  </span>
  

      </a>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<p><a href="../../">← Neural Networks</a></p>
<p><img alt="" src="../files/transformer_poster.jpg" /></p>
<h1 id="transformer">Transformer</h1>
<h2 id="attention-is-all-you-need">Attention Is All You Need</h2>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
</code></pre></div>
<p>This paper introduced the first sequence to sequence architecture using only attention mechanism, called "The Transformer". </p>
<blockquote>
<p><em>To the best of our knowledge, however, the Transformer is the first transduction model relying
entirely on self-attention to compute representations of its input and output without using sequence aligned RNNs or convolution.</em></p>
</blockquote>
<h2 id="what-is-attention">What is Attention?</h2>
<p>Attention was first proposed in <a href="https://arxiv.org/abs/1409.0473">Neural Machine Translation by Jointly Learning to Align and Translate</a>. It was introduced as part of recurrent networks especially in an encoder-decoder situation.</p>
<h3 id="before-attention">Before Attention</h3>
<p>Let's take the example of Neural Machine Translation, a common way to achieve it was by a recurrent encoder which encodes the text in one language into a context vector and that context vector was passed onto the decoder to autoregresively decode the text in target language.</p>
<p>As per Bhadanau et al.2015,</p>
<blockquote>
<p><em>A potential issue with this encoder–decoder approach is that a neural network needs to be able to
compress all the necessary information of a source sentence into a fixed-length vector. This may
make it difficult for the neural network to cope with long sentences, especially those that are longer
than the sentences in the training corpus.</em></p>
</blockquote>
<p>The problem of compression loses information which may be vital at some timestep in decoder. A new method was introduced which instead of looking at this context vector, a soft-search system would determine which timesteps in the encoder are most relevant, i.e while decoding current timestep which timestep encodings should it <em>attend</em> most to.</p>
<p>Let's start formalizing it a bit,</p>
<p>An encoder reads the input sentence, a sequence of vectors <span class="arithmatex">\(\mathbf{x} = \begin{pmatrix}x_1 &amp; \cdots &amp; x_{T_x}\end{pmatrix}\)</span>, into a vector <span class="arithmatex">\(c\)</span>. The most common approach is to use an RNN such that</p>
<div class="arithmatex">\[
\begin{align*}
h_t &amp;= f(x_t, h_{t-1})\\
c &amp;= q(\{h_1, h_2, \dots, h_{T_x}\})
\end{align*}
\]</div>
<p>where <span class="arithmatex">\(h_t\)</span> is a hidden state at timestep <span class="arithmatex">\(t\)</span> and <span class="arithmatex">\(c\)</span> is the context vector passed onto the decoder. <span class="arithmatex">\(f\)</span> and <span class="arithmatex">\(q\)</span> are some non-linear functions. <span class="arithmatex">\(f\)</span> is usually a LSTM/GRU and <span class="arithmatex">\(q(\{h_1, h_2, \dots, h_{T_x}\}) = h_{T_x}\)</span>, i.e the last hidden state is chosen as the context vector.</p>
<p>The decoder is trained to predict the next timestep <span class="arithmatex">\(y_{t'}\)</span> given the previous outputs <span class="arithmatex">\(\{y_1, \dots, y_{t'-1}\}\)</span> and the context vector <span class="arithmatex">\(c\)</span>.</p>
<blockquote>
<p>In other words, the decoder defines a probability over
the translation <span class="arithmatex">\(\mathbf{y}\)</span> by decomposing the joint probability into the ordered conditionals:</p>
</blockquote>
<div class="arithmatex">\[p(\mathbf{y}) = \prod_{t=1}^T p(y_t \mid \{y_1, \dots, y_{t-1}\}, c)\]</div>
<p>where <span class="arithmatex">\(\mathbf{y} = \begin{pmatrix}y_1 &amp; y_2 &amp; \cdots &amp; y_{T_y}\end{pmatrix}\)</span>.</p>
<p>With a Recurrent Network, it can be modelled as:</p>
<div class="arithmatex">\[p(y_t \mid \{y_1, \dots, y_{t-1}\}, c) = g(y_{t-1}, s_t, c)\]</div>
<p>where <span class="arithmatex">\(g\)</span> is a non-linear funtion, <span class="arithmatex">\(s_t\)</span> is the hidden state of RNN.</p>
<h3 id="introduction">Introduction</h3>
<p>As we can see at the decoding of each time step the same context vector is being passed, which is a compressed vector of all hidden states in encoder. At each timestep the decoder's dependency on encoder output can vary. The major idea by Bhadanau et al.2015 was to introduce a dynamic context vector which changes for each timestep depending on what context is deemed necessary by the decoder at that moment. It should be able to pull from the raw hidden states.</p>
<p>The probability was redefined to depend on not a context vector but on the input space for the decoder, so</p>
<div class="arithmatex">\[p(y_i \mid \{y_1, \dots, y_{i-1}\}, \mathbf{x}) = g(y_{i-1}, s_i, c_i)\]</div>
<p>where <span class="arithmatex">\(s_i\)</span> is the hidden state of decoder. As we can see <span class="arithmatex">\(c\)</span> has been changed to <span class="arithmatex">\(c_i\)</span>. <span class="arithmatex">\(c\)</span> originally was a summarization of all hidden states of encoder i.e: <span class="arithmatex">\(c = q(\{h_1, h_2, \dots, h_{T_x}\})\)</span>.</p>
<p>Now <span class="arithmatex">\(c_i\)</span> is the weighted sum of all hidden states of encoder where the weights themselves are dynamic.</p>
<div class="arithmatex">\[c_i = \sum_{j=1}^{T_x} \alpha_{ij} h_j\]</div>
<p>The weight <span class="arithmatex">\(\alpha_{ij}\)</span> is a softmaxed version of underlying scores(because weights should sum to 1).</p>
<div class="arithmatex">\[\alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k=1}^{T_x} \exp(e_{ik})}\]</div>
<p>where <span class="arithmatex">\(e_{ij}\)</span>, called an <em>alignment model</em>, is how well the inputs around position <span class="arithmatex">\(j\)</span> and the outcome at position <span class="arithmatex">\(i\)</span> match. The score is based on the previous decoder hidden state <span class="arithmatex">\(s_{i-1}\)</span> and <span class="arithmatex">\(j\)</span> th hidden state of encoder.</p>
<div class="arithmatex">\[e_{ij} = a(s_{i-1}, h_j)\]</div>
<p>So while decoding we first compute all scores w.r.t to all encoder hidden states, softmax them and take the weighted average of encoder hidden states to form out context vector at that timestep.</p>
<p>Now depending on how <span class="arithmatex">\(a\)</span> is defined we can have multiple types of attentions. One simple way is to concat(or add) inputs and pass it through a feedforward network which can be trained jointly with the whole system, which is what bhadanau et al.2015 used.</p>
<p>By Bhadanau et at.2015:</p>
<blockquote>
<p>The probability <span class="arithmatex">\(\alpha_{ij}\)</span> , or its associated energy <span class="arithmatex">\(e_{ij}\)</span> , reflects the importance of the annotation <span class="arithmatex">\(h_j\)</span> with respect to the previous hidden state <span class="arithmatex">\(s_{i−1}\)</span> in deciding the next state <span class="arithmatex">\(s_i\)</span> and generating <span class="arithmatex">\(y_i\)</span>. Intuitively, this implements a mechanism of attention in the decoder. The decoder decides parts of the source sentence to pay attention to. By letting the decoder have an attention mechanism, we relieve the encoder from the burden of having to encode all information in the source sentence into a fixedlength vector. With this new approach the information can be spread throughout the sequence of annotations, which can be selectively retrieved by the decoder accordingly.</p>
</blockquote>
<h3 id="scaled-dot-product-attention">Scaled Dot Product Attention</h3>
<p>The attention mechanism introduced by Bhadanau et al.2015 is called <em>additive</em> or <em>concat</em> attention. Another form of attention is <em>dot product attention</em> where instead of using a feedforward network to compute an energy, a dot product is used. Dot product is also an indicator of how close two vectors can be. </p>
<p>The dot product attention is defined as:</p>
<div class="arithmatex">\[a(s_{i-1}, h_j) = s_{i-1}^\intercal h_j\]</div>
<p>Vaswani et. al2017, introduced the <em>scaled dot product attention</em>, which is defined as</p>
<div class="arithmatex">\[a(s_{i-1}, h_j) = \frac{s_{i-1}^\intercal h_j}{\sqrt{n}}\]</div>
<p>where <span class="arithmatex">\(n\)</span> is the dimension of <span class="arithmatex">\(h_j\)</span>.</p>
<p>The idea behind this scaling was that for very large <span class="arithmatex">\(n\)</span> dimenional vectors, the dot-product can be huge pushing the softmax into regions with very small gradients. So, the scaling is required.</p>
<p><code>Question: Explain how gradients will be small?</code></p>
<p>From Vaswani et .al2017,</p>
<blockquote>
<p>The two most commonly used attention functions are additive attention, and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of <span class="arithmatex">\(\frac{1}{\sqrt{d_k}}\)</span>. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.</p>
</blockquote>
<p><code>Question: how is it faster?</code></p>
<h2 id="model-architecture">Model Architecture</h2>
<p>Now, with the attention mechanism introduced, the major novelty of this paper was the removal of recurrent networks. The transformer network only relied on attention mechanism to learn, arguing <em>attention is all you need</em>.</p>
<p>It is still an encoder-decoder system with encoder producing some features at each timestep, which are later used by decoder to autoregressively decode. But how are encodings generated of a sequence, where each timestep is not independent. Afterall, this is what recurrent networks were adding, the sense of "time" and capturing the dependency of different timesteps. </p>
<p>Answer? Attention. Or more specifically, <strong>self-attention</strong>. Before we start going further, let's try to rephrase attention to not depend on "hidden states" or any recurrent networks.</p>
<h3 id="keys-queries-and-values">Keys, Queries and Values</h3>
<p>We can see attention needs not be on hidden states of encoder and decoder. Any sequence of "annotations" can be attented by a vector. The annotations happened to be hidden states of encoder and the vector was the hidden state of decoder at that timestep.</p>
<p>Vaswani et. al2017 generalized it to a broader context of any sequences. Let's see how.</p>
<p>We want to compute the attention score of each decoder timestep. At each time step we want to find the weights relating to current timestep. It can be seen as doing a soft-search our current decoder hidden state in all the encoder hidden states, to remove the concept of "hidden states", we call our current decoder hidden state as a "query" to look up in a database of "keys", our encoder hidden states.</p>
<p>A single query <span class="arithmatex">\(q_j\)</span>, of dimension <span class="arithmatex">\((1, d_k)\)</span> and a matrix of all keys <span class="arithmatex">\(K\)</span>, of dimension, <span class="arithmatex">\((T_k, d_k)\)</span> we can compute (scaled dot-product)attention scores vector,<span class="arithmatex">\(E_j\)</span> as:</p>
<div class="arithmatex">\[E_j = \frac{q_jK^\intercal}{\sqrt{d_k}}\]</div>
<p>which is a <span class="arithmatex">\((1,T_k)\)</span> dimensional vector containing scores for each timestep which need to be softmaxed to create attention weights, <span class="arithmatex">\(\mathbf{\alpha}_j\)</span></p>
<p>If we somehow have all the queries, <span class="arithmatex">\(Q\)</span> in a <span class="arithmatex">\((T_q, d_k)\)</span> matrix, we can compute all the attention weights for all timesteps as:</p>
<div class="arithmatex">\[\alpha = \text{softmax}\left(\frac{QK^\intercal}{\sqrt{d_k}}\right)\]</div>
<p>where <span class="arithmatex">\(\alpha\)</span> is a <span class="arithmatex">\((T_q, T_k)\)</span> matrix representing all weights for all Queries.</p>
<p>Now, once we have the attention weights, we have to do a weighted sum on the encoder hidden states again for each query. We can clearly see that can be as:</p>
<div class="arithmatex">\[\begin{align*}
\text{Attention} &amp;= \alpha K\\
                                  &amp;=\text{softmax}(\frac{QK^\intercal}{\sqrt{d_k}})K
\end{align*}\]</div>
<p>Here, we sum over our keys again, but that may not be the case. To generalize it further the sequence to sum it over, we call them "values" represented by <span class="arithmatex">\(V\)</span> of shape  <span class="arithmatex">\((T_k, d_v)\)</span>. So using this new notation of keys queries and values, we can define (scaled dot-product) Attention as:</p>
<div class="arithmatex">\[\text{Attention}(Q,K,V) = \text{softmax}(\frac{QK^\intercal}{\sqrt{d_k}})V\]</div>
<p>This is basically a database where we wanna fetch some values which are indexed by some keys and we have some queries which we will apply on keys and then fetch the corresponding values of these keys, but in a "soft" way.</p>
<p>As we can see, the output shape of Attention is <span class="arithmatex">\((T_q, d_v)\)</span> that is we generated a new sequence.</p>
<p>So given a sequence of embeddings we can create a new sequence of same shape by using them as keys, queries and values. This is how we can replace the concurrent part and replace it by self-attention. Each timestep in output looks at each timestep in input to create a new input and attention weights provide the dependency across timesteps.</p>
<p><strong>Note:</strong> In attention, optionally, one might want to apply a mask to determine which keys should be available for attention for a certain query. This masking is done before softmax. It is saved in a matrix <span class="arithmatex">\(M\)</span> same shape as <span class="arithmatex">\((T_q, T_k)\)</span> but with zeros at places we want to be part of attention and negative infinty at places we don't want to attend to and this matrix is added to scaled dot product output.</p>
<div class="arithmatex">\[\text{Attention}(Q,K,V) = \text{softmax}(\frac{QK^\intercal}{\sqrt{d_k}}+M)V\]</div>
<p><img alt="" src="../files/scaled_dot_product_attn.jpg" /></p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">scaled_dot_product_attn</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">M</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">d_k</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="c1">## all inputs are batched</span>
    <span class="n">K_T</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">dot_product</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K_T</span><span class="p">)</span>
    <span class="n">scaled_dot_product</span> <span class="o">=</span> <span class="n">dot_product</span><span class="o">/</span><span class="n">d_k</span>
    <span class="k">if</span> <span class="n">M</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">scaled_dot_product</span><span class="o">+=</span><span class="n">M</span>
    <span class="n">attn_wts</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scaled_dot_product</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">attn_wts</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">d_k</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">d_q</span> <span class="o">=</span> <span class="n">d_k</span>
<span class="n">d_v</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">T_k</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">T_q</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">T_v</span> <span class="o">=</span> <span class="n">T_k</span>
<span class="n">batch</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">K</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">T_k</span><span class="p">,</span> <span class="n">d_k</span><span class="p">)</span>
<span class="n">Q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">T_q</span><span class="p">,</span> <span class="n">d_q</span><span class="p">)</span>
<span class="n">V</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">T_v</span><span class="p">,</span> <span class="n">d_v</span><span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">scaled_dot_product_attn</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">T_q</span><span class="p">,</span> <span class="n">T_k</span><span class="p">)</span><span class="o">&lt;</span><span class="mf">0.2</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="o">-</span><span class="mf">1e11</span><span class="p">)</span>
<span class="n">masked_output</span> <span class="o">=</span> <span class="n">scaled_dot_product_attn</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">masked_output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div>
<pre><code>torch.Size([2, 4, 3])
torch.Size([2, 4, 3])
</code></pre>
<h3 id="multi-head-attention">Multi Head Attention</h3>
<p>This was another addition by the paper. Instead of computing attention only once, we linearly project queries, keys and values <span class="arithmatex">\(h\)</span> different times and perform <span class="arithmatex">\(h\)</span> different attentions paralelly which are later concatenated.</p>
<p>According to Vaswani et al.2017,</p>
<blockquote>
<p>Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.</p>
</blockquote>
<p>It is illustrated in image below:</p>
<p><img alt="" src="../files/multi-head-attention.png" /></p>
<p>So,</p>
<div class="arithmatex">\[\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1,\dots, \text{head}_h)W^O\]</div>
<p>where</p>
<div class="arithmatex">\[\text{head}_i = \text{Attention}(QW^Q_i, KW^K_i, VW^V_i)\]</div>
<p>where the projections are parameter matrices <span class="arithmatex">\(W^Q_i \in \mathbb{R}^{d_{\text{model}} \times d_k}\)</span> ,  <span class="arithmatex">\(W^K_i \in \mathbb{R}^{d_{\text{model}} \times d_k}\)</span>, <span class="arithmatex">\(W^V_i \in \mathbb{R}^{d_{\text{model}} \times d_v}\)</span> and <span class="arithmatex">\(W^O \in \mathbb{R}^{hd_v \times d_{\text{model}}}\)</span>.</p>
<p>It is common to have <span class="arithmatex">\(d_k = d_v = \frac{d_{\text{model}}}{h}\)</span>. </p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">nn</span>

<span class="k">class</span><span class="w"> </span><span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">d_model</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MultiHeadAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># instead of having h layers from d_model to d_k, </span>
        <span class="c1"># we can have one layer from d_model to h*d_k</span>
        <span class="c1"># h*d_k = d_model</span>
        <span class="k">assert</span> <span class="n">d_model</span><span class="o">%</span><span class="n">num_heads</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;model dim should be divisible by num_heads&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">o_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>


    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>

        <span class="c1">#q: batch, T_q, d_model</span>
        <span class="c1">#q_proj: batch, T_q, d_model</span>
        <span class="n">bsize</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">q_proj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_layer</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>

        <span class="n">k_proj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_layer</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
        <span class="n">v_proj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_layer</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>

        <span class="c1"># q_proj_heads: batch,T_q, n_heads, d_q</span>
        <span class="n">q_proj_heads</span> <span class="o">=</span> <span class="n">q_proj</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsize</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>  <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> 
                                    <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">))</span>
        <span class="n">k_proj_heads</span> <span class="o">=</span> <span class="n">k_proj</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsize</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>  <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> 
                                    <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">))</span>
        <span class="n">v_proj_heads</span> <span class="o">=</span> <span class="n">v_proj</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsize</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>  <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> 
                                    <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">))</span>

        <span class="c1"># put all heads in batch dim since scaled_dot_product_attn </span>
        <span class="c1"># already handles batch</span>

        <span class="c1"># q_proj_batched: batch*n_heads, T_q, dq</span>
        <span class="n">q_proj_batched</span> <span class="o">=</span> <span class="n">q_proj_heads</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">bsize</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> 
                                <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">)</span> <span class="p">)</span>
        <span class="n">k_proj_batched</span> <span class="o">=</span> <span class="n">k_proj_heads</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">bsize</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> 
                                <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">)</span> <span class="p">)</span>
        <span class="n">v_proj_batched</span> <span class="o">=</span> <span class="n">v_proj_heads</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">bsize</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> 
                                <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">)</span> <span class="p">)</span>

        <span class="c1">##attn_out: batch*n_heads, T_q, dq</span>
        <span class="n">attn_out</span> <span class="o">=</span> <span class="n">scaled_dot_product_attn</span><span class="p">(</span><span class="n">q_proj_batched</span><span class="p">,</span> <span class="n">k_proj_batched</span><span class="p">,</span> 
                                            <span class="n">v_proj_batched</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
        <span class="c1">## batch, n_heads, Tq, dq</span>
        <span class="n">attn_out</span> <span class="o">=</span> <span class="n">attn_out</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> 
                                <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">))</span>
        <span class="c1">##batch, Tq, n_heads, dq</span>
        <span class="n">attn_out</span> <span class="o">=</span> <span class="n">attn_out</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="c1">##batch, Tq, n_heads, h*dq(d_model)</span>
        <span class="n">concat_out</span> <span class="o">=</span> <span class="n">attn_out</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">bsize</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>
<span class="c1">#         concat_out = torch.cat(attns_outs, dim=-1)</span>
        <span class="n">mha_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">o_layer</span><span class="p">(</span><span class="n">concat_out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">mha_out</span>          
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">d_model</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">n_heads</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">batch</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">T</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
<span class="n">k</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>

<span class="n">mha</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
<span class="n">mha</span><span class="p">(</span><span class="n">q</span><span class="p">,</span><span class="n">k</span><span class="p">,</span><span class="n">v</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div>
<pre><code>torch.Size([2, 10, 64])
</code></pre>
<h3 id="the-transformer-encoder">The Transformer Encoder</h3>
<p>Now with the information we are ready to build the first component of the transformer. It is the replacement of the recurrent network in common sequence to sequence tasks. This encodes the input into a series of annotations or hidden states which are passed onto the decoder.</p>
<p>An Encoder comprises of multiple encoder layers (similar to satcking of RNNs etc). Each encoding layer consists of two major components. First <strong>multi head self attention layer</strong> (i.e key, query, value are all same, the previous layer's output) and mask consists of all zeros. Every timestep in input is allowed to be attended. Second is a simple, positionwise fully connected feed-forward network.</p>
<p>Residual Skip Connections are added around the two sublayers, followed by a Layer Normalization. Dropout is added to the output of sublayer before adding.  That is,  <span class="arithmatex">\(\text{LayerNorm}(x + \text{Dropout}(\text{Sublayer}(x)))\)</span>, where sublayer can be an MHA or FeedForward Network.</p>
<p><img alt="" src="../files/transformer_encoder_layer.jpg" /></p>
<p>These Encoder layers are stacked together to consume previous layers output to form an encoder. The original transformer Encoder has 6 such layers. </p>
<p>The feed-forward network is usually a two layered network with a ReLU activation in between which is applied on each time-step independently. The input dimension and output dimension of feed-forward network would be <span class="arithmatex">\(d_{\text{model}}\)</span>, but the intermediate layer can be different, this is called the feed-forward dim and denoted as <span class="arithmatex">\(d_{ff}\)</span>. </p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">GlobalSelfAttention</span><span class="p">(</span><span class="n">MultiHeadAttention</span><span class="p">):</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

<span class="k">class</span><span class="w"> </span><span class="nc">PositionWiseFeedForward</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">d_model</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PositionWiseFeedForward</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_ff</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">act</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">class</span><span class="w"> </span><span class="nc">AddNorm</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">AddNorm</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layernorm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">sublayer</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm</span><span class="p">(</span><span class="n">x</span><span class="o">+</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">sublayer</span><span class="p">))</span>
</code></pre></div>
<p>With fundamental building blocks ready, let's write an Encoder Layer</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">EncoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">EncoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">GlobalSelfAttention</span><span class="p">(</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ff</span> <span class="o">=</span> <span class="n">PositionWiseFeedForward</span><span class="p">(</span><span class="n">d_ff</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">addnorm1</span> <span class="o">=</span> <span class="n">AddNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">addnorm2</span> <span class="o">=</span> <span class="n">AddNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1">## self globall attn</span>
        <span class="n">attn_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">addnorm1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">attn_out</span><span class="p">)</span>
        <span class="n">ff_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ff</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">addnorm2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ff_out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">d_model</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">d_ff</span> <span class="o">=</span> <span class="mi">256</span>
<span class="n">batch</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">T</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">n_heads</span> <span class="o">=</span> <span class="mi">8</span>

<span class="n">input_seq</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
<span class="n">enc</span> <span class="o">=</span> <span class="n">EncoderLayer</span><span class="p">(</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span>
<span class="n">enc</span><span class="p">(</span><span class="n">input_seq</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div>
<pre><code>torch.Size([2, 20, 64])
</code></pre>
<h4 id="positional-encoding">Positional Encoding</h4>
<p>The Transformer encoder is a stack of these layers and some other parts. Let's look at the complete figure of encoder.</p>
<p><img alt="" src="../files/transformer_encoder.jpg" /></p>
<p>As we can see, it is comprised of <span class="arithmatex">\(N\)</span> encoder layers and has inputs and something called a Positional Encoding System. What is that?</p>
<p>Recall how we used self attention to replace the recurrence which helped in making decisions across multiple timesteps. However, the order of a sequence also matters, which recurrent networks also handled. </p>
<p>In the current self attention mechanism, if we permute the input tokens randomly, the output would be same (in that permuted order), it does not take into account which token came after which one, it just utilizes the fact that these tokens have come together. </p>
<p>To introduce a sense of time Positional Encodings are introduced.</p>
<p>By Vaswani et al.2017:</p>
<blockquote>
<p>Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add "positional encodings" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension <span class="arithmatex">\(d_{\text{model}}\)</span> as the embeddings, so that the two can be summed.</p>
</blockquote>
<p>One simple way is to learn the position encodings by the position of each token through an Embedding Layer and then add them to input embeddings. Other way proposed by this paper is using sine and cosine functions:</p>
<div class="arithmatex">\[
\begin{align*}
PE_{(pos, 2i)} &amp;= \sin{(pos/10000^{2i/d_\text{model}})}\\
PE_{(pos, 2i+1)} &amp;= \cos{(pos/10000^{2i/d_\text{model}})}\\
\end{align*}
\]</div>
<p>where <span class="arithmatex">\(pos\)</span> is the position and <span class="arithmatex">\(i\)</span> is the dimension</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">positional_encoding</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">d_model</span><span class="p">):</span>
    <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="n">pos</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span><span class="o">/</span><span class="n">d_model</span><span class="p">)</span>
    <span class="n">angle</span> <span class="o">=</span> <span class="n">pos</span><span class="o">/</span><span class="n">div</span>
    <span class="n">pe</span><span class="p">[:,</span> <span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">angle</span><span class="p">)</span>
    <span class="n">pe</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">angle</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">pe</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">T</span> <span class="o">=</span> <span class="mi">60</span>
<span class="n">d_model</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">pos_e</span> <span class="o">=</span> <span class="n">positional_encoding</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">pcolormesh</span><span class="p">(</span><span class="n">pos_e</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;RdBu&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Position&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Depth&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>
<p><img alt="" src="../files/notes_40_0.png" /></p>
<p>One natural question is why this? There are multiple reasons. One it is not learnable. Values are between -1 and 1. Unique for each position. And it can extrapolate to longer sequences than present in training.</p>
<p>Also, mentioned in paper:</p>
<blockquote>
<p>We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset <span class="arithmatex">\(k\)</span>, <span class="arithmatex">\(PE_{pos+k}\)</span> can be represented as a linear function of <span class="arithmatex">\(PE_{pos}\)</span>.</p>
</blockquote>
<p>What does it mean <span class="arithmatex">\(PE_{pos+k}\)</span> to be a linear function of <span class="arithmatex">\(PE_{pos}\)</span>? Well, it means there exists a matrix <span class="arithmatex">\(M^{(k)}\)</span> independent of <span class="arithmatex">\(pos\)</span> such that <span class="arithmatex">\(M^{(k)}PE_{pos} = PE_{pos+k}\)</span>.</p>
<p>A general proof can be found here: https://blog.timodenk.com/linear-relationships-in-the-transformers-positional-encoding/</p>
<p>But I will provide a small proof here as well.</p>
<p><span class="arithmatex">\(PE_{pos}\)</span> can be seen as a vector of pairs of <span class="arithmatex">\(\sin\)</span> and <span class="arithmatex">\(\cos\)</span> functions, e.g for dimensions <span class="arithmatex">\(d\)</span> and <span class="arithmatex">\(pos=t\)</span>,</p>
<div class="arithmatex">\[
PE_t = 
\begin{bmatrix}
\sin (t/10000^{(0/d)}) \\
\cos (t/10000^{(0/d)}) \\
\sin (t/10000^{(1/d)}) \\
\cos (t/10000^{(1/d)}) \\
\vdots
\end{bmatrix}
= 
\begin{bmatrix}
\sin (\omega_0 t) \\
\cos (\omega_0 t) \\
\sin (\omega_1 t) \\
\cos (\omega_1 t) \\
\vdots
\end{bmatrix}
\]</div>
<p>where <span class="arithmatex">\(\omega_p =\frac{1}{10000^{(p/d)}}\)</span> is the frequency.</p>
<p>For every sine cosine pair of frequency <span class="arithmatex">\(\omega_p\)</span>, there is a linear transformation <span class="arithmatex">\(M \in \mathbb{R}^{2\times2}\)</span>, independent of <span class="arithmatex">\(t\)</span>, for which the following is true:</p>
<div class="arithmatex">\[M \begin{bmatrix}
\sin (\omega_p t) \\
\cos (\omega_p t) \\
\end{bmatrix}
=
\begin{bmatrix}
\sin (\omega_p (t+k)) \\
\cos (\omega_p (t+k)) \\
\end{bmatrix}
\]</div>
<p><strong>Proof</strong>:
Let <span class="arithmatex">\(M\)</span> be a <span class="arithmatex">\(2 \times 2\)</span> matrix, and <span class="arithmatex">\(v_1, v_2, v_3, v_4\)</span> are the elements. We want to find them.</p>
<div class="arithmatex">\[
\begin{bmatrix}
v_1 &amp; v_2 \\ v_3 &amp; v_4
\end{bmatrix}
\begin{bmatrix}
\sin (\omega_p t) \\
\cos (\omega_p t) \\
\end{bmatrix}
=
\begin{bmatrix}
\sin (\omega_p (t+k)) \\
\cos (\omega_p (t+k)) \\
\end{bmatrix}
\]</div>
<div class="arithmatex">\[
\begin{bmatrix}
v_1\sin (\omega_p t) + v_2\cos (\omega_p t)\\
v_3\sin (\omega_p t) + v_4\cos (\omega_p t)\\
\end{bmatrix}
=
\begin{bmatrix}
\sin (\omega_p t) \cos(\omega_p k) + \cos(\omega_p t) \sin(\omega_p k)\\
\cos (\omega_p t) \cos(\omega_p k) - \sin(\omega_p t) \sin(\omega_p k)\\
\end{bmatrix}
\]</div>
<p>By solving the above equations,</p>
<div class="arithmatex">\[v_1 = \cos(\omega_pk), v_2 = \sin(\omega_pk), v_3 = -\sin(\omega_pk), v_4 = \cos(\omega_pk)\]</div>
<p>So,</p>
<div class="arithmatex">\[M = 
\begin{bmatrix}
\cos(\omega_pk) &amp; \sin(\omega_pk) \\ -\sin(\omega_pk) &amp; \cos(\omega_pk)
\end{bmatrix}
\]</div>
<p>Similarly we can find M for other pairs and those can be stacked diagonally to form the final matrix, which is independent of <span class="arithmatex">\(t\)</span> for a certain <span class="arithmatex">\(k\)</span>.</p>
<p>Check out this awesome blog about positional encodings and some more intuition behind it: https://kazemnejad.com/blog/transformer_architecture_positional_encoding/</p>
<p>Finally these encodings are added to the input embeddings and then passed onto the first encoder layer, which gives us the complete encoder.</p>
<p>According to paper:</p>
<blockquote>
<p>In the embedding layers, we multiply those weights by <span class="arithmatex">\(\sqrt{d_\text{model}}\)</span>.</p>
</blockquote>
<p>Let's complete the code</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">PositionalEmbedding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PositionalEmbedding</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">emb</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seq</span><span class="p">):</span>
        <span class="n">timesteps</span> <span class="o">=</span> <span class="n">seq</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">emb</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span>
        <span class="n">emb</span> <span class="o">=</span> <span class="n">emb</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">))</span>
        <span class="n">pe</span> <span class="o">=</span> <span class="n">positional_encoding</span><span class="p">(</span><span class="n">timesteps</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">emb</span><span class="o">+</span><span class="n">pe</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div>
<p>In the paper these embedding weights are shared, but we will keep them separate them out here.</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">Encoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> 
                    <span class="n">num_heads</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Encoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                        <span class="o">*</span><span class="p">[</span><span class="n">EncoderLayer</span><span class="p">(</span><span class="n">num_heads</span><span class="p">,</span> 
                                        <span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)]</span>
                                        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pos_emb</span> <span class="o">=</span> <span class="n">PositionalEmbedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seq</span><span class="p">):</span>
        <span class="n">emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_emb</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder_layers</span><span class="p">(</span><span class="n">emb</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">n_layers</span> <span class="o">=</span> <span class="mi">6</span>
<span class="n">d_model</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">d_ff</span> <span class="o">=</span> <span class="mi">2048</span>
<span class="n">num_heads</span> <span class="o">=</span> <span class="mi">8</span>

<span class="n">T</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">batch</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">122</span>

<span class="n">dummy_inp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">T</span><span class="p">))</span>
<span class="n">enc</span> <span class="o">=</span> <span class="n">Encoder</span><span class="p">(</span><span class="n">n_layers</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">enc</span><span class="p">(</span><span class="n">dummy_inp</span><span class="p">)</span>
<span class="n">out</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div>
<pre><code>torch.Size([4, 128, 512])
</code></pre>
<h3 id="the-transformer-decoder">The Transformer Decoder</h3>
<p>The transformer decoder is a replacement for recurrent decoder. It works in the same way fundamentally, where it predicts an output which is fed back into at the next timestep and the next token is predicted. This is called autoregressive prediction.</p>
<p>Recalling the original recurrent network, attention was used from hidden states of encoder to each timestep in decoder. Here, the idea is same. At each decoder layer, Queries will be fetched from decoder timesteps, and Keys and Values will be the final outputs of the Encoder.</p>
<p>The recurrence is replaced by the self attention, same as the encoder. The attention from the encoder is called cross attention. </p>
<p>Another benefit of replacing recurrence with attention is that we can use teacher forcing and parallelize decoding. We pass the whole Target Sequence into decoder shifted right, adding <code>&lt;SOS&gt;</code> as the beginning of sequence and predict the unshifted sequence.</p>
<p>We have to be careful that a timestep does not attend into future timesteps. That is where the attention mask comes in handy. This is called Masked Self Attention or Causal Self Attention.</p>
<p><img alt="" src="../files/CausalSelfAttention-new.png" />
Figure: Causal Self attention or masked self attention</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">CausalSelfAttention</span><span class="p">(</span><span class="n">MultiHeadAttention</span><span class="p">):</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">seq_length</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">num_heads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span>

        <span class="n">mask_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch_size</span><span class="o">*</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">)</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="o">*</span><span class="n">mask_shape</span><span class="p">),</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span><span class="o">*-</span><span class="mf">1e9</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>
</code></pre></div>
<p>Let's look at the complete Decoder Layer Architecture.</p>
<p><img alt="" src="../files/transformer_decoder_layer.jpg" /></p>
<p>The Masked Attention was just explained. Now the cross attention, where the key and Values are from the encoder outputs and the queries are from decoder. This is the original concept of encoder-decoder attention by Bhadanau.</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">CrossAttention</span><span class="p">(</span><span class="n">MultiHeadAttention</span><span class="p">):</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">encoder_output</span><span class="p">,</span> <span class="n">v</span><span class="o">=</span><span class="n">encoder_output</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</code></pre></div>
<p>With that we are now ready with complete Decoder Layer.</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">DecoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DecoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">CausalSelfAttention</span><span class="p">(</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cross_attn</span> <span class="o">=</span> <span class="n">CrossAttention</span><span class="p">(</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ff</span> <span class="o">=</span> <span class="n">PositionWiseFeedForward</span><span class="p">(</span><span class="n">d_ff</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">addnorm1</span> <span class="o">=</span> <span class="n">AddNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">addnorm2</span> <span class="o">=</span> <span class="n">AddNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">addnorm3</span> <span class="o">=</span> <span class="n">AddNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">):</span>
        <span class="c1">## self globall attn</span>
        <span class="n">attn_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">addnorm1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">attn_out</span><span class="p">)</span>
        <span class="n">attn_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cross_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">addnorm2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">attn_out</span><span class="p">)</span>
        <span class="n">ff_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ff</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">addnorm3</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ff_out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">d_model</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">d_ff</span> <span class="o">=</span> <span class="mi">256</span>
<span class="n">batch</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">T</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">n_heads</span> <span class="o">=</span> <span class="mi">8</span>

<span class="n">input_seq</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
<span class="n">enc_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
<span class="n">dec</span> <span class="o">=</span> <span class="n">DecoderLayer</span><span class="p">(</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span>
<span class="n">dec</span><span class="p">(</span><span class="n">input_seq</span><span class="p">,</span> <span class="n">enc_out</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div>
<pre><code>torch.Size([2, 20, 64])
</code></pre>
<p>Now the complete Decoder.</p>
<p><img alt="" src="../files/transformer_decoder.jpg" /></p>
<p>We know the drill, add positinal encodings and a few more layers at the top. We will transform final outputs to be the same dims as out vocab size.</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">Decoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> 
                    <span class="n">d_ff</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">vocab_dim</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Decoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
                    <span class="p">[</span><span class="n">DecoderLayer</span><span class="p">(</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> 
                                    <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">vocab_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pos_emb</span> <span class="o">=</span> <span class="n">PositionalEmbedding</span><span class="p">(</span><span class="n">vocab_dim</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seq</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_emb</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder_layers</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">n_layers</span> <span class="o">=</span> <span class="mi">6</span>
<span class="n">d_model</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">d_ff</span> <span class="o">=</span> <span class="mi">2048</span>
<span class="n">num_heads</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">vocab_size</span><span class="o">=</span> <span class="mi">64</span>

<span class="n">T</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">batch</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">encoder_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
<span class="n">dummy_inp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">T</span><span class="p">))</span>
<span class="n">dec</span> <span class="o">=</span> <span class="n">Decoder</span><span class="p">(</span><span class="n">n_layers</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">dec</span><span class="p">(</span><span class="n">dummy_inp</span><span class="p">,</span> <span class="n">encoder_out</span><span class="p">)</span>
<span class="n">out</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div>
<pre><code>torch.Size([4, 128, 64])
</code></pre>
<p>Let's put the transformer together now.</p>
<p><img alt="" src="../files/transformer.jpg" /></p>
<p>We will have some tokenized inputs and tokenized targets. Inputs will go into an embedding layer and then passed onto the encoder to produce some encodings. Target embeddings will be passed onto decoder along with encoder output to produce next timestep sequence.</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">Transformer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">enc_layers</span><span class="p">,</span> <span class="n">dec_layers</span><span class="p">,</span> 
                 <span class="n">enc_d_ff</span><span class="p">,</span> <span class="n">dec_d_ff</span><span class="p">,</span> <span class="n">enc_num_heads</span><span class="p">,</span> <span class="n">dec_num_heads</span><span class="p">,</span>
                 <span class="n">inp_vocab_dim</span><span class="p">,</span> <span class="n">out_vocab_dim</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Transformer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">Encoder</span><span class="p">(</span><span class="n">enc_layers</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> 
                                <span class="n">enc_d_ff</span><span class="p">,</span> <span class="n">enc_num_heads</span><span class="p">,</span> <span class="n">inp_vocab_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">Decoder</span><span class="p">(</span><span class="n">dec_layers</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> 
                                <span class="n">dec_d_ff</span><span class="p">,</span> <span class="n">dec_num_heads</span><span class="p">,</span> <span class="n">out_vocab_dim</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inp_seq</span><span class="p">,</span> <span class="n">shifted_target</span><span class="p">):</span>
        <span class="n">encoder_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">inp_seq</span><span class="p">)</span>
        <span class="n">decoder_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">shifted_target</span><span class="p">,</span> <span class="n">encoder_out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">decoder_out</span>     
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">d_model</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">enc_layers</span> <span class="o">=</span> <span class="n">dec_layers</span> <span class="o">=</span> <span class="mi">6</span>
<span class="n">enc_d_ff</span> <span class="o">=</span> <span class="n">dec_d_ff</span> <span class="o">=</span> <span class="mi">2048</span>
<span class="n">enc_num_heads</span> <span class="o">=</span> <span class="n">dec_num_heads</span> <span class="o">=</span> <span class="mi">8</span>

<span class="n">input_vocab_dim</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">output_vocab_dim</span> <span class="o">=</span> <span class="mi">52</span>

<span class="n">batch</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">T</span> <span class="o">=</span> <span class="mi">128</span>

<span class="n">input_seq</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">input_vocab_dim</span><span class="p">,</span> <span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">T</span><span class="p">))</span>
<span class="n">shifted_target_seq</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">output_vocab_dim</span><span class="p">,</span> <span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">T</span><span class="p">))</span>

<span class="n">transformer</span> <span class="o">=</span> <span class="n">Transformer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">enc_layers</span><span class="p">,</span> <span class="n">dec_layers</span><span class="p">,</span> <span class="n">enc_d_ff</span><span class="p">,</span> <span class="n">dec_d_ff</span><span class="p">,</span> 
                            <span class="n">enc_num_heads</span><span class="p">,</span> <span class="n">dec_num_heads</span><span class="p">,</span> <span class="n">input_vocab_dim</span><span class="p">,</span> <span class="n">output_vocab_dim</span><span class="p">)</span>

<span class="n">transformer</span><span class="p">(</span><span class="n">input_seq</span><span class="p">,</span> <span class="n">shifted_target_seq</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div>
<pre><code>torch.Size([5, 128, 52])
</code></pre>
<h2 id="resources">Resources:</h2>
<ol>
<li><a href="https://lilianweng.github.io/posts/2018-06-24-attention/">https://lilianweng.github.io/posts/2018-06-24-attention/</a> Great post for different types of attentions.</li>
<li><a href="https://www.tensorflow.org/text/tutorials/transformer">https://www.tensorflow.org/text/tutorials/transformer</a> Official TensorFlow post on transformer information.</li>
<li><a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html">https://nlp.seas.harvard.edu/2018/04/03/attention.html</a> The Annotated Transformer (although i think Add and Norm, is implemented a bit differently)</li>
<li><a href="https://mfaizan.github.io/2023/04/02/sines.html">https://mfaizan.github.io/2023/04/02/sines.html</a> Interesting blog developing intuition of positional encoding by using, <em>complex numbers</em>, of all numbers. </li>
</ol>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2025 Murtaza Nazir
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../../..", "features": ["content.code.copy"], "search": "../../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
        <script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
      
    
  </body>
</html>